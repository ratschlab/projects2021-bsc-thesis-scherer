{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, urllib.request, json\n",
    "import pandas as pd\n",
    "import shutil\n",
    "from io import BytesIO\n",
    "from urllib.request import urlopen\n",
    "from zipfile import ZipFile\n",
    "from icalendar import Calendar\n",
    "import datetime\n",
    "from datetime import date, timedelta, timezone\n",
    "pd.options.display.max_rows = None\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import requests\n",
    "\n",
    "cantonKeys = ['AG','AI','AR', 'BE', 'BL', 'BS', 'FR', 'GE', 'GL', 'GR', 'JU', 'LU', 'NE', 'NW', 'OW', 'SG', 'SH', 'SO', 'SZ', 'TG', 'TI', 'UR', 'VD', 'VS', 'ZG','ZH']\n",
    "# special mapping for weather API\n",
    "#['AG','AI','AR', 'BE', 'BL', 'BS', 'FR', 'GE', 'GL', 'GR', 'JU', 'LU', 'NE', 'NW', 'OW', 'SG', 'SH', 'SO', 'SZ', 'TG', 'TI', 'UR', 'VD', 'VS', 'ZG','ZH']\n",
    "#[\"Aarau\",\"Appenzell\",\"Herisau\",\"Bern\",\"Liestal\",\"Basel\",\"Fribourg\",\"Geneve\",\"2660594\",\"Chur\",\"Delemont\",\"Luzern\",\"Neuchatel\",\"Stans\",\"Sarnen\",\"Sankt Gallen\", \"Schaffhausen\", \"Olten\",\"Schwyz\",\"Frauenfeld\",\"Bellinzona\",\"2661780\",\"Lausanne\",\"Sion\",\"Zug\",\"Zurich\"]\n",
    "weatherDictionary = dict(zip(cantonKeys,[2661881,2661740,2660365,2661552,2659891,2661604,2660718,2660646,2660594,2661169,2661035,2659811,2659496,2658504,2658786,2658822,2658761,2658564,2658665,2660727,2661567,2661780,2659994,2658576,2657908,2657896]))\n",
    "apiKey = \"0077c15de8e01960cc024a8b11751ead\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOPH data downloaded (5.579061985015869 seconds)\n"
     ]
    }
   ],
   "source": [
    "# download FOPH data\n",
    "start = time.time()\n",
    "# get newest available string to download FOPH data\n",
    "zipurl = ''\n",
    "with urllib.request.urlopen(\"https://www.covid19.admin.ch/api/data/context\") as url:\n",
    "    data = json.loads(url.read().decode())\n",
    "    zipurl = data[\"sources\"][\"zip\"][\"csv\"]\n",
    "\n",
    "# download the FOPH data (use this data also for the virus variants)\n",
    "with urlopen(zipurl) as zipresp:\n",
    "    with ZipFile(BytesIO(zipresp.read())) as zfile:\n",
    "        zfile.extractall('data/FOPH')\n",
    "\n",
    "print(\"FOPH data downloaded (%s seconds)\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google mobility data downloaded (25.807024240493774 seconds)\n"
     ]
    }
   ],
   "source": [
    "# download the Google mobility data\n",
    "start = time.time()\n",
    "zipurl = 'https://www.gstatic.com/covid19/mobility/Region_Mobility_Report_CSVs.zip'\n",
    "with urlopen(zipurl) as zipresp:\n",
    "    with ZipFile(BytesIO(zipresp.read())) as zfile:\n",
    "        zfile.extractall('data/GoogleMobility')\n",
    "\n",
    "print(\"Google mobility data downloaded (%s seconds)\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intervista mobility data downloaded (4.982004880905151 seconds)\n"
     ]
    }
   ],
   "source": [
    "# download intervista mobility data\n",
    "start = time.time()\n",
    "zipurl = 'https://www.intervista.ch/media/2020/03/Download_Mobilit%C3%A4ts-Monitoring_Covid-19.zip'\n",
    "with urlopen(zipurl) as zipresp:\n",
    "    with ZipFile(BytesIO(zipresp.read())) as zfile:\n",
    "        zfile.extractall('data/IntervistaMobility')\n",
    "print(\"Intervista mobility data downloaded (%s seconds)\" % (time.time() - start))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KOF strigency data downloaded (0.8076879978179932 seconds)\n"
     ]
    }
   ],
   "source": [
    "# download KOF strigency index data\n",
    "start = time.time()\n",
    "df = pd.read_csv('https://datenservice.kof.ethz.ch/api/v1/public/sets/stringency_plus_web?mime=csv&df=Y-m-d')\n",
    "if not os.path.exists('data/KOF'):\n",
    "    os.makedirs('data/KOF')\n",
    "df.to_csv('data/KOF/KOFStrigencyIndex.csv')\n",
    "print(\"KOF strigency data downloaded (%s seconds)\" % (time.time() - start)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holyday/Vacation data computed (0.5700135231018066 seconds)\n"
     ]
    }
   ],
   "source": [
    "# compute holydays and vacations per canton from ical files (has only to run once to create holydayvacation.csv)\n",
    "\n",
    "start = time.time()\n",
    "holydayVacationTable = pd.DataFrame(index=pd.date_range(start='1/1/2020', end='31/12/2021'), columns = cantonKeys)\n",
    "holydayVacationTable.index = pd.to_datetime(holydayVacationTable.index) \n",
    "holydayVacationTable[cantonKeys] = 0\n",
    "\n",
    "\n",
    "def fillCantonHolidays(cantonKey, filename):\n",
    "    # extract dates from file and fill in all special holidays\n",
    "    file = open(filename, 'rb')\n",
    "    cal = Calendar.from_ical(file.read())\n",
    "    for e in cal.walk('vevent'):\n",
    "        start = e['DTSTART'].to_ical().decode('utf-8')\n",
    "        parsedDate = datetime.datetime.strptime(start, '%Y%m%d')\n",
    "        holydayVacationTable[cantonKey][parsedDate] = 1  \n",
    "    \n",
    "\n",
    "def fillCantonVacation(cantonKey, filename):\n",
    "    # extract dates from file\n",
    "    file = open(filename, 'rb')\n",
    "    cal = Calendar.from_ical(file.read())\n",
    "    for e in cal.walk('vevent'):\n",
    "        startDate = e['DTSTART'].to_ical().decode('utf-8')\n",
    "        endDate = e['DTEND'].to_ical().decode('utf-8')\n",
    "        parsedStartDate = datetime.datetime.strptime(startDate, '%Y%m%d')\n",
    "        parsedEndDate = datetime.datetime.strptime(endDate, '%Y%m%d') \n",
    "        if parsedEndDate > datetime.datetime(2021, 12, 31):\n",
    "            parsedEndDate = datetime.datetime(2021, 12, 31)\n",
    "        r = pd.date_range(start=parsedStartDate, end=parsedEndDate)\n",
    "        holydayVacationTable[cantonKey][r] = 1\n",
    "\n",
    "        \n",
    "# special mapping for holiday and vacation file names\n",
    "vacHolyDictionary = dict(zip(cantonKeys, ['aargau','appenzell-innerrhoden','appenzell-ausserrhoden','bern','basel-land','basel-stadt','freiburg','genf','glarus','graubuenden','jura','luzern','neuenburg','nidwalden','obwalden','sankt-gallen','schaffhausen','solothurn','schwyz','thurgau','tessin','uri','waadt','wallis','zug','zuerich']))       \n",
    "for c in cantonKeys:\n",
    "    # fill the canton holidays which are only single days\n",
    "    for p in ['static_data/holidays/2020/', 'static_data/holidays/2021/']:\n",
    "        matches = [match for match in os.listdir(p) if vacHolyDictionary[c] in match]\n",
    "        filename = matches[0]\n",
    "        path = p + filename\n",
    "        fillCantonHolidays(c, path)\n",
    "       \n",
    "    # fill the school vacations which have a start and end date\n",
    "    for p in ['static_data/vacations/2020/', 'static_data/vacations/2021/']:\n",
    "        matches = [match for match in os.listdir(p) if vacHolyDictionary[c] in match]\n",
    "        filename = matches[0]\n",
    "        path = p + filename\n",
    "        fillCantonVacation(c, path)\n",
    "\n",
    "'''\n",
    "# offset = 6 gets all sundays, offset 5 all saturdays\n",
    "def getDays(year, offset):\n",
    "   d = date(year, 1, 1)                    \n",
    "   d += timedelta(days = offset - d.weekday())  \n",
    "   while d.year == year:\n",
    "      yield d\n",
    "      d += timedelta(days = 7)\n",
    "\n",
    "# set weekends\n",
    "listOfSaturdaysSundays = []\n",
    "for year in [2020,2021]:\n",
    "    for weekday in [5,6]:\n",
    "        for day in getDays(year, weekday):\n",
    "            listOfSaturdaysSundays.append(day)\n",
    "for e in listOfSaturdaysSundays:\n",
    "    holydayVacationTable.loc[datetime.datetime.combine(e, datetime.datetime.min.time())]=1\n",
    "'''\n",
    "    \n",
    "holydayVacationTable.index = holydayVacationTable.index.rename(\"date\")\n",
    "\n",
    "if not os.path.exists('data/HolidayVacation'):\n",
    "    os.makedirs('data/HolidayVacation')    \n",
    "holydayVacationTable.to_csv('data/HolidayVacation/HolidayVacation.csv')\n",
    "print(\"Holyday/Vacation data computed (%s seconds)\" % (time.time() - start)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECDC data downloaded (2.777618646621704 seconds)\n"
     ]
    }
   ],
   "source": [
    "# subnational data for bordering countries (only weekly) \n",
    "start = time.time()\n",
    "df = pd.read_csv('https://opendata.ecdc.europa.eu/covid19/subnationalcaseweekly/csv')\n",
    "if not os.path.exists('data/ECDC'):\n",
    "    os.makedirs('data/ECDC')\n",
    "df.to_csv('data/ECDC/ECDCsubnationalcaseweekly.csv')\n",
    "print(\"ECDC data downloaded (%s seconds)\" % (time.time() - start)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "federal and cantonal measures computed (50.00833606719971 seconds)\n"
     ]
    }
   ],
   "source": [
    "# compute federal and cantonal measures\n",
    "start = time.time()\n",
    "#======================== construct the federal measures ================\n",
    "columnNames = pd.read_excel('static_data/measures/measures.xlsx', sheet_name = 'Federal').columns\n",
    "measures = pd.read_excel('static_data/measures/measures.xlsx', sheet_name = 'Federal', skiprows=6, names=columnNames)\n",
    "\n",
    "measures = measures.set_index('Time')\n",
    "\n",
    "for day in pd.date_range(start=datetime.datetime(2020, 1, 1), end=datetime.datetime(2021, 12, 31)):\n",
    "    if not day in measures.index:\n",
    "        measures.loc[day] = [float('NaN')] * len(measures.columns)\n",
    "\n",
    "measures = measures.sort_index()\n",
    "\n",
    "# propagate the update changes to all other days\n",
    "for j in measures.columns: #measure\n",
    "    dailyMeasureLevel = 0\n",
    "    for i in measures.index: #day\n",
    "        if math.isnan(measures.loc[i, j]):\n",
    "            measures.loc[i, j] = dailyMeasureLevel\n",
    "        else:\n",
    "            dailyMeasureLevel = measures.loc[i, j]\n",
    "\n",
    "if not os.path.exists('data/measures'):\n",
    "    os.makedirs('data/measures')\n",
    "measures.to_csv('data/measures/federal.csv')\n",
    "#======================== construct the cantonal measures ================\n",
    "for c in cantonKeys: \n",
    "    # copy the federal measures and use max function (with some exceptions)\n",
    "    cantMeasuresComplete = measures.copy()\n",
    "    cantMeasures = pd.read_excel('static_data/measures/measures.xlsx', sheet_name = c)\n",
    "    cantMeasures = cantMeasures.set_index('Time')\n",
    "\n",
    "    for day in pd.date_range(start=datetime.datetime(2020, 1, 1), end=datetime.datetime(2021, 12, 31)):\n",
    "        if not day in cantMeasures.index:\n",
    "            cantMeasures.loc[day] = [float('NaN')] * len(cantMeasures.columns)\n",
    "\n",
    "    cantMeasures = cantMeasures.sort_index()\n",
    "\n",
    "\n",
    "    for m in cantMeasures.columns:\n",
    "        dailyMeasureLevel = float('NaN')\n",
    "        for day in cantMeasures.index:\n",
    "            if not math.isnan(cantMeasures.loc[day,m]):\n",
    "                if cantMeasures.loc[day,m] != -1:\n",
    "                    dailyMeasureLevel = cantMeasures.loc[day,m]\n",
    "                else:\n",
    "                    cantMeasures.loc[day,m] = 0\n",
    "                    dailyMeasureLevel = float('NaN')\n",
    "            elif math.isnan(cantMeasures.loc[day,m]) and not math.isnan(dailyMeasureLevel):\n",
    "                cantMeasures.loc[day,m] = dailyMeasureLevel\n",
    "            else:\n",
    "                cantMeasures.loc[day,m] = 0\n",
    "\n",
    "    # from 22.12 until 9.1 cantonal measures are stronger than federal for restaurants, recreational, sport facilities\n",
    "    for m in cantMeasures.columns:\n",
    "        for day in cantMeasures.index:\n",
    "            if (day < datetime.datetime(2021, 1, 9) and day >= datetime.datetime(2020, 12, 22)  and (m == 'Restaurants' or m=='Cultural, entertainment and recreational facilities' or m=='Sport/Wellness facilities')) or (day >= datetime.datetime(2020, 12, 23) and day < datetime.datetime(2021, 1, 3) and m=='Gatherings/private events'):\n",
    "                # cantonal exeption possible\n",
    "                if cantMeasures.loc[day,m] != 0:\n",
    "                    cantMeasuresComplete.loc[day,m] = cantMeasures.loc[day,m]\n",
    "            else:\n",
    "                cantMeasuresComplete.loc[day,m] = max(cantMeasuresComplete.loc[day,m], cantMeasures.loc[day,m])\n",
    "    \n",
    "    if not os.path.exists('data/measures'):\n",
    "        os.makedirs('data/measures')\n",
    "    cantMeasuresComplete.to_csv('data/measures/'+c+'.csv')\n",
    "print(\"federal and cantonal measures computed (%s seconds)\" % (time.time() - start)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloaded historical statistical weather data(1621.4294505119324 seconds)\n"
     ]
    }
   ],
   "source": [
    "# download the historical statistical weather data from 1.1.2020 to 18.3.2020 (has only to run once)\n",
    "'''\n",
    "start = time.time()\n",
    "for cantonId in weatherDictionary.keys():\n",
    "    cityId = str(weatherDictionary[cantonId])\n",
    "    statisticalData = pd.DataFrame()\n",
    "    for everyDay in pd.date_range(start=datetime.datetime(2020, 1, 1), end=datetime.datetime(2020, 3, 18)):\n",
    "        monthNumber = str(everyDay.month)\n",
    "        dayNumber = str(everyDay.day)\n",
    "        apiCall =\"https://history.openweathermap.org/data/2.5/aggregated/day?id=\"+cityId+\"&month=\"+monthNumber+\"&day=\"+dayNumber+\"&appid=\"+apiKey\n",
    "        with urllib.request.urlopen(apiCall) as url:\n",
    "                    data = json.loads(url.read().decode())\n",
    "                    dfloaded = pd.json_normalize(data[\"result\"])\n",
    "                    statisticalData = statisticalData.append(dfloaded)\n",
    "    if not os.path.exists('static_data/statistical_historicweather'):\n",
    "        os.makedirs('static_data/statistical_historicweather')\n",
    "    statisticalData.to_csv(\"static_data/statistical_historicweather/statistical_\"+ cantonId +\".csv\")\n",
    "print(\"downloaded historical statistical weather data(%s seconds)\" % (time.time() - start)) \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT run this code again, this code ran on 18.3.2021 and the data was manually moved to a special folder\n",
    "'''\n",
    "for cantonId in weatherDictionary.keys():\n",
    "    cityId = str(weatherDictionary[cantonId])\n",
    "\n",
    "    # create new data frame for each canton\n",
    "    weather = pd.DataFrame(columns=['dt', 'weather', 'main.temp', 'main.feels_like', 'main.pressure',\n",
    "           'main.humidity', 'main.temp_min', 'main.temp_max', 'wind.speed',\n",
    "           'wind.deg', 'clouds.all', 'rain.1h'])\n",
    "\n",
    "    # can only get one week for one call\n",
    "    startDate = datetime.datetime(2020, 3, 19)\n",
    "    endDate = datetime.datetime.today()\n",
    "    for week in pd.date_range(start=startDate, end=endDate, freq='W-THU'):\n",
    "        unixTimeUTCstart = int(week.replace(tzinfo=timezone.utc).timestamp())\n",
    "        unixTimeUTCend = int(endDate.replace(tzinfo=timezone.utc).timestamp())\n",
    "        apiCall = \"http://history.openweathermap.org/data/2.5/history/city?id=\"+cityId+\"&type=hour&start=\"+str(unixTimeUTCstart)+\"&end=\"+str(unixTimeUTCend)+\"&appid=\"+apiKey\n",
    "        with urllib.request.urlopen(apiCall) as url:\n",
    "            data = json.loads(url.read().decode())\n",
    "            dfloaded = pd.json_normalize(data[\"list\"])\n",
    "            weather = weather.append(dfloaded, ignore_index=True)\n",
    "\n",
    "    # remove some duplicates (first entry overlaps)\n",
    "    weather.drop_duplicates(subset=['dt'])\n",
    "    # transform unix time to datetime\n",
    "    weather[\"dt\"] = weather[\"dt\"].apply(lambda x: datetime.datetime.utcfromtimestamp(x).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    if not os.path.exists('static_data/historicweather'):\n",
    "        os.makedirs('static_data/historicweather')\n",
    "    weather.to_csv(\"static_data/historicweather/\"+ cantonId +\".csv\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# historic weather update (load recently historic weather data) (from 4.2.21 because of weekly fetches)\n",
    "for cantonId in weatherDictionary.keys():\n",
    "    cityId = str(weatherDictionary[cantonId])\n",
    "\n",
    "    # create new data frame for each canton\n",
    "    weather = pd.DataFrame(columns=['dt', 'weather', 'main.temp', 'main.feels_like', 'main.pressure',\n",
    "           'main.humidity', 'main.temp_min', 'main.temp_max', 'wind.speed',\n",
    "           'wind.deg', 'clouds.all', 'rain.1h'])\n",
    "\n",
    "    # can only get one week for one call\n",
    "    startDate = datetime.datetime(2021, 2, 1)\n",
    "    endDate = datetime.datetime.today()\n",
    "    for week in pd.date_range(start=startDate, end=endDate, freq='W-THU'):\n",
    "        unixTimeUTCstart = int(week.replace(tzinfo=timezone.utc).timestamp())\n",
    "        unixTimeUTCend = int(endDate.replace(tzinfo=timezone.utc).timestamp())\n",
    "        apiCall = \"http://history.openweathermap.org/data/2.5/history/city?id=\"+cityId+\"&type=hour&start=\"+str(unixTimeUTCstart)+\"&end=\"+str(unixTimeUTCend)+\"&appid=\"+apiKey\n",
    "        with urllib.request.urlopen(apiCall) as url:\n",
    "            data = json.loads(url.read().decode())\n",
    "            dfloaded = pd.json_normalize(data[\"list\"])\n",
    "            weather = weather.append(dfloaded, ignore_index=True)\n",
    "\n",
    "    # remove some duplicates (first entry overlaps)\n",
    "    weather.drop_duplicates(subset=['dt'])\n",
    "    # transform unix time to datetime\n",
    "    weather[\"dt\"] = weather[\"dt\"].apply(lambda x: datetime.datetime.utcfromtimestamp(x).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    if not os.path.exists('data/historicweatherupdate'):\n",
    "        os.makedirs('data/historicweatherupdate')\n",
    "    weather.to_csv(\"data/historicweatherupdate/\"+ cantonId +\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'backups/backup-2021-04-06-15-50-46'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a backup of the data we just loaded\n",
    "if not os.path.exists('backups'):\n",
    "    os.makedirs('backups')\n",
    "now = datetime.datetime.now()\n",
    "backupname = now.strftime(\"backup-%Y-%m-%d-%H-%M-%S\")\n",
    "shutil.copytree('data', 'backups/'+backupname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets the 16 day weather forecast\n",
    "for cantonId in weatherDictionary.keys():\n",
    "    cityId = str(weatherDictionary[cantonId])\n",
    "    forecastData = pd.DataFrame()\n",
    "    apiCall = \"https://api.openweathermap.org/data/2.5/forecast/daily?id=\"+cityId+\"&cnt=16&appid=\"+apiKey\n",
    "    with urllib.request.urlopen(apiCall) as url:\n",
    "        data = json.loads(url.read().decode())\n",
    "        forecastData = pd.json_normalize(data[\"list\"])\n",
    "\n",
    "    if not os.path.exists('data/weatherforecast'):\n",
    "        os.makedirs('data/weatherforecast')\n",
    "    forecastData.to_csv(\"data/weatherforecast/\"+ cantonId +\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"  \\n# Oxford COVID-19 Government Response Tracker\\ndf = pd.read_csv('https://raw.githubusercontent.com/OxCGRT/covid-policy-tracker/master/data/OxCGRT_latest.csv')\\nif not os.path.exists('data/Oxford'):\\n    os.makedirs('data/Oxford')\\ndf.to_csv('data/Oxford/OxfordStrigencyIndex.csv')\\n\\n# International data for bordering countries (only country level data)\\ndf = pd.read_csv('https://covid.ourworldindata.org/data/owid-covid-data.csv')\\nif not os.path.exists('data/OWID'):\\n    os.makedirs('data/OWID')\\ndf.to_csv('data/OWID/OWIDcoviddata.csv')\\n\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unused data\n",
    "'''  \n",
    "# Oxford COVID-19 Government Response Tracker\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/OxCGRT/covid-policy-tracker/master/data/OxCGRT_latest.csv')\n",
    "if not os.path.exists('data/Oxford'):\n",
    "    os.makedirs('data/Oxford')\n",
    "df.to_csv('data/Oxford/OxfordStrigencyIndex.csv')\n",
    "\n",
    "# International data for bordering countries (only country level data)\n",
    "df = pd.read_csv('https://covid.ourworldindata.org/data/owid-covid-data.csv')\n",
    "if not os.path.exists('data/OWID'):\n",
    "    os.makedirs('data/OWID')\n",
    "df.to_csv('data/OWID/OWIDcoviddata.csv')\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
