{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, urllib.request, json\n",
    "import pandas as pd\n",
    "import shutil\n",
    "from io import BytesIO\n",
    "from urllib.request import urlopen\n",
    "from zipfile import ZipFile\n",
    "from icalendar import Calendar\n",
    "import datetime\n",
    "from datetime import date, timedelta, timezone\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "cantonKeys = ['AG','AI','AR', 'BE', 'BL', 'BS', 'FR', 'GE', 'GL', 'GR', 'JU', 'LU', 'NE', 'NW', 'OW', 'SG', 'SH', 'SO', 'SZ', 'TG', 'TI', 'UR', 'VD', 'VS', 'ZG','ZH']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'backups/backup-2021-03-18-16-20-00'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# create dictionary\n",
    "\n",
    "\n",
    "# this code section loads all needed data to a local folder and immediately creates a backup\n",
    "\n",
    "# get newest available string to download FOPH data\n",
    "zipurl = ''\n",
    "with urllib.request.urlopen(\"https://www.covid19.admin.ch/api/data/context\") as url:\n",
    "    data = json.loads(url.read().decode())\n",
    "    zipurl = data[\"sources\"][\"zip\"][\"csv\"]\n",
    "\n",
    "# download the FOPH data (use this data also for the virus variants)\n",
    "with urlopen(zipurl) as zipresp:\n",
    "    with ZipFile(BytesIO(zipresp.read())) as zfile:\n",
    "        zfile.extractall('data/FOPH')\n",
    "\n",
    "# download the Google Mobility data\n",
    "zipurl = 'https://www.gstatic.com/covid19/mobility/Region_Mobility_Report_CSVs.zip'\n",
    "with urlopen(zipurl) as zipresp:\n",
    "    with ZipFile(BytesIO(zipresp.read())) as zfile:\n",
    "        zfile.extractall('data/GoogleMobility')\n",
    "\n",
    "# download the Intervista Mobility data\n",
    "zipurl = 'https://www.intervista.ch/media/2020/03/Download_Mobilit%C3%A4ts-Monitoring_Covid-19.zip'\n",
    "with urlopen(zipurl) as zipresp:\n",
    "    with ZipFile(BytesIO(zipresp.read())) as zfile:\n",
    "        zfile.extractall('data/IntervistaMobility')\n",
    "\n",
    "# KOF strigency index data\n",
    "df = pd.read_csv('https://datenservice.kof.ethz.ch/api/v1/public/sets/stringency_plus_web?mime=csv&df=Y-m-d')\n",
    "if not os.path.exists('data/KOF'):\n",
    "    os.makedirs('data/KOF')\n",
    "df.to_csv('data/KOF/KOFStrigencyIndex.csv')\n",
    "      \n",
    "# Oxford COVID-19 Government Response Tracker\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/OxCGRT/covid-policy-tracker/master/data/OxCGRT_latest.csv')\n",
    "if not os.path.exists('data/Oxford'):\n",
    "    os.makedirs('data/Oxford')\n",
    "df.to_csv('data/Oxford/OxfordStrigencyIndex.csv')\n",
    "\n",
    "# TODO: weather (open weather api key requested, but i will only get past data maximum one year back)\n",
    "\n",
    "# ==================== THE MANUAL WORK ==================\n",
    "holydayVacationTable = pd.DataFrame(index=pd.date_range(start='1/1/2020', end='31/12/2021'), columns = cantonKeys)\n",
    "holydayVacationTable[cantonKeys] = 0\n",
    "\n",
    "\n",
    "def fillCantonHolidays(cantonKey, filename):\n",
    "    # extract dates from file and fill in all special holidays\n",
    "    file = open(filename, 'rb')\n",
    "    cal = Calendar.from_ical(file.read())\n",
    "    for e in cal.walk('vevent'):\n",
    "        start = e['DTSTART'].to_ical().decode('utf-8')\n",
    "        parsedDate = datetime.datetime.strptime(start, '%Y%m%d')\n",
    "        holydayVacationTable[cantonKey][parsedDate] = 1  \n",
    "    \n",
    "\n",
    "def fillCantonVacation(cantonKey, filename):\n",
    "    # extract dates from file\n",
    "    file = open(filename, 'rb')\n",
    "    cal = Calendar.from_ical(file.read())\n",
    "    for e in cal.walk('vevent'):\n",
    "        startDate = e['DTSTART'].to_ical().decode('utf-8')\n",
    "        endDate = e['DTEND'].to_ical().decode('utf-8')\n",
    "        parsedStartDate = datetime.datetime.strptime(startDate, '%Y%m%d')\n",
    "        parsedEndDate = datetime.datetime.strptime(endDate, '%Y%m%d') \n",
    "        if parsedEndDate > datetime.datetime(2021, 12, 31):\n",
    "            parsedEndDate = datetime.datetime(2021, 12, 31)\n",
    "        r = pd.date_range(start=parsedStartDate, end=parsedEndDate)\n",
    "        holydayVacationTable[cantonKey][r] = 1\n",
    "\n",
    "        \n",
    "# special mapping for holiday and vacation file names\n",
    "vacHolyDictionary = dict(zip(cantonKeys, ['aargau','appenzell-innerrhoden','appenzell-ausserrhoden','bern','basel-land','basel-stadt','freiburg','genf','glarus','graubuenden','jura','luzern','neuenburg','nidwalden','obwalden','sankt-gallen','schaffhausen','solothurn','schwyz','thurgau','tessin','uri','waadt','wallis','zug','zuerich']))       \n",
    "for c in cantonKeys:\n",
    "    # fill the canton holidays which are only single days\n",
    "    for p in ['static_data/holidays/2020/', 'static_data/holidays/2021/']:\n",
    "        matches = [match for match in os.listdir(p) if vacHolyDictionary[c] in match]\n",
    "        filename = matches[0]\n",
    "        path = p + filename\n",
    "        fillCantonHolidays(c, path)\n",
    "       \n",
    "    # fill the school vacations which have a start and end date\n",
    "    for p in ['static_data/vacations/2020/', 'static_data/vacations/2021/']:\n",
    "        matches = [match for match in os.listdir(p) if vacHolyDictionary[c] in match]\n",
    "        filename = matches[0]\n",
    "        path = p + filename\n",
    "        fillCantonVacation(c, path)\n",
    "    \n",
    "# offset = 6 gets all sundays, offset 5 all saturdays\n",
    "def getDays(year, offset):\n",
    "   d = date(year, 1, 1)                    \n",
    "   d += timedelta(days = offset - d.weekday())  \n",
    "   while d.year == year:\n",
    "      yield d\n",
    "      d += timedelta(days = 7)\n",
    "\n",
    "listOfSaturdaysSundays = []\n",
    "for year in [2020,2021]:\n",
    "    for weekday in [5,6]:\n",
    "        for day in getDays(year, weekday):\n",
    "           listOfSaturdaysSundays.append(day)\n",
    "\n",
    "\n",
    "for e in listOfSaturdaysSundays:\n",
    "    holydayVacationTable.loc[e]=1\n",
    "\n",
    "\n",
    "if not os.path.exists('data/HolidayVacation'):\n",
    "    os.makedirs('data/HolidayVacation')\n",
    "holydayVacationTable.to_csv('data/HolidayVacation/HolidayVacation.csv')\n",
    "# ========================================================\n",
    "\n",
    "\n",
    "# International data for bordering countries (only weekly because germany, also earliest data point is from week 13 2020)\n",
    "df = pd.read_csv('https://opendata.ecdc.europa.eu/covid19/subnationalcaseweekly/csv')\n",
    "if not os.path.exists('data/ECDC'):\n",
    "    os.makedirs('data/ECDC')\n",
    "df.to_csv('data/ECDC/ECDCsubnationalcaseweekly.csv')\n",
    "\n",
    "# International data for bordering countries (only country level data)\n",
    "df = pd.read_csv('https://covid.ourworldindata.org/data/owid-covid-data.csv')\n",
    "if not os.path.exists('data/OWID'):\n",
    "    os.makedirs('data/OWID')\n",
    "df.to_csv('data/OWID/OWIDcoviddata.csv')\n",
    "\n",
    "#======================== construct the federal measures ================\n",
    "columnNames = pd.read_excel('static_data/measures/measures.xlsx', sheet_name = 'Federal').columns\n",
    "measures = pd.read_excel('static_data/measures/measures.xlsx', sheet_name = 'Federal', skiprows=6, names=columnNames)\n",
    "\n",
    "measures = measures.set_index('Time')\n",
    "\n",
    "for day in pd.date_range(start=datetime.datetime(2020, 1, 1), end=datetime.datetime(2021, 12, 31)):\n",
    "    if not day in measures.index:\n",
    "        measures.loc[day] = [float('NaN')] * len(measures.columns)\n",
    "\n",
    "measures = measures.sort_index()\n",
    "\n",
    "# propagate the update changes to all other days\n",
    "for j in measures.columns: #measure\n",
    "    dailyMeasureLevel = 0\n",
    "    for i in measures.index: #day\n",
    "        if math.isnan(measures.loc[i, j]):\n",
    "            measures.loc[i, j] = dailyMeasureLevel\n",
    "        else:\n",
    "            dailyMeasureLevel = measures.loc[i, j]\n",
    "\n",
    "'''\n",
    "# plotting federal measures\n",
    "for m in measures.columns:\n",
    "    measures.plot(kind='line', y=m, figsize=(15,15))\n",
    "    plt.show()\n",
    "'''\n",
    "if not os.path.exists('data/measures'):\n",
    "    os.makedirs('data/measures')\n",
    "measures.to_csv('data/measures/federal.csv')\n",
    "\n",
    "#======================== construct the cantonal measures ================\n",
    "for c in cantonKeys: \n",
    "    # copy the federal measures and use max function (with some exceptions)\n",
    "    cantMeasuresComplete = measures.copy()\n",
    "    cantMeasures = pd.read_excel('static_data/measures/measures.xlsx', sheet_name = c)\n",
    "\n",
    "    cantMeasures = cantMeasures.set_index('Time')\n",
    "\n",
    "    for day in pd.date_range(start=datetime.datetime(2020, 1, 1), end=datetime.datetime(2021, 12, 31)):\n",
    "        if not day in cantMeasures.index:\n",
    "            cantMeasures.loc[day] = [float('NaN')] * len(cantMeasures.columns)\n",
    "\n",
    "    cantMeasures = cantMeasures.sort_index()\n",
    "\n",
    "\n",
    "    for m in cantMeasures.columns:\n",
    "        dailyMeasureLevel = float('NaN')\n",
    "        for day in cantMeasures.index:\n",
    "            if not math.isnan(cantMeasures.loc[day,m]):\n",
    "                if cantMeasures.loc[day,m] != -1:\n",
    "                    dailyMeasureLevel = cantMeasures.loc[day,m]\n",
    "                else:\n",
    "                    cantMeasures.loc[day,m] = 0\n",
    "                    dailyMeasureLevel = float('NaN')\n",
    "            elif math.isnan(cantMeasures.loc[day,m]) and not math.isnan(dailyMeasureLevel):\n",
    "                cantMeasures.loc[day,m] = dailyMeasureLevel\n",
    "            else:\n",
    "                cantMeasures.loc[day,m] = 0\n",
    "\n",
    "    # from 22.12 until 9.1 cantonal measures are stronger than federal for restaurants, recreational, sport facilities\n",
    "    for m in cantMeasures.columns:\n",
    "        for day in cantMeasures.index:\n",
    "            if (day < datetime.datetime(2021, 1, 9) and day >= datetime.datetime(2020, 12, 22)  and (m == 'Restaurants' or m=='Cultural, entertainment and recreational facilities' or m=='Sport/Wellness facilities')) or (day >= datetime.datetime(2020, 12, 23) and day < datetime.datetime(2021, 1, 3) and m=='Gatherings/private events'):\n",
    "                # cantonal exeption possible\n",
    "                if cantMeasures.loc[day,m] != 0:\n",
    "                    cantMeasuresComplete.loc[day,m] = cantMeasures.loc[day,m]\n",
    "            else:\n",
    "                cantMeasuresComplete.loc[day,m] = max(cantMeasuresComplete.loc[day,m], cantMeasures.loc[day,m])\n",
    "    \n",
    "    if not os.path.exists('data/measures'):\n",
    "        os.makedirs('data/measures')\n",
    "    cantMeasuresComplete.to_csv('data/measures/'+c+'.csv')\n",
    "\n",
    "'''            \n",
    "# plotting selected canton against federal measures         \n",
    "c = []\n",
    "for m in cantMeasuresComplete.columns:\n",
    "    c.append(\"cant\"+m)            \n",
    "cantMeasuresComplete.columns = c\n",
    "result = pd.concat([cantMeasuresComplete, measures], axis=1, join=\"inner\")\n",
    "\n",
    "for m in measures.columns:\n",
    "    result.plot(kind='line', y=[m,\"cant\"+m], figsize=(10,10))\n",
    "    #cantMeasures.reset_index().plot.scatter(x = 'Time', y = m, figsize=(15,15))\n",
    "    plt.show()\n",
    "'''\n",
    "\n",
    "# create a backup of the data we just loaded\n",
    "if not os.path.exists('backups'):\n",
    "    os.makedirs('backups')\n",
    "now = datetime.datetime.now()\n",
    "backupname = now.strftime(\"backup-%Y-%m-%d-%H-%M-%S\")\n",
    "shutil.copytree('data', 'backups/'+backupname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# special mapping for weather API\n",
    "#['AG','AI','AR', 'BE', 'BL', 'BS', 'FR', 'GE', 'GL', 'GR', 'JU', 'LU', 'NE', 'NW', 'OW', 'SG', 'SH', 'SO', 'SZ', 'TG', 'TI', 'UR', 'VD', 'VS', 'ZG','ZH']\n",
    "#[\"Aarau\",\"Appenzell\",\"Herisau\",\"Bern\",\"Liestal\",\"Basel\",\"Fribourg\",\"Geneve\",\"2660594\",\"Chur\",\"Delemont\",\"Luzern\",\"Neuchatel\",\"Stans\",\"Sarnen\",\"Sankt Gallen\", \"Schaffhausen\", \"Olten\",\"Schwyz\",\"Frauenfeld\",\"Bellinzona\",\"2661780\",\"Lausanne\",\"Sion\",\"Zug\",\"Zurich\"]\n",
    "weatherDictionary = dict(zip(cantonKeys,[2661881,2661740,2660365,2661552,2659891,2661604,2660718,2660646,2660594,2661169,2661035,2659811,2659496,2658504,2658786,2658822,2658761,2658564,2658665,2660727,2661567,2661780,2659994,2658576,2657908,2657896]))\n",
    "\n",
    "apiKey = \"0077c15de8e01960cc024a8b11751ead\"\n",
    "\n",
    "for cantonId in weatherDictionary.keys():\n",
    "    cityId = str(weatherDictionary[cantonId])\n",
    "\n",
    "    # create new data frame for each canton\n",
    "    weather = pd.DataFrame(columns=['dt', 'weather', 'main.temp', 'main.feels_like', 'main.pressure',\n",
    "           'main.humidity', 'main.temp_min', 'main.temp_max', 'wind.speed',\n",
    "           'wind.deg', 'clouds.all', 'rain.1h'])\n",
    "\n",
    "    # can only get one week for one call\n",
    "    startDate = datetime.datetime(2020, 3, 19)\n",
    "    endDate = datetime.datetime.today()\n",
    "    for week in pd.date_range(start=startDate, end=endDate, freq='W-THU'):\n",
    "        unixTimeUTCstart = int(week.replace(tzinfo=timezone.utc).timestamp())\n",
    "        unixTimeUTCend = int(endDate.replace(tzinfo=timezone.utc).timestamp())\n",
    "        apiCall = \"http://history.openweathermap.org/data/2.5/history/city?id=\"+cityId+\"&type=hour&start=\"+str(unixTimeUTCstart)+\"&end=\"+str(unixTimeUTCend)+\"&appid=\"+apiKey\n",
    "        with urllib.request.urlopen(apiCall) as url:\n",
    "            data = json.loads(url.read().decode())\n",
    "            dfloaded = pd.json_normalize(data[\"list\"])\n",
    "            weather = weather.append(dfloaded, ignore_index=True)\n",
    "\n",
    "    # remove some duplicates (first entry overlaps)\n",
    "    weather.drop_duplicates(subset=['dt'])\n",
    "    # transform unix time to datetime\n",
    "    weather[\"dt\"] = weather[\"dt\"].apply(lambda x: datetime.datetime.utcfromtimestamp(x).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    if not os.path.exists('static_data/historicweather'):\n",
    "        os.makedirs('static_data/historicweather')\n",
    "    weather.to_csv(\"static_data/historicweather/\"+ cantonId +\".csv\")\n",
    "\n",
    "#pd.options.display.max_rows = 999\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============== this script has only to run once and never again (and it did) ==================\n",
    "for cantonId in weatherDictionary.keys():\n",
    "    cityId = str(weatherDictionary[cantonId])\n",
    "    statisticalData = pd.DataFrame()\n",
    "    for everyDay in pd.date_range(start=datetime.datetime(2020, 1, 1), end=datetime.datetime(2020, 3, 18)):\n",
    "        monthNumber = str(everyDay.month)\n",
    "        dayNumber = str(everyDay.day)\n",
    "        apiCall =\"https://history.openweathermap.org/data/2.5/aggregated/day?id=\"+cityId+\"&month=\"+monthNumber+\"&day=\"+dayNumber+\"&appid=\"+apiKey\n",
    "        with urllib.request.urlopen(apiCall) as url:\n",
    "                    data = json.loads(url.read().decode())\n",
    "                    dfloaded = pd.json_normalize(data[\"result\"])\n",
    "                    statisticalData = statisticalData.append(dfloaded)\n",
    "    if not os.path.exists('static_data/historicweather'):\n",
    "        os.makedirs('static_data/historicweather')\n",
    "    statisticalData.to_csv(\"static_data/historicweather/statistical_\"+ cantonId +\".csv\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets the 16 day weather forecast\n",
    "for cantonId in weatherDictionary.keys():\n",
    "    cityId = str(weatherDictionary[cantonId])\n",
    "    forecastData = pd.DataFrame()\n",
    "    apiCall = \"https://api.openweathermap.org/data/2.5/forecast/daily?id=\"+cityId+\"&cnt=16&appid=\"+apiKey\n",
    "    with urllib.request.urlopen(apiCall) as url:\n",
    "        data = json.loads(url.read().decode())\n",
    "        forecastData = pd.json_normalize(data[\"list\"])\n",
    "\n",
    "    if not os.path.exists('data/weatherforecast'):\n",
    "        os.makedirs('data/weatherforecast')\n",
    "    forecastData.to_csv(\"data/weatherforecast/\"+ cantonId +\".csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
