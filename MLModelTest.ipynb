{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "impressive-tissue",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS FILE WAS USED TO TEST THE SELECTED MODEL ON THE TEST SET\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time\n",
    "import logging\n",
    "from joblib import dump, load\n",
    "from datetime import timedelta\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.multioutput import RegressorChain\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import xgboost as xgb\n",
    "import lightgbm as lightgbm\n",
    "\n",
    "# settings:\n",
    "numberOfInputWeeks = 3 # must be equal to the number of input weeks set in data preperator\n",
    "numberOfOutputWeeks = 4 # must be equal to the number of output week set in data preperator\n",
    "\n",
    "\n",
    "\n",
    "# data preperation\n",
    "cantonKeys = ['AG','AI','AR', 'BE', 'BL', 'BS', 'FR', 'GE', 'GL', 'GR', 'JU', 'LU', 'NE', 'NW', 'OW', 'SG', 'SH', 'SO', 'SZ', 'TG', 'TI', 'UR', 'VD', 'VS', 'ZG','ZH']\n",
    "data = pd.read_csv(\"completedata.csv\")\n",
    "\n",
    "\n",
    "outputCategories = ['case_inz_entries_7dayAverage',\n",
    "                  'hosp_inz_entries_7dayAverage',\n",
    "                  'death_inz_entries_7dayAverage',\n",
    "                  'testPositvity_7dayAverage',\n",
    "                  'transit_stations_percent_change_from_baseline_7dayAverage',\n",
    "                  'workplaces_percent_change_from_baseline_7dayAverage'\n",
    "                   ]\n",
    "numberOfOutputs = len(outputCategories)\n",
    "\n",
    "split = numberOfOutputs * numberOfOutputWeeks + 2\n",
    "\n",
    "train_features = data[(data['category']=='train') | (data['category']=='validation 1') | (data['category']=='validation 2')].iloc[:,0:-split].drop(['lastInputDay'], axis=1)\n",
    "train_labels = data[(data['category']=='train') | (data['category']=='validation 1') | (data['category']=='validation 2')].iloc[:,-split:-2]\n",
    "\n",
    "test1_features = data[data['category']=='test 1'].iloc[:,0:-split].drop(['lastInputDay'], axis=1)\n",
    "test1_labels = data[data['category']=='test 1'].iloc[:,-split:-2]\n",
    "test2_features = data[data['category']=='test 2'].iloc[:,0:-split].drop(['lastInputDay'], axis=1)\n",
    "test2_labels = data[data['category']=='test 2'].iloc[:,-split:-2]\n",
    "test1And2_labels = data[(data['category']=='test 1') | (data['category']=='test 2')].iloc[:,-split:-2]\n",
    "test1And2_features = data[(data['category']=='test 1') | (data['category']=='test 2')].iloc[:,0:-split].drop(['lastInputDay'], axis=1)\n",
    "\n",
    "pip = Pipeline([('minmax_scaler', MinMaxScaler())])\n",
    "X_train = pip.fit_transform(train_features[train_features.columns].values)\n",
    "X_test1 = pip.transform(test1_features[train_features.columns].values)\n",
    "X_test2 = pip.transform(test2_features[train_features.columns].values)\n",
    "X_test1And2 = pip.transform(test1And2_features[train_features.columns].values)\n",
    "\n",
    "\n",
    "    \n",
    "def generic1(learningrate, hiddenLayers, dropout, l1, l2, isMultiOutput):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Dense(hiddenLayers[0], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2), input_shape=X_train.shape[1:]))\n",
    "    model.add(keras.layers.Dropout(rate=dropout))\n",
    "    model.add(keras.layers.Dense(numberOfOutputWeeks) if isMultiOutput else keras.layers.Dense(1))\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.Adam(learning_rate=learningrate))\n",
    "    return model\n",
    "    \n",
    "def generic2(learningrate, hiddenLayers, dropout, l1, l2, isMultiOutput):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Dense(hiddenLayers[0], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2), input_shape=X_train.shape[1:]))\n",
    "    model.add(keras.layers.Dropout(rate=dropout))\n",
    "    model.add(keras.layers.Dense(hiddenLayers[1], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2)))\n",
    "    model.add(keras.layers.Dropout(rate=dropout))\n",
    "    model.add(keras.layers.Dense(numberOfOutputWeeks) if isMultiOutput else keras.layers.Dense(1))\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.Adam(learning_rate=learningrate))\n",
    "    return model\n",
    "\n",
    "def generic3(learningrate, hiddenLayers, dropout, l1, l2, isMultiOutput):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Dense(hiddenLayers[0], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2), input_shape=X_train.shape[1:]))\n",
    "    model.add(keras.layers.Dropout(rate=dropout))\n",
    "    model.add(keras.layers.Dense(hiddenLayers[1], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2)))\n",
    "    model.add(keras.layers.Dropout(rate=dropout))\n",
    "    model.add(keras.layers.Dense(hiddenLayers[2], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2)))\n",
    "    model.add(keras.layers.Dropout(rate=dropout))\n",
    "    model.add(keras.layers.Dense(numberOfOutputWeeks) if isMultiOutput else keras.layers.Dense(1))\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.Adam(learning_rate=learningrate))\n",
    "    return model\n",
    "\n",
    "def generic4(learningrate, hiddenLayers, dropout, l1, l2, isMultiOutput):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Dense(hiddenLayers[0], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2), input_shape=X_train.shape[1:]))\n",
    "    model.add(keras.layers.Dropout(rate=dropout))\n",
    "    model.add(keras.layers.Dense(hiddenLayers[1], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2)))\n",
    "    model.add(keras.layers.Dropout(rate=dropout))\n",
    "    model.add(keras.layers.Dense(hiddenLayers[2], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2)))\n",
    "    model.add(keras.layers.Dropout(rate=dropout))\n",
    "    model.add(keras.layers.Dense(hiddenLayers[3], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2)))\n",
    "    model.add(keras.layers.Dropout(rate=dropout))\n",
    "    model.add(keras.layers.Dense(numberOfOutputWeeks) if isMultiOutput else keras.layers.Dense(1))\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.Adam(learning_rate=learningrate))\n",
    "    return model\n",
    "\n",
    "def generic5(learningrate, hiddenLayers, dropout, l1, l2, isMultiOutput):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Dense(hiddenLayers[0], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2), input_shape=X_train.shape[1:]))\n",
    "    model.add(keras.layers.Dropout(rate=dropout))\n",
    "    model.add(keras.layers.Dense(hiddenLayers[1], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2)))\n",
    "    model.add(keras.layers.Dropout(rate=dropout))\n",
    "    model.add(keras.layers.Dense(hiddenLayers[2], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2)))\n",
    "    model.add(keras.layers.Dropout(rate=dropout))\n",
    "    model.add(keras.layers.Dense(hiddenLayers[3], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2)))\n",
    "    model.add(keras.layers.Dropout(rate=dropout))\n",
    "    model.add(keras.layers.Dense(hiddenLayers[4], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2)))\n",
    "    model.add(keras.layers.Dropout(rate=dropout))\n",
    "    model.add(keras.layers.Dense(numberOfOutputWeeks) if isMultiOutput else keras.layers.Dense(1))\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.Adam(learning_rate=learningrate))\n",
    "    return model\n",
    "\n",
    "def generic6(learningrate, hiddenLayers, dropout, l1, l2, isMultiOutput):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Dense(hiddenLayers[0], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2), input_shape=X_train.shape[1:]))\n",
    "    model.add(keras.layers.Dropout(rate=dropout))\n",
    "    model.add(keras.layers.Dense(hiddenLayers[1], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2)))\n",
    "    model.add(keras.layers.Dropout(rate=dropout))\n",
    "    model.add(keras.layers.Dense(hiddenLayers[2], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2)))\n",
    "    model.add(keras.layers.Dropout(rate=dropout))\n",
    "    model.add(keras.layers.Dense(hiddenLayers[3], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2)))\n",
    "    model.add(keras.layers.Dropout(rate=dropout))\n",
    "    model.add(keras.layers.Dense(hiddenLayers[4], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2)))\n",
    "    model.add(keras.layers.Dropout(rate=dropout))\n",
    "    model.add(keras.layers.Dense(hiddenLayers[5], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2)))\n",
    "    model.add(keras.layers.Dropout(rate=dropout))\n",
    "    model.add(keras.layers.Dense(numberOfOutputWeeks) if isMultiOutput else keras.layers.Dense(1))\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.Adam(learning_rate=learningrate))\n",
    "    return model\n",
    "\n",
    "def generic7(learningrate, hiddenLayers, dropout, l1, l2, isMultiOutput):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Dense(hiddenLayers[0], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2), input_shape=X_train.shape[1:]))\n",
    "    model.add(keras.layers.Dropout(rate=dropout))\n",
    "    model.add(keras.layers.Dense(hiddenLayers[1], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2)))\n",
    "    model.add(keras.layers.Dropout(rate=dropout))\n",
    "    model.add(keras.layers.Dense(hiddenLayers[2], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2)))\n",
    "    model.add(keras.layers.Dropout(rate=dropout))\n",
    "    model.add(keras.layers.Dense(hiddenLayers[3], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2)))\n",
    "    model.add(keras.layers.Dropout(rate=dropout))\n",
    "    model.add(keras.layers.Dense(hiddenLayers[4], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2)))\n",
    "    model.add(keras.layers.Dropout(rate=dropout))\n",
    "    model.add(keras.layers.Dense(hiddenLayers[5], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2)))\n",
    "    model.add(keras.layers.Dropout(rate=dropout))\n",
    "    model.add(keras.layers.Dense(hiddenLayers[6], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2)))\n",
    "    model.add(keras.layers.Dropout(rate=dropout))\n",
    "    model.add(keras.layers.Dense(numberOfOutputWeeks) if isMultiOutput else keras.layers.Dense(1))\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.Adam(learning_rate=learningrate))\n",
    "    return model\n",
    "\n",
    "def generic8(learningrate, hiddenLayers, dropout, l1, l2, isMultiOutput):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Dense(hiddenLayers[0], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2), input_shape=X_train.shape[1:]))\n",
    "    model.add(keras.layers.Dropout(rate=dropout))\n",
    "    model.add(keras.layers.Dense(hiddenLayers[1], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2)))\n",
    "    model.add(keras.layers.Dropout(rate=dropout))\n",
    "    model.add(keras.layers.Dense(hiddenLayers[2], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2)))\n",
    "    model.add(keras.layers.Dropout(rate=dropout))\n",
    "    model.add(keras.layers.Dense(hiddenLayers[3], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2)))\n",
    "    model.add(keras.layers.Dropout(rate=dropout))\n",
    "    model.add(keras.layers.Dense(hiddenLayers[4], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2)))\n",
    "    model.add(keras.layers.Dropout(rate=dropout))\n",
    "    model.add(keras.layers.Dense(hiddenLayers[5], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2)))\n",
    "    model.add(keras.layers.Dropout(rate=dropout))\n",
    "    model.add(keras.layers.Dense(hiddenLayers[6], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2)))\n",
    "    model.add(keras.layers.Dropout(rate=dropout))\n",
    "    model.add(keras.layers.Dense(hiddenLayers[7], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2)))\n",
    "    model.add(keras.layers.Dropout(rate=dropout))\n",
    "    model.add(keras.layers.Dense(numberOfOutputWeeks) if isMultiOutput else keras.layers.Dense(1))\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.Adam(learning_rate=learningrate))\n",
    "    return model\n",
    "    \n",
    "\n",
    "    \n",
    "def genericKerasModel(learningrate, hiddenLayers, dropout, l1regularization, l2regularization, isMultioutput):\n",
    "    if len(hiddenLayers) == 1:\n",
    "        return generic1(learningrate, hiddenLayers, dropout, l1regularization, l2regularization, isMultioutput)\n",
    "    elif len(hiddenLayers) == 2:\n",
    "        return generic2(learningrate, hiddenLayers, dropout, l1regularization, l2regularization, isMultioutput)\n",
    "    elif len(hiddenLayers) == 3:\n",
    "        return generic3(learningrate, hiddenLayers, dropout, l1regularization, l2regularization, isMultioutput)\n",
    "    elif len(hiddenLayers) == 4:\n",
    "        return generic4(learningrate, hiddenLayers, dropout, l1regularization, l2regularization, isMultioutput)\n",
    "    elif len(hiddenLayers) == 5:\n",
    "        return generic5(learningrate, hiddenLayers, dropout, l1regularization, l2regularization, isMultioutput)\n",
    "    elif len(hiddenLayers) == 6:\n",
    "        return generic6(learningrate, hiddenLayers, dropout, l1regularization, l2regularization, isMultioutput)\n",
    "    elif len(hiddenLayers) == 7:\n",
    "        return generic7(learningrate, hiddenLayers, dropout, l1regularization, l2regularization, isMultioutput)\n",
    "    else:\n",
    "        return generic8(learningrate, hiddenLayers, dropout, l1regularization, l2regularization, isMultioutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "compressed-deputy",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-8b0d05e8db7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    241\u001b[0m         estimator = linear_model.MultiTaskLasso(alpha = estimators[estimatorId][\"alpha\"],\n\u001b[1;32m    242\u001b[0m                                                tol = estimators[estimatorId][\"tol\"])\n\u001b[0;32m--> 243\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m         \u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"models/trainedOnTrainingAndValidationData/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimatorId\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mestimators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mestimatorId\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"task\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_sklearn.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconstructResults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mestimatorId\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"task\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1950\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdual_gap_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter_\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1951\u001b[0;31m             cd_fast.enet_coordinate_descent_multi_task(\n\u001b[0m\u001b[1;32m   1952\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml1_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml2_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1953\u001b[0m                 check_random_state(self.random_state), random)\n",
      "\u001b[0;32msklearn/linear_model/_cd_fast.pyx\u001b[0m in \u001b[0;36msklearn.linear_model._cd_fast.enet_coordinate_descent_multi_task\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     12\u001b[0m ]\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mset_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'numpy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \"\"\"Convert the input to an array.\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "estimators = []\n",
    "numberOfEstimators = 0\n",
    "results = pd.DataFrame()\n",
    "\n",
    "'''\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"jobnr\", help=\"computes job\",type=int)\n",
    "args = parser.parse_args()\n",
    "jobnr = args.jobnr\n",
    "'''\n",
    "jobnr = 99\n",
    "\n",
    "\n",
    "# Test\n",
    "\n",
    "# 'case_inz_entries_7dayAverage'\n",
    "estimators.append({\n",
    "                      \"modelClass\": \"MultiTaskLasso\",\n",
    "                      \"isMultiWeek\": True,\n",
    "                      \"alpha\": 0.7,\n",
    "                      \"tol\": 1e-05,\n",
    "                      \"task\":'case_inz_entries_7dayAverage',\n",
    "                  }\n",
    "                )\n",
    "\n",
    "\n",
    "# 'hosp_inz_entries_7dayAverage'\n",
    "estimators.append({\n",
    "                      \"modelClass\": \"MultiTaskLasso\",\n",
    "                      \"isMultiWeek\": True,\n",
    "                      \"alpha\": 0.014,\n",
    "                      \"tol\": 0.0001,\n",
    "                      \"task\":'hosp_inz_entries_7dayAverage',\n",
    "                  }\n",
    "                )\n",
    "\n",
    "\n",
    "# 'death_inz_entries_7dayAverage'\n",
    "estimators.append({\n",
    "                      \"modelClass\": \"Lasso\",\n",
    "                      \"isMultiWeek\": True,\n",
    "                      \"tol\": 0.0001,\n",
    "                      \"alpha\": 0.0009,\n",
    "                      \"task\":'death_inz_entries_7dayAverage',\n",
    "                  }\n",
    "                )\n",
    "\n",
    "\n",
    "\n",
    "# 'testPositvity_7dayAverageBoth'\n",
    "estimators.append({\n",
    "                      \"modelClass\": \"Keras\",\n",
    "                      \"isMultiWeek\": True,\n",
    "                      \"hiddenLayers\": [350, 175, 85],\n",
    "                      \"numberOfhiddenLayers\": 3,\n",
    "                      \"isTwoWay\": False,\n",
    "                      \"l1reg\": 0,\n",
    "                      \"alpha\": 0.0001,\n",
    "                      \"dropoutValue\": 0,\n",
    "                      \"learningRate\": 0.0001,\n",
    "                      \"task\":'testPositvity_7dayAverage',\n",
    "                      \"epochs\":73\n",
    "                  }\n",
    "                )\n",
    "\n",
    "\n",
    "# 'workplaces_percent_change_from_baseline_7dayAverage'\n",
    "estimators.append({\n",
    "                      \"modelClass\": \"RandomForrest\",\n",
    "                      \"isMultiWeek\": True,\n",
    "                      \"n_estimators\": 1000,\n",
    "                      \"max_features\": 'auto',\n",
    "                      \"min_samples_leaf\": 8,\n",
    "                      \"task\":'workplaces_percent_change_from_baseline_7dayAverage',\n",
    "                  }\n",
    "                )\n",
    "\n",
    "# 'transit_stations_percent_change_from_baseline_7dayAverage'\n",
    "estimators.append({\n",
    "                      \"modelClass\": \"Keras\",\n",
    "                      \"isMultiWeek\": True,\n",
    "                      \"hiddenLayers\": [500, 250],\n",
    "                      \"numberOfhiddenLayers\": 2,\n",
    "                      \"isTwoWay\": False,\n",
    "                      \"l1reg\": 0.001,\n",
    "                      \"alpha\": 0.0001,\n",
    "                      \"dropoutValue\": 0.5,\n",
    "                      \"learningRate\": 0.0001,\n",
    "                      \"task\":'transit_stations_percent_change_from_baseline_7dayAverage',\n",
    "                      \"epochs\":94\n",
    "                  }\n",
    "                )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# constructs and saves the results of a fitted estimator    \n",
    "def constructResults(estimator, task, numberOfRanEpochs):\n",
    "    # predictions for all weeks\n",
    "    predictions1 = pd.DataFrame(estimator.predict(X_test1), index=test1_labels.index, columns=[\"pred_week_\"+task+\"_\"+str(outputWeekNumber) for outputWeekNumber in range(0,numberOfOutputWeeks)]) \n",
    "    predictions2 = pd.DataFrame(estimator.predict(X_test2), index=test2_labels.index, columns=[\"pred_week_\"+task+\"_\"+str(outputWeekNumber) for outputWeekNumber in range(0,numberOfOutputWeeks)])\n",
    "\n",
    "    # test for all weeks\n",
    "    y_test1 = test1_labels[[\"output_\"+task+\"_\"+str(outputWeekNumber) for outputWeekNumber in range(0,numberOfOutputWeeks)]]\n",
    "    y_test2 = test2_labels[[\"output_\"+task+\"_\"+str(outputWeekNumber) for outputWeekNumber in range(0,numberOfOutputWeeks)]]\n",
    "\n",
    "    resultsDf = pd.DataFrame()\n",
    "    # compute and safe results for every week\n",
    "    for outputWeekNumber in range(0,numberOfOutputWeeks):\n",
    "        # compute root mean squared error for test sets\n",
    "        rmse1 = np.sqrt(mean_squared_error(predictions1[\"pred_week_\"+task+\"_\"+str(outputWeekNumber)], y_test1[\"output_\"+task+\"_\"+str(outputWeekNumber)]))\n",
    "        rmse2 = np.sqrt(mean_squared_error(predictions2[\"pred_week_\"+task+\"_\"+str(outputWeekNumber)], y_test2[\"output_\"+task+\"_\"+str(outputWeekNumber)]))\n",
    "        # safe the results and all model parameters\n",
    "        res = {}\n",
    "        if estimators[estimatorId][\"modelClass\"] == \"Keras\":\n",
    "            res = {'modelId':[generateModelId(estimators[estimatorId])],\n",
    "                  'modelClass':[estimators[estimatorId][\"modelClass\"]],\n",
    "                 'task':[task],\n",
    "                 'week':[outputWeekNumber], \n",
    "                 'model rmse 1':[rmse1], \n",
    "                 'model rmse 2':[rmse2],\n",
    "                 'isMultiWeek':[estimators[estimatorId][\"isMultiWeek\"]],\n",
    "                 \"hiddenLayers\":[estimators[estimatorId][\"hiddenLayers\"]],\n",
    "                 \"numberOfhiddenLayers\":[len(estimators[estimatorId][\"hiddenLayers\"])],\n",
    "                 \"isTwoWay\": [estimators[estimatorId][\"isTwoWay\"]],\n",
    "                 \"l1reg\": [estimators[estimatorId][\"l1reg\"]],\n",
    "                 \"alpha\": [estimators[estimatorId][\"alpha\"]],\n",
    "                 \"dropoutValue\":[estimators[estimatorId][\"dropoutValue\"]],\n",
    "                 \"learningRate\":[estimators[estimatorId][\"learningRate\"]],\n",
    "                 \"numberOfRanEpochs\": [numberOfRanEpochs]\n",
    "                }\n",
    "        elif estimators[estimatorId][\"modelClass\"] == \"RandomForrest\":\n",
    "            res = {'modelId':[generateModelId(estimators[estimatorId])],\n",
    "                   'modelClass':[estimators[estimatorId][\"modelClass\"]],\n",
    "                 'task':[task],\n",
    "                 'week':[outputWeekNumber], \n",
    "                 'model rmse 1':[rmse1], \n",
    "                 'model rmse 2':[rmse2],\n",
    "                 'isMultiWeek':[estimators[estimatorId][\"isMultiWeek\"]],\n",
    "                 \"n_estimators\": [estimators[estimatorId][\"n_estimators\"]],\n",
    "                 \"max_features\": [estimators[estimatorId][\"max_features\"]],\n",
    "                 \"min_samples_leaf\": [estimators[estimatorId][\"min_samples_leaf\"]]\n",
    "                }               \n",
    "        \n",
    "        elif estimators[estimatorId][\"modelClass\"] == \"Lasso\":\n",
    "            res = {'modelId':[generateModelId(estimators[estimatorId])],\n",
    "                   'modelClass':[estimators[estimatorId][\"modelClass\"]],\n",
    "                 'task':[task],\n",
    "                 'week':[outputWeekNumber], \n",
    "                 'model rmse 1':[rmse1], \n",
    "                 'model rmse 2':[rmse2],\n",
    "                 'isMultiWeek':[estimators[estimatorId][\"isMultiWeek\"]],\n",
    "                 \"alpha\": [estimators[estimatorId][\"alpha\"]],\n",
    "                 \"tol\": [estimators[estimatorId][\"tol\"]]\n",
    "                }\n",
    "        elif estimators[estimatorId][\"modelClass\"] == \"MultiTaskLasso\":\n",
    "            res = {'modelId':[generateModelId(estimators[estimatorId])],\n",
    "                   'modelClass':[estimators[estimatorId][\"modelClass\"]],\n",
    "                 'task':[task],\n",
    "                 'week':[outputWeekNumber], \n",
    "                 'model rmse 1':[rmse1], \n",
    "                 'model rmse 2':[rmse2],\n",
    "                 'isMultiWeek':[estimators[estimatorId][\"isMultiWeek\"]],\n",
    "                 \"alpha\": [estimators[estimatorId][\"alpha\"]],\n",
    "                 \"tol\": [estimators[estimatorId][\"tol\"]]\n",
    "                }\n",
    "        else:\n",
    "            raise ValueError('Tried to save results for an unsupported estimator')\n",
    "        resultsDf = resultsDf.append(pd.DataFrame(data=res), ignore_index = True)\n",
    "    return resultsDf\n",
    "   \n",
    "\n",
    "        \n",
    "def generateModelId(dictionary):\n",
    "    modelId = \"\"\n",
    "    for key in dictionary.keys():\n",
    "        modelId = modelId + key +\"=\"+ str(dictionary[key]) +\"/\"\n",
    "    modelId = modelId[0:-1]\n",
    "    return modelId    \n",
    "    \n",
    "\n",
    "# training\n",
    "\n",
    "\n",
    "for estimatorId in range(0,len(estimators)):\n",
    "\n",
    "    # get train labels for all weeks\n",
    "    y_train = train_labels[[\"output_\"+estimators[estimatorId][\"task\"]+\"_\"+str(outputWeekNumber) for outputWeekNumber in range(0,numberOfOutputWeeks)]].values\n",
    "    if estimators[estimatorId][\"modelClass\"] == \"Keras\":\n",
    "        # get test labels for all weeks (used for early stopping)\n",
    "        y_test1And2 = test1And2_labels[[\"output_\"+estimators[estimatorId][\"task\"]+\"_\"+str(outputWeekNumber) for outputWeekNumber in range(0,numberOfOutputWeeks)]].values \n",
    "        # construct Keras model\n",
    "\n",
    "        estimator = genericKerasModel(\n",
    "            estimators[estimatorId][\"learningRate\"],\n",
    "            estimators[estimatorId][\"hiddenLayers\"],\n",
    "            estimators[estimatorId][\"dropoutValue\"],\n",
    "            estimators[estimatorId][\"l1reg\"],\n",
    "            estimators[estimatorId][\"alpha\"],\n",
    "            estimators[estimatorId][\"isMultiWeek\"]\n",
    "        )\n",
    "\n",
    "        # fit Keras model\n",
    "        history = estimator.fit(X_train, \n",
    "          y_train, \n",
    "          batch_size=32, \n",
    "          epochs=estimators[estimatorId][\"epochs\"], \n",
    "          verbose=0)\n",
    "        \n",
    "        \n",
    "        if not os.path.exists('models/trainedOnTrainingAndValidationData/'):\n",
    "            os.makedirs('models')\n",
    "            os.makedirs('models/trainedOnTrainingAndValidationData/')\n",
    "        estimator.save(\"models/trainedOnTrainingAndValidationData/\"+str(jobnr)+\"_\"+estimators[estimatorId][\"task\"])\n",
    "        \n",
    "        \n",
    "        numberOfRanEpochs = len(history.history['loss'])\n",
    "        results = results.append(constructResults(estimator, estimators[estimatorId][\"task\"], numberOfRanEpochs), ignore_index = True)\n",
    "\n",
    "    elif estimators[estimatorId][\"modelClass\"] == \"RandomForrest\":\n",
    "        estimator = RandomForestRegressor(n_estimators=estimators[estimatorId][\"n_estimators\"], \n",
    "                                          max_features=estimators[estimatorId][\"max_features\"],\n",
    "                                          min_samples_leaf=estimators[estimatorId][\"min_samples_leaf\"],\n",
    "                                          n_jobs=-1\n",
    "                                         )\n",
    "        estimator.fit(X_train,y_train)\n",
    "        dump(estimator, \"models/trainedOnTrainingAndValidationData/\"+str(estimatorId)+\"_\"+estimators[estimatorId][\"task\"]+\"_sklearn.pkl\")\n",
    "        results = results.append(constructResults(estimator, estimators[estimatorId][\"task\"], -1), ignore_index = True)\n",
    "    elif estimators[estimatorId][\"modelClass\"] == \"Lasso\":\n",
    "        estimator = linear_model.Lasso(alpha=estimators[estimatorId][\"alpha\"],\n",
    "                                               tol = estimators[estimatorId][\"tol\"])\n",
    "        estimator.fit(X_train,y_train)\n",
    "        dump(estimator, \"models/trainedOnTrainingAndValidationData/\"+str(estimatorId)+\"_\"+estimators[estimatorId][\"task\"]+\"_sklearn.pkl\")\n",
    "        results = results.append(constructResults(estimator, estimators[estimatorId][\"task\"], -1), ignore_index = True)\n",
    "    elif estimators[estimatorId][\"modelClass\"] == \"MultiTaskLasso\":\n",
    "        estimator = linear_model.MultiTaskLasso(alpha = estimators[estimatorId][\"alpha\"],\n",
    "                                               tol = estimators[estimatorId][\"tol\"])\n",
    "        estimator.fit(X_train,y_train)\n",
    "        dump(estimator, \"models/trainedOnTrainingAndValidationData/\"+str(estimatorId)+\"_\"+estimators[estimatorId][\"task\"]+\"_sklearn.pkl\")\n",
    "        results = results.append(constructResults(estimator, estimators[estimatorId][\"task\"], -1), ignore_index = True)\n",
    "    else:\n",
    "        raise ValueError('Tried to fit an unsupported estimator')            \n",
    "    \n",
    "\n",
    "\n",
    "results['iteration'] = jobnr\n",
    "    \n",
    "\n",
    "if not os.path.exists('test_results/'):\n",
    "    os.makedirs('test_results')\n",
    "results.to_csv(\"test_results/\"+str(jobnr)+\".csv\", header=True, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standard-immune",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "results = pd.read_csv(\"test_results/\"+str(jobnr)+\".csv\", low_memory=False)\n",
    "\n",
    "for task in outputCategories: # for all output tasks\n",
    "    for outputWeekNumber in range(0,numberOfOutputWeeks): # for every output week\n",
    "\n",
    "        # get the ground truth\n",
    "        groundtruth1 = test1_labels[[\"output_\"+task+\"_\"+str(outputWeekNumber)]]\n",
    "        groundtruth2 = test2_labels[[\"output_\"+task+\"_\"+str(outputWeekNumber)]]\n",
    "\n",
    "        # get the easy baseline (is the same for all output weeks)\n",
    "        last_value_baseline1 = test1_features[[task+\"_last_\"+str(numberOfInputWeeks-1)]]\n",
    "        last_value_baseline2 = test2_features[[task+\"_last_\"+str(numberOfInputWeeks-1)]]\n",
    "\n",
    "        last_value_baseline1_rmse = np.sqrt(mean_squared_error(last_value_baseline1,groundtruth1))\n",
    "        last_value_baseline2_rmse = np.sqrt(mean_squared_error(last_value_baseline2,groundtruth2))\n",
    "\n",
    "\n",
    "        results.loc[(results.task == task) & (results.week == outputWeekNumber),'last value baseline 1 rmse']=last_value_baseline1_rmse\n",
    "        results.loc[(results.task == task) & (results.week == outputWeekNumber),'last value baseline 2 rmse']=last_value_baseline2_rmse\n",
    "\n",
    "results = results[['modelId','task','week','model rmse 1','model rmse 2','last value baseline 1 rmse','last value baseline 2 rmse']]\n",
    "\n",
    "results['week'] = results['week']+1\n",
    "\n",
    "\n",
    "for task in outputCategories:\n",
    "    temp2 = results[results['task']==task]\n",
    "    temp2 = temp2.groupby(['modelId']).sum()\n",
    "    # add relative score\n",
    "    temp2['rel. diff. to last value baseline 1'] = (temp2['last value baseline 1 rmse']-temp2['model rmse 1'])/temp2['last value baseline 1 rmse']\n",
    "    temp2['rel. diff. to last value baseline 2'] = (temp2['last value baseline 2 rmse']-temp2['model rmse 2'])/temp2['last value baseline 2 rmse']\n",
    "    display(temp2)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fundamental-table",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nestimator = load(\\'models/trainedOnTrainingAndValidationData/1_hosp_inz_entries_7dayAverage_sklearn.pkl\\')\\ntask = \\'hosp_inz_entries_7dayAverage\\'\\n\\n\\nestimator = load(\\'models/trainedOnTrainingAndValidationData/2_death_inz_entries_7dayAverage_sklearn.pkl\\')\\ntask = \\'death_inz_entries_7dayAverage\\'\\n\\n                  \\n\\npredictions1 = pd.DataFrame(estimator.predict(X_test1), index=test1_labels.index, columns=[\"pred_week_\"+task+\"_\"+str(outputWeekNumber) for outputWeekNumber in range(0,numberOfOutputWeeks)]) \\npredictions2 = pd.DataFrame(estimator.predict(X_test2), index=test2_labels.index, columns=[\"pred_week_\"+task+\"_\"+str(outputWeekNumber) for outputWeekNumber in range(0,numberOfOutputWeeks)])\\n\\n# test for all weeks\\ny_test1 = test1_labels[[\"output_\"+task+\"_\"+str(outputWeekNumber) for outputWeekNumber in range(0,numberOfOutputWeeks)]]\\ny_test2 = test2_labels[[\"output_\"+task+\"_\"+str(outputWeekNumber) for outputWeekNumber in range(0,numberOfOutputWeeks)]]\\n\\nresultsDf = pd.DataFrame()\\n# compute and safe results for every week\\nfor outputWeekNumber in range(0,numberOfOutputWeeks):\\n    # compute root mean squared error for test sets\\n    rmse1 = np.sqrt(mean_squared_error(predictions1[\"pred_week_\"+task+\"_\"+str(outputWeekNumber)], y_test1[\"output_\"+task+\"_\"+str(outputWeekNumber)]))\\n    rmse2 = np.sqrt(mean_squared_error(predictions2[\"pred_week_\"+task+\"_\"+str(outputWeekNumber)], y_test2[\"output_\"+task+\"_\"+str(outputWeekNumber)]))\\n\\n    \\n    groundtruth1 = test1_labels[[\"output_\"+task+\"_\"+str(outputWeekNumber)]]\\n    groundtruth2 = test2_labels[[\"output_\"+task+\"_\"+str(outputWeekNumber)]]\\n    \\n    temp = pd.DataFrame(predictions1[\"pred_week_\"+task+\"_\"+str(outputWeekNumber)])\\n    temp = temp.join(groundtruth1)\\n    display(temp.plot(kind=\\'box\\', ylabel=\\'7 day case incidence average\\', figsize=(10,5)))\\n\\n    # get the easy baseline (is the same for all output weeks)\\n    last_value_baseline1 = test1_features[[task+\"_last_\"+str(numberOfInputWeeks-1)]]\\n    last_value_baseline2 = test2_features[[task+\"_last_\"+str(numberOfInputWeeks-1)]]\\n\\n    last_value_baseline1_rmse = np.sqrt(mean_squared_error(last_value_baseline1,groundtruth1))\\n    last_value_baseline2_rmse = np.sqrt(mean_squared_error(last_value_baseline2,groundtruth2))\\n    \\n    \\n    res = {\\n         \\'week\\':[outputWeekNumber], \\n         \\'model rmse 1\\':[rmse1], \\n         \\'model rmse 2\\':[rmse2],\\n         \\'last_value_baseline1_rmse\\':[last_value_baseline1_rmse], \\n         \\'last_value_baseline2_rmse\\':[last_value_baseline2_rmse],\\n        }\\n    resultsDf = resultsDf.append(pd.DataFrame(data=res), ignore_index = True)\\n\\ndisplay(results)\\ndisplay(resultsDf.sum())\\n\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "estimator = load('models/trainedOnTrainingAndValidationData/0_case_inz_entries_7dayAverage_sklearn.pkl')\n",
    "task = 'case_inz_entries_7dayAverage'\n",
    "\n",
    "'''\n",
    "'''\n",
    "estimator = load('models/trainedOnTrainingAndValidationData/1_hosp_inz_entries_7dayAverage_sklearn.pkl')\n",
    "task = 'hosp_inz_entries_7dayAverage'\n",
    "\n",
    "\n",
    "estimator = load('models/trainedOnTrainingAndValidationData/2_death_inz_entries_7dayAverage_sklearn.pkl')\n",
    "task = 'death_inz_entries_7dayAverage'\n",
    "\n",
    "                  \n",
    "\n",
    "predictions1 = pd.DataFrame(estimator.predict(X_test1), index=test1_labels.index, columns=[\"pred_week_\"+task+\"_\"+str(outputWeekNumber) for outputWeekNumber in range(0,numberOfOutputWeeks)]) \n",
    "predictions2 = pd.DataFrame(estimator.predict(X_test2), index=test2_labels.index, columns=[\"pred_week_\"+task+\"_\"+str(outputWeekNumber) for outputWeekNumber in range(0,numberOfOutputWeeks)])\n",
    "\n",
    "# test for all weeks\n",
    "y_test1 = test1_labels[[\"output_\"+task+\"_\"+str(outputWeekNumber) for outputWeekNumber in range(0,numberOfOutputWeeks)]]\n",
    "y_test2 = test2_labels[[\"output_\"+task+\"_\"+str(outputWeekNumber) for outputWeekNumber in range(0,numberOfOutputWeeks)]]\n",
    "\n",
    "resultsDf = pd.DataFrame()\n",
    "# compute and safe results for every week\n",
    "for outputWeekNumber in range(0,numberOfOutputWeeks):\n",
    "    # compute root mean squared error for test sets\n",
    "    rmse1 = np.sqrt(mean_squared_error(predictions1[\"pred_week_\"+task+\"_\"+str(outputWeekNumber)], y_test1[\"output_\"+task+\"_\"+str(outputWeekNumber)]))\n",
    "    rmse2 = np.sqrt(mean_squared_error(predictions2[\"pred_week_\"+task+\"_\"+str(outputWeekNumber)], y_test2[\"output_\"+task+\"_\"+str(outputWeekNumber)]))\n",
    "\n",
    "    \n",
    "    groundtruth1 = test1_labels[[\"output_\"+task+\"_\"+str(outputWeekNumber)]]\n",
    "    groundtruth2 = test2_labels[[\"output_\"+task+\"_\"+str(outputWeekNumber)]]\n",
    "    \n",
    "    temp = pd.DataFrame(predictions1[\"pred_week_\"+task+\"_\"+str(outputWeekNumber)])\n",
    "    temp = temp.join(groundtruth1)\n",
    "    display(temp.plot(kind='box', ylabel='7 day case incidence average', figsize=(10,5)))\n",
    "\n",
    "    # get the easy baseline (is the same for all output weeks)\n",
    "    last_value_baseline1 = test1_features[[task+\"_last_\"+str(numberOfInputWeeks-1)]]\n",
    "    last_value_baseline2 = test2_features[[task+\"_last_\"+str(numberOfInputWeeks-1)]]\n",
    "\n",
    "    last_value_baseline1_rmse = np.sqrt(mean_squared_error(last_value_baseline1,groundtruth1))\n",
    "    last_value_baseline2_rmse = np.sqrt(mean_squared_error(last_value_baseline2,groundtruth2))\n",
    "    \n",
    "    \n",
    "    res = {\n",
    "         'week':[outputWeekNumber], \n",
    "         'model rmse 1':[rmse1], \n",
    "         'model rmse 2':[rmse2],\n",
    "         'last_value_baseline1_rmse':[last_value_baseline1_rmse], \n",
    "         'last_value_baseline2_rmse':[last_value_baseline2_rmse],\n",
    "        }\n",
    "    resultsDf = resultsDf.append(pd.DataFrame(data=res), ignore_index = True)\n",
    "\n",
    "display(results)\n",
    "display(resultsDf.sum())\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seasonal-hughes",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bibliographic-tractor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protected-making",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
