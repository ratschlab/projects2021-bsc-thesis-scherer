{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "animal-abortion",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "pd.options.display.max_rows = 10\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "import time\n",
    "\n",
    "cantonKeys = ['AG','AI','AR', 'BE', 'BL', 'BS', 'FR', 'GE', 'GL', 'GR', 'JU', 'LU', 'NE', 'NW', 'OW', 'SG', 'SH', 'SO', 'SZ', 'TG', 'TI', 'UR', 'VD', 'VS', 'ZG','ZH']\n",
    "\n",
    "# we discard the first days of 2020 and future data\n",
    "yesterday = str(datetime.date.today()- timedelta(days = 1))\n",
    "start = '2020-02-15'\n",
    "end = '2021-04-05' #use '2021-04-05' for testing and yesterday for production\n",
    "\n",
    "mergedDict = {}\n",
    "for cantonId in cantonKeys: \n",
    "    mergedDict[cantonId] = pd.read_csv(\"data/merged/\"+cantonId+\".csv\").set_index('date')[start:end]\n",
    "    mergedDict[cantonId].index = pd.to_datetime(mergedDict[cantonId].index)\n",
    "    \n",
    "interpolMet = 'linear'\n",
    "originalDict = mergedDict.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "laden-resistance",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "for cantonId in cantonKeys:\n",
    "    merged = mergedDict[cantonId].copy()\n",
    "    \n",
    "    # fill missing vaccine data\n",
    "    vaccine = ['VaccDosesAdministered sumTotal','VaccDosesAdministered per100PersonsTotal', \n",
    "               'FullyVaccPersons sumTotal', 'FullyVaccPersons per100PersonsTotal']\n",
    "    merged.loc['2020-02-15',vaccine] = 0\n",
    "    merged[vaccine] = merged[vaccine].fillna(method='ffill')\n",
    "    \n",
    "    # fill missing total hospital capacities\n",
    "    hospitalCols = ['ICU_Capacity','ICU_FreeCapacity','Total_Capacity','Total_FreeCapacity']\n",
    "    merged[hospitalCols] = merged[hospitalCols].interpolate(method=interpolMet)\n",
    "    merged[hospitalCols] = merged[hospitalCols].fillna(method='bfill')  \n",
    "\n",
    "    # fill in missing hospital capacity data\n",
    "    # we make the assumption that at the beginning there were no covid patients\n",
    "    merged.loc['2020-02-15',['Total_Covid19Patients','ICU_Covid19Patients']] = 0\n",
    "    patientCols = ['Total_Covid19Patients','Total_NonCovid19Patients','Total_AllPatients','ICU_Covid19Patients',\n",
    "                   'ICU_NonCovid19Patients','ICU_AllPatients']\n",
    "    merged[patientCols] = merged[patientCols].interpolate(method=interpolMet)\n",
    "    merged[['Total_NonCovid19Patients','ICU_NonCovid19Patients']] = merged[['Total_NonCovid19Patients','ICU_NonCovid19Patients']].fillna(method='bfill')\n",
    "    # Covid19Patients + NonCovid19Patients = AllPatients\n",
    "    merged['Total_AllPatients'].fillna(merged[['Total_Covid19Patients','Total_NonCovid19Patients']].sum(axis=1), inplace=True)\n",
    "    merged['ICU_AllPatients'].fillna(merged[['ICU_Covid19Patients','ICU_NonCovid19Patients']].sum(axis=1), inplace=True)\n",
    "    \n",
    "    # fill in missing Google mobility data\n",
    "    googleMobilityCols = ['retail_and_recreation_percent_change_from_baseline','grocery_and_pharmacy_percent_change_from_baseline',\n",
    "    'parks_percent_change_from_baseline','transit_stations_percent_change_from_baseline','workplaces_percent_change_from_baseline'\n",
    "    ,'residential_percent_change_from_baseline']\n",
    "    # use the swiss average when possible\n",
    "    googleMobCH = pd.read_csv(\"data/GoogleMobility/2020_CH_Region_Mobility_Report.csv\")\n",
    "    googleMobCH = googleMobCH.loc[googleMobCH[\"sub_region_1\"].isna()].set_index('date')[googleMobilityCols]\n",
    "    googleMobCH.index = pd.to_datetime(googleMobCH.index)\n",
    "    for col in googleMobilityCols:\n",
    "        merged[col].fillna(googleMobCH[col], inplace=True)\n",
    "    # for the rest interpolate\n",
    "    merged[googleMobilityCols] = merged[googleMobilityCols].interpolate(method=interpolMet)\n",
    "    merged[googleMobilityCols] = merged[googleMobilityCols].fillna(method='ffill')\n",
    "    \n",
    "    # fill in missing Intervista mobility data \n",
    "    merged[['intervistaMob']] = merged[['intervistaMob']].interpolate(method=interpolMet)\n",
    "    merged['intervistaMob'].fillna(method='ffill', inplace=True)\n",
    "        \n",
    "    # fill in missing r values\n",
    "    rvalues = ['median_R_mean','median_R_highHPD','median_R_lowHPD']\n",
    "    merged[rvalues] = merged[rvalues].interpolate(method=interpolMet)  \n",
    "    merged[rvalues] = merged[rvalues].fillna(method='ffill')\n",
    "    merged[rvalues] = merged[rvalues].fillna(method='bfill')\n",
    "  \n",
    "    # fill in missing variants\n",
    "    # first detected case for variants of concerne in Switzerland is 2020-10-14\n",
    "    variants = ['lower_ci_day','upper_ci_day','anteil_pos']\n",
    "    merged.loc['2020-10-13',['lower_ci_day','anteil_pos']] = 0\n",
    "    merged.loc['2020-10-13',['upper_ci_day']] = 100\n",
    "    merged[variants] = merged[variants].interpolate(method=interpolMet)\n",
    "    merged[variants] = merged[variants].fillna(method='ffill')\n",
    "    merged[variants] = merged[variants].fillna(method='bfill')\n",
    "\n",
    "    # fill in daily incoming missing data\n",
    "    zeroAndInterpolate = ['case_entries','hosp_entries','death_entries','case_inz_entries','hosp_inz_entries',\n",
    "                          'death_inz_entries','case_inzsumTotal','hosp_inzsumTotal','death_inzsumTotal',\n",
    "                          'test_inzsumTotal', 'meanNeighborIncidence', 'maxNeighborIncidence',\n",
    "                          'Cases entries 0 - 9','Cases entries 10 - 19','Cases entries 20 - 29',\n",
    "                          'Cases entries 30 - 39','Cases entries 40 - 49','Cases entries 50 - 59',\n",
    "                          'Cases entries 60 - 69','Cases entries 70 - 79','Cases entries 80+',\n",
    "                          'Cases entries male','Cases entries female','Cases inz_entries 0 - 9',\n",
    "                          'Cases inz_entries 10 - 19','Cases inz_entries 20 - 29','Cases inz_entries 30 - 39',\n",
    "                          'Cases inz_entries 40 - 49','Cases inz_entries 50 - 59','Cases inz_entries 60 - 69',\n",
    "                          'Cases inz_entries 70 - 79','Cases inz_entries 80+','Cases inz_entries male',\n",
    "                          'Cases inz_entries female','Cases inzsumTotal 0 - 9','Cases inzsumTotal 10 - 19',\n",
    "                          'Cases inzsumTotal 20 - 29','Cases inzsumTotal 30 - 39','Cases inzsumTotal 40 - 49',\n",
    "                          'Cases inzsumTotal 50 - 59','Cases inzsumTotal 60 - 69','Cases inzsumTotal 70 - 79',\n",
    "                          'Cases inzsumTotal 80+','Cases inzsumTotal male','Cases inzsumTotal female',\n",
    "                          'Death entries 0 - 9','Death entries 10 - 19','Death entries 20 - 29',\n",
    "                          'Death entries 30 - 39','Death entries 40 - 49','Death entries 50 - 59',\n",
    "                          'Death entries 60 - 69','Death entries 70 - 79','Death entries 80+',\n",
    "                          'Death entries male','Death entries female','Death inz_entries 0 - 9',\n",
    "                          'Death inz_entries 10 - 19','Death inz_entries 20 - 29','Death inz_entries 30 - 39',\n",
    "                          'Death inz_entries 40 - 49','Death inz_entries 50 - 59','Death inz_entries 60 - 69',\n",
    "                          'Death inz_entries 70 - 79','Death inz_entries 80+','Death inz_entries male',\n",
    "                          'Death inz_entries female','Death inzsumTotal 0 - 9','Death inzsumTotal 10 - 19',\n",
    "                          'Death inzsumTotal 20 - 29','Death inzsumTotal 30 - 39','Death inzsumTotal 40 - 49',\n",
    "                          'Death inzsumTotal 50 - 59','Death inzsumTotal 60 - 69','Death inzsumTotal 70 - 79',\n",
    "                          'Death inzsumTotal 80+','Death inzsumTotal male','Death inzsumTotal female',\n",
    "                          'Hosp entries 0 - 9','Hosp entries 10 - 19','Hosp entries 20 - 29',\n",
    "                          'Hosp entries 30 - 39','Hosp entries 40 - 49','Hosp entries 50 - 59',\n",
    "                          'Hosp entries 60 - 69','Hosp entries 70 - 79','Hosp entries 80+','Hosp entries male',\n",
    "                          'Hosp entries female','Hosp inz_entries 0 - 9','Hosp inz_entries 10 - 19',\n",
    "                          'Hosp inz_entries 20 - 29','Hosp inz_entries 30 - 39','Hosp inz_entries 40 - 49',\n",
    "                          'Hosp inz_entries 50 - 59','Hosp inz_entries 60 - 69','Hosp inz_entries 70 - 79',\n",
    "                          'Hosp inz_entries 80+','Hosp inz_entries male','Hosp inz_entries female',\n",
    "                          'Hosp inzsumTotal 0 - 9','Hosp inzsumTotal 10 - 19','Hosp inzsumTotal 20 - 29',\n",
    "                          'Hosp inzsumTotal 30 - 39','Hosp inzsumTotal 40 - 49','Hosp inzsumTotal 50 - 59',\n",
    "                          'Hosp inzsumTotal 60 - 69','Hosp inzsumTotal 70 - 79','Hosp inzsumTotal 80+',\n",
    "                          'Hosp inzsumTotal male','Hosp inzsumTotal female']\n",
    "    merged.loc['2020-02-15',zeroAndInterpolate] = 0\n",
    "    merged[zeroAndInterpolate] = merged[zeroAndInterpolate].interpolate(method=interpolMet)\n",
    "    \n",
    "    if not os.path.exists('data/filled'):\n",
    "        os.makedirs('data/filled')\n",
    "    merged.to_csv('data/filled/'+cantonId+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "enclosed-colombia",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# check complete data if there are any NaNs left\n",
    "for cantonId in cantonKeys:\n",
    "    filled = pd.read_csv('data/filled/'+cantonId+'.csv')\n",
    "    for col in filled.columns:\n",
    "        if filled[col].isna().sum() != 0:\n",
    "            print(cantonId+\" \"+col+\" (#NaN/#NotNaN): (\" + str(filled[col].isna().sum())+\"/\"+str(filled[col].notna().sum())+\")\")\n",
    "            #dict[cantonId][col].plot(kind='line',y=[col], figsize=(20,10))\n",
    "            #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "floral-genome",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plotting original data vs filled data\n",
    "#for col in dict['AG'].columns:\n",
    "#    comparingDf = pd.concat([dict['AG'][[col]],originalData[[\"original_\"+col]]], axis=1)\n",
    "    #comparingDf[['original_'+col]].reset_index().plot(kind='scatter', x=['date'], y=['original_'+col], figsize=(20,10))\n",
    "    #comparingDf['2020-02-15':'2021-04-05'].plot(kind='line',y=[col], figsize=(20,10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "seasonal-bradford",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURE ENGINEERING\n",
    "filledDict = {}\n",
    "\n",
    "for cantonId in cantonKeys:\n",
    "    filledDict[cantonId] = pd.read_csv(\"data/filled/\"+cantonId+\".csv\").set_index('date')\n",
    "    \n",
    "    dailyFeatures = filledDict[cantonId].copy()\n",
    "\n",
    "    # summarize mask mandatories\n",
    "    maskMandatories = [ 'Mask mandatory in publicly accessible establishments/ spaces (shops etc.)',\n",
    "                       'Mask mandatory in public transport','Masks mandatory in schools','Masks mandatory at work']\n",
    "    dailyFeatures[['maskMandatories']] = dailyFeatures[maskMandatories].sum(axis=1)\n",
    "    dailyFeatures.drop(maskMandatories, axis=1, inplace=True)\n",
    "\n",
    "    dailyFeatures[['googleMobility']] = dailyFeatures[['retail_and_recreation_percent_change_from_baseline',\n",
    "                                                       'grocery_and_pharmacy_percent_change_from_baseline',\n",
    "                                                       'parks_percent_change_from_baseline',\n",
    "                                                       'transit_stations_percent_change_from_baseline',\n",
    "                                                       'workplaces_percent_change_from_baseline',\n",
    "                                                       'residential_percent_change_from_baseline']].mean(axis=1)\n",
    "\n",
    "    # r value accuracy\n",
    "    dailyFeatures[['R_error']] = dailyFeatures['median_R_highHPD']-dailyFeatures['median_R_lowHPD']\n",
    "    dailyFeatures.drop(['median_R_highHPD','median_R_lowHPD'],axis=1, inplace=True)\n",
    "\n",
    "    # variants accuracy\n",
    "    #features[['anteil_pos','upper_ci_day','lower_ci_day']] = features[['anteil_pos','upper_ci_day','lower_ci_day']].rolling(window=w).mean()\n",
    "    dailyFeatures[['variant_error']] = dailyFeatures['upper_ci_day']-dailyFeatures['lower_ci_day']\n",
    "    dailyFeatures.drop(['upper_ci_day','lower_ci_day'],axis=1, inplace=True)\n",
    "\n",
    "    # vaccine\n",
    "    dailyFeatures.drop(['VaccDosesAdministered sumTotal','FullyVaccPersons sumTotal'],axis=1, inplace=True)\n",
    "    vaccine = ['VaccDosesAdministered per100PersonsTotal',\n",
    "               'FullyVaccPersons per100PersonsTotal']\n",
    "    #features[vaccine] = features[vaccine].rolling(window=w).mean()\n",
    "    \n",
    "    # test positivity rate\n",
    "    #display(len(features[['case_entries']]))\n",
    "    #display(len(features[['test_entries']]))\n",
    "    dailyFeatures[['testPositvity']] = dailyFeatures['case_entries']/dailyFeatures['test_entries']\n",
    "\n",
    "    # remove absolut values which are included in the incidenc rates\n",
    "    absVal = ['Cases entries 0 - 9','Cases entries 10 - 19','Cases entries 20 - 29','Cases entries 30 - 39',\n",
    "              'Cases entries 40 - 49','Cases entries 50 - 59','Cases entries 60 - 69','Cases entries 70 - 79',\n",
    "              'Cases entries 80+','Death entries 0 - 9','Death entries 10 - 19','Death entries 20 - 29',\n",
    "              'Death entries 30 - 39','Death entries 40 - 49','Death entries 50 - 59','Death entries 60 - 69',\n",
    "              'Death entries 70 - 79','Death entries 80+', 'Hosp entries 0 - 9','Hosp entries 10 - 19',\n",
    "              'Hosp entries 20 - 29','Hosp entries 30 - 39','Hosp entries 40 - 49','Hosp entries 50 - 59',\n",
    "              'Hosp entries 60 - 69','Hosp entries 70 - 79','Hosp entries 80+', 'Cases entries female',\n",
    "              'Cases entries male','Death entries female','Death entries male', 'Hosp entries female',\n",
    "              'Hosp entries male','case_entries','hosp_entries','death_entries','test_entries']\n",
    "    dailyFeatures.drop(absVal,axis=1, inplace=True)\n",
    "\n",
    "    # hospital capacities\n",
    "    hospCap = [ 'ICU_AllPatients',\n",
    "     'ICU_Covid19Patients',\n",
    "     'ICU_Capacity',\n",
    "     'Total_AllPatients',\n",
    "     'Total_Covid19Patients',\n",
    "     'Total_Capacity',\n",
    "     'ICU_NonCovid19Patients',\n",
    "     'ICU_FreeCapacity',\n",
    "     'Total_NonCovid19Patients',\n",
    "     'Total_FreeCapacity']\n",
    "    staticCantonal = pd.read_excel(\"static_data/staticCantonalData.xlsx\").set_index('canton').transpose()\n",
    "    dailyFeatures[[col + \"_inz\" for col in hospCap]] = 100000*(dailyFeatures[hospCap]/staticCantonal.loc[[cantonId]]['residents'][0])\n",
    "    dailyFeatures.drop(hospCap,axis=1, inplace=True)\n",
    "\n",
    "    if not os.path.exists('data/dailyFeatures'):\n",
    "        os.makedirs('data/dailyFeatures')\n",
    "    dailyFeatures.to_csv('data/dailyFeatures/'+cantonId+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "infectious-nursery",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSTRUCTING INPUT/OUTPUT INTERVALS\n",
    "\n",
    "\n",
    "# generate date ranges\n",
    "dataStart = pd.Timestamp(start)\n",
    "dataEnd = pd.Timestamp(end)\n",
    "\n",
    "daysIn = 7 # last n days of input\n",
    "daysOut = 7 # next n days of output\n",
    "\n",
    "listOfInputIntervals = []\n",
    "listOfOutputIntervals = []\n",
    "for e in pd.date_range(start=dataStart,end=dataEnd, freq='D'):\n",
    "    if (e+timedelta(days = daysIn+daysOut-1) <= dataEnd.date()):\n",
    "        listOfInputIntervals.append((e.date(),(e+timedelta(days = (daysIn-1))).date()))\n",
    "        listOfOutputIntervals.append(((e+timedelta(days = daysIn)).date(),(e+timedelta(days = daysIn+daysOut-1)).date()))\n",
    "    \n",
    "#display(listOfInputIntervals)\n",
    "#display(listOfOutputIntervals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "aquatic-ranking",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AG'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'AI'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'AR'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'BE'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'BL'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'BS'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'FR'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'GE'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'GL'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'GR'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'JU'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'LU'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'NE'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'NW'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'OW'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'SG'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'SH'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'SO'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'SZ'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'TG'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'TI'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'UR'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'VD'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'VS'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'ZG'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'ZH'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dailyFeaturesDict = {}\n",
    "    \n",
    "# CONSTRUCTING ACTUAL INPUTS\n",
    "for cantonId in cantonKeys:\n",
    "    dailyFeaturesDict[cantonId] = pd.read_csv(\"data/dailyFeatures/\"+cantonId+\".csv\").set_index('date')\n",
    "    dailyFeaturesDict[cantonId].index = pd.to_datetime(dailyFeaturesDict[cantonId].index)\n",
    "    \n",
    "    display(cantonId)\n",
    "    features = pd.DataFrame()\n",
    "    \n",
    "    # construction of input features\n",
    "    for t in listOfInputIntervals:\n",
    "        \n",
    "        # average features\n",
    "        # features which will be averaged over the whole input interval\n",
    "        averageFeatures = ['Cases inz_entries 0 - 9','Cases inz_entries 10 - 19','Cases inz_entries 20 - 29',\n",
    "                  'Cases inz_entries 30 - 39','Cases inz_entries 40 - 49','Cases inz_entries 50 - 59',\n",
    "                  'Cases inz_entries 60 - 69','Cases inz_entries 70 - 79','Cases inz_entries 80+',\n",
    "                  'Cases inzsumTotal 0 - 9','Cases inzsumTotal 10 - 19','Cases inzsumTotal 20 - 29',\n",
    "                  'Cases inzsumTotal 30 - 39','Cases inzsumTotal 40 - 49','Cases inzsumTotal 50 - 59',\n",
    "                  'Cases inzsumTotal 60 - 69','Cases inzsumTotal 70 - 79','Cases inzsumTotal 80+',\n",
    "                  'Death inz_entries 0 - 9','Death inz_entries 10 - 19','Death inz_entries 20 - 29',\n",
    "                  'Death inz_entries 30 - 39','Death inz_entries 40 - 49','Death inz_entries 50 - 59',\n",
    "                  'Death inz_entries 60 - 69','Death inz_entries 70 - 79','Death inz_entries 80+',\n",
    "                  'Death inzsumTotal 0 - 9','Death inzsumTotal 10 - 19','Death inzsumTotal 20 - 29',\n",
    "                  'Death inzsumTotal 30 - 39','Death inzsumTotal 40 - 49','Death inzsumTotal 50 - 59',\n",
    "                  'Death inzsumTotal 60 - 69','Death inzsumTotal 70 - 79','Death inzsumTotal 80+',\n",
    "                  'Hosp inz_entries 0 - 9','Hosp inz_entries 10 - 19','Hosp inz_entries 20 - 29',\n",
    "                  'Hosp inz_entries 30 - 39','Hosp inz_entries 40 - 49','Hosp inz_entries 50 - 59',\n",
    "                  'Hosp inz_entries 60 - 69','Hosp inz_entries 70 - 79','Hosp inz_entries 80+',\n",
    "                  'Hosp inzsumTotal 0 - 9','Hosp inzsumTotal 10 - 19','Hosp inzsumTotal 20 - 29',\n",
    "                  'Hosp inzsumTotal 30 - 39','Hosp inzsumTotal 40 - 49','Hosp inzsumTotal 50 - 59',\n",
    "                  'Hosp inzsumTotal 60 - 69','Hosp inzsumTotal 70 - 79','Hosp inzsumTotal 80+',\n",
    "                  'Cases inz_entries female','Cases inz_entries male','Cases inzsumTotal female',\n",
    "                  'Cases inzsumTotal male','Death inz_entries female','Death inz_entries male',\n",
    "                  'Death inzsumTotal female','Death inzsumTotal male','Hosp inz_entries female',\n",
    "                  'Hosp inz_entries male','Hosp inzsumTotal female','Hosp inzsumTotal male', \n",
    "                  'VaccDosesAdministered per100PersonsTotal',\n",
    "                  'FullyVaccPersons per100PersonsTotal',\n",
    "                  'anteil_pos',\n",
    "                  'variant_error',\n",
    "                  'case_inzsumTotal','hosp_inzsumTotal','death_inzsumTotal','test_inzsumTotal','case_inz_entries',\n",
    "                  'hosp_inz_entries','death_inz_entries','test_inz_entries','testPositvity',\n",
    "                  'median_R_mean','R_error',\n",
    "                  'meanNeighborIncidence','maxNeighborIncidence',\n",
    "                  'kofStrigency',\n",
    "                  'Borders','Events','Gatherings/private events','Demonstrations',\n",
    "                  'Primary (includes kindergarten) and lower secondary school','Upper secondary school, vocational schools and higher education',\n",
    "                  'universities and other educational establishments\\xa0','Mountain railways','Homeworking','Restaurants',\n",
    "                  'Discos/Nightclubs','Shops/Markets','Penalties','Cultural, entertainment and recreational facilities',\n",
    "                  'Sport/Wellness facilities','Sport activities','Religious services','Singing allowed','maskMandatories',\n",
    "                  'ICU_AllPatients_inz','ICU_Covid19Patients_inz','ICU_Capacity_inz','Total_AllPatients_inz',\n",
    "                  'Total_Covid19Patients_inz','Total_Capacity_inz','ICU_NonCovid19Patients_inz','ICU_FreeCapacity_inz',\n",
    "                  'Total_NonCovid19Patients_inz','Total_FreeCapacity_inz']\n",
    "        featureRow = dailyFeaturesDict[cantonId][t[0]:t[1]][averageFeatures].mean().to_frame().transpose()\n",
    "\n",
    "        # direct features\n",
    "        # features which will be direct input for every day of the input interval\n",
    "        # attention: this can potentially increase the number of input features significantly\n",
    "        # added features are len(directFea)*daysIn\n",
    "        # only add features for which have a large variance from one day to another day\n",
    "        directFeatures = ['retail_and_recreation_percent_change_from_baseline',\n",
    "                     'grocery_and_pharmacy_percent_change_from_baseline',\n",
    "                     'parks_percent_change_from_baseline',\n",
    "                     'transit_stations_percent_change_from_baseline',\n",
    "                     'workplaces_percent_change_from_baseline',\n",
    "                     'residential_percent_change_from_baseline',\n",
    "                     'intervistaMob',\n",
    "                     'isHolyday',\n",
    "                     'temp_min','temp_max','clouds','precipitation']\n",
    "        for f in directFeatures:\n",
    "            directFe = dailyFeaturesDict[cantonId][t[0]:t[1]][[f]].transpose()\n",
    "            directFe.columns = [f+'_day_'+str(d) for d in range(0,daysIn)]\n",
    "            directFe = directFe.reset_index().drop(['index'], axis=1)\n",
    "            featureRow = pd.concat([featureRow,directFe], axis=1)\n",
    "        \n",
    "        \n",
    "        # future features\n",
    "        futureFeatures = ['temp_min','temp_max','clouds','precipitation']\n",
    "        indexOfInputTuple = listOfInputIntervals.index(t)\n",
    "        ot = listOfOutputIntervals[indexOfInputTuple]\n",
    "        for ff in futureFeatures:\n",
    "            futureFe = dailyFeaturesDict[cantonId][ot[0]:ot[1]][[ff]].transpose()\n",
    "            futureFe.columns = [ff+'_future_day_'+str(d) for d in range(0,daysOut)]\n",
    "            futureFe = futureFe.reset_index().drop(['index'], axis=1)\n",
    "            featureRow = pd.concat([featureRow,futureFe], axis=1)\n",
    "        \n",
    "        features = features.append(featureRow, ignore_index=True)\n",
    "\n",
    "    # static cantonal features\n",
    "    staticCantonal = pd.read_excel(\"static_data/staticCantonalData.xlsx\").set_index('canton').transpose()\n",
    "    # households\n",
    "    households = ['1PersonHouseholds', '2PersonHouseholds','3PersonHouseholds', '4PersonHouseholds', \n",
    "                  '5PersonHouseholds','6+PersonHouseholds']\n",
    "    for h in households:\n",
    "        features[[h+\"_perc\"]] = staticCantonal.loc[[cantonId]][h][0]/staticCantonal.loc[[cantonId]]['totalHousholds'][0]\n",
    "    features[['averageHousehold']] = staticCantonal.loc[[cantonId]]['residents'][0]/staticCantonal.loc[[cantonId]]['totalHousholds'][0]\n",
    "    # add static features\n",
    "    staticFeatures = ['percentage 65 years or over','urbanPopulationPercent','homeownershipPercent', \n",
    "                      'livingSpaceInm2','carsPer1000inhabitants', 'publicTransportationPercent',\n",
    "                      'privateMotorisedTransportPercent','DoctorsPer100Kinhabitants','residentsPerKm2']\n",
    "    for f in staticFeatures:\n",
    "        features[[f]] = staticCantonal.loc[[cantonId]][f][0]\n",
    "    # construct settlement area feature\n",
    "    residents = staticCantonal.loc[[cantonId]]['residents'][0]\n",
    "    settlementArea = staticCantonal.loc[[cantonId]]['areaInKm2'][0]*(staticCantonal.loc[[cantonId]]['settlementAreaPercent'][0]/100)\n",
    "    features[['residentsPerKm2SettlementArea']] = residents/settlementArea    \n",
    "    \n",
    "    if not os.path.exists('data/features'):\n",
    "        os.makedirs('data/features')\n",
    "    features.to_csv('data/features/'+cantonId+'.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "# CONSTRUCTING ACTUAL OUTPUTS\n",
    "for cantonId in cantonKeys:\n",
    "    outputs = pd.DataFrame()\n",
    "    for t in listOfOutputIntervals:\n",
    "        outputCols = ['hosp_inz_entries','death_inz_entries','testPositvity','googleMobility']\n",
    "        outputRow = dailyFeaturesDict[cantonId][t[0]:t[1]][outputCols].mean().to_frame().transpose()   \n",
    "        outputs = outputs.append(outputRow, ignore_index=True)    \n",
    "    if not os.path.exists('data/outputs'):\n",
    "        os.makedirs('data/outputs')\n",
    "    outputs.to_csv('data/outputs/'+cantonId+'.csv', index=False)\n",
    "\n",
    "\n",
    "# MERGE ALL CANTONAL TRAIN DATA TOGETHER\n",
    "train_features = pd.DataFrame()\n",
    "train_labels = pd.DataFrame()\n",
    "for cantonId in [canton for canton in cantonKeys if canton not in ['SG','NE','NW']]: # exclude test cantons\n",
    "    train_features = train_features.append(pd.read_csv(\"data/features/\"+cantonId+\".csv\"))\n",
    "    train_labels = train_labels.append(pd.read_csv(\"data/outputs/\"+cantonId+\".csv\"))\n",
    "train_features.to_csv('train_features.csv', index=False)\n",
    "train_labels.to_csv('train_labels.csv', index=False)\n",
    "  \n",
    "    \n",
    "# MERGE ALL CANTONAL TEST DATA TOGETHER\n",
    "test_features = pd.DataFrame()\n",
    "test_labels = pd.DataFrame()\n",
    "for cantonId in ['SG','NE','NW']:\n",
    "    test_features = test_features.append(pd.read_csv(\"data/features/\"+cantonId+\".csv\"))\n",
    "    test_labels = test_labels.append(pd.read_csv(\"data/outputs/\"+cantonId+\".csv\"))\n",
    "test_features.to_csv('test_features.csv', index=False)\n",
    "test_labels.to_csv('test_labels.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attractive-scope",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
