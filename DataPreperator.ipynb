{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "jewish-namibia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS FILE PREPARES THE COLLECTED DATA FOR THE MACHINE LEARNING ALGORITHM, IT ALSO DOES THE TRAINING,\n",
    "# VALIDATION AND TEST SET CATEGORIZATION\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "from datetime import date, timedelta, timezone\n",
    "from sklearn.impute import KNNImputer\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.options.display.max_rows = 10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cantonKeys = ['AG','AI','AR', 'BE', 'BL', 'BS', 'FR', 'GE', 'GL', 'GR', 'JU', 'LU', 'NE', 'NW', 'OW', 'SG', 'SH', 'SO', 'SZ', 'TG', 'TI', 'UR', 'VD', 'VS', 'ZG','ZH']\n",
    "googleMobDict = dict(zip(cantonKeys,[\"Aargau\",\"Appenzell Innerrhoden\",\"Appenzell Ausserrhoden\",\"Canton of Bern\",\"Basel-Landschaft\",\"Basel City\",\n",
    "                                                    \"Fribourg\",\"Geneva\",\"Glarus\",\"Grisons\",\"Jura\",\"Lucerne\",\"Neuch√¢tel\",\"Nidwalden\",\"Obwalden\",\"St. Gallen\",\n",
    "                                                    \"Schaffhausen\",\"Solothurn\",\"Schwyz\",\"Thurgau\",\"Ticino\",\"Uri\",\"Vaud\",\"Valais\",\"Canton of Zug\",\"Zurich\"]))\n",
    "def getDays(year, offset):\n",
    "   d = date(year, 1, 1)                    \n",
    "   d += timedelta(days = offset - d.weekday())  \n",
    "   while d.year == year:\n",
    "      yield d\n",
    "      d += timedelta(days = 7)\n",
    "\n",
    "listOfMondays = []\n",
    "for year in [2020,2021]:\n",
    "    for day in getDays(year, 7):\n",
    "       listOfMondays.append(day)\n",
    "    \n",
    "def addZero(x):\n",
    "    if len(x)==1:\n",
    "        return \"0\"+x\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "yearWeek = [str(x.isocalendar()[0])+addZero(str(x.isocalendar()[1])) for x in listOfMondays]\n",
    "\n",
    "\n",
    "\n",
    "mondaysByWeekNr = dict(zip(yearWeek,listOfMondays))\n",
    "\n",
    "\n",
    "# we discard the first days of 2020 and future data\n",
    "yesterday = datetime.date.today()-timedelta(days = 1)\n",
    "yesterdayStr = str(yesterday)\n",
    "start = '2020-02-15'\n",
    "end = '2021-06-25' #use '2021-05-11' for testing and yesterdayStr for production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlimited-accessory",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# MERGE THE COLLECTED DATA TO ONE FILE FOR EACH CANTON\n",
    "variantList = []\n",
    "\n",
    "\n",
    "for cantonId in cantonKeys:\n",
    "    df = pd.DataFrame(index=pd.date_range(start=datetime.datetime(2020, 1, 1), end=datetime.datetime(2021, 12, 31)))\n",
    "    \n",
    "    # weekly age classified FOPH data\n",
    "    for category in ['Cases','Death','Hosp']: \n",
    "        age = pd.read_csv(\"data/FOPH/data/COVID19\"+category+\"_geoRegion_AKL10_w.csv\")\n",
    "        age[['datum']] = age[['datum']].applymap(lambda x: mondaysByWeekNr[str(x)])\n",
    "        age = age.loc[age['geoRegion']==cantonId][['datum','altersklasse_covid19','inz_entries','inzsumTotal']]\n",
    "        age = age.pivot(index=\"datum\", columns=\"altersklasse_covid19\")\n",
    "        age.columns = [category+\" \"+' '.join(col) for col in age.columns.values]\n",
    "        age = age.drop([category+' inz_entries Unbekannt', category+' inzsumTotal Unbekannt'], axis=1)\n",
    "        df = df.join(age)\n",
    "    \n",
    "    # weekly gender classified FOPH data\n",
    "    for category in ['Cases','Death','Hosp']: \n",
    "        gender = pd.read_csv(\"data/FOPH/data/COVID19\"+category+\"_geoRegion_sex_w.csv\")\n",
    "        gender[['datum']] = gender[['datum']].applymap(lambda x: mondaysByWeekNr[str(x)])\n",
    "        gender = gender.loc[gender['geoRegion']==cantonId][['datum','sex','inz_entries','inzsumTotal']]\n",
    "        gender = gender.pivot(index=\"datum\", columns=\"sex\")\n",
    "        gender.columns = [category+\" \"+' '.join(col) for col in gender.columns.values]\n",
    "        gender = gender.drop([category+' inz_entries unknown', category+' inzsumTotal unknown'],axis=1)\n",
    "        df = df.join(gender)\n",
    "      \n",
    "    # COVID19VaccPersons.csv\n",
    "    vacc = pd.read_csv(\"data/FOPH/data/COVID19VaccPersons.csv\")\n",
    "    temp1Dosis = vacc[(vacc['geoRegion']==cantonId) & (vacc['type']==\"COVID19AtLeastOneDosePersons\")]\n",
    "    temp2Dosis = vacc[(vacc['geoRegion']==cantonId) & (vacc['type']==\"COVID19FullyVaccPersons\")]\n",
    "    temp1Dosis = temp1Dosis[['date','per100PersonsTotal']]\n",
    "    temp2Dosis = temp2Dosis[['date','per100PersonsTotal']]\n",
    "    temp1Dosis = temp1Dosis.set_index('date')\n",
    "    temp2Dosis = temp2Dosis.set_index('date')\n",
    "    temp1Dosis.rename(columns = {\"per100PersonsTotal\":'AtLeastOneDosePersons per100PersonsTotal'}, inplace = True)\n",
    "    temp2Dosis.rename(columns = {\"per100PersonsTotal\":'COVID19FullyVaccPersons per100PersonsTotal'}, inplace = True)\n",
    "    temp1Dosis.index = pd.to_datetime(temp1Dosis.index) \n",
    "    temp2Dosis.index = pd.to_datetime(temp2Dosis.index)\n",
    "    df = df.join(temp1Dosis)\n",
    "    df = df.join(temp2Dosis)\n",
    "    \n",
    "    \n",
    "    # daily hospital capacity FOPH data\n",
    "    capacity = pd.DataFrame()\n",
    "    if (cantonId=='AI'):\n",
    "        capacity = pd.read_csv(\"static_data/historicHospitalCapacities/hospCapacitiesForAI.csv\")\n",
    "    else:\n",
    "        capacity = pd.read_csv(\"data/FOPH/data/COVID19HospCapacity_geoRegion.csv\")\n",
    "    \n",
    "    capacity = capacity.loc[capacity['geoRegion']==cantonId]\n",
    "    capacity = capacity.set_index('date').sort_index()\n",
    "    capacity = capacity[['ICU_AllPatients','ICU_Covid19Patients','ICU_Capacity','Total_AllPatients',\n",
    "                         'Total_Covid19Patients','Total_Capacity','ICU_NonCovid19Patients','ICU_FreeCapacity',\n",
    "                         'Total_NonCovid19Patients','Total_FreeCapacity','type_variant']]\n",
    "    capacity = capacity.drop_duplicates()\n",
    "    capacity = capacity.loc[capacity['type_variant']=='fp7d']\n",
    "    capacity = capacity.drop(['type_variant'], axis=1)\n",
    "    capacity.index = pd.to_datetime(capacity.index) \n",
    "    df = df.join(capacity)\n",
    "    \n",
    "    \n",
    "    # percentage of virus variants (swiss based)\n",
    "    variants = pd.read_csv(\"data/FOPH/data/COVID19Variants_wgs.csv\", low_memory=False)\n",
    "    variantList = variants[(variants[\"data_source\"]==\"wgs\") & (variants[\"variant_type\"]!=\"other_lineages\") & (variants[\"variant_type\"]!=\"all_sequenced\")][['variant_type']].drop_duplicates().values\n",
    "    variantList = variantList.ravel().tolist()\n",
    "    for v in variantList:\n",
    "        #variants[variants['']]\n",
    "        temp = variants[(variants['data_source']==\"wgs\") & (variants['variant_type']==v)]\n",
    "        temp = temp.set_index('date').sort_index()[[\"prct\"]]\n",
    "        temp.rename(columns = {\"prct\":'prct_'+v}, inplace = True)\n",
    "        temp.index = pd.to_datetime(temp.index)\n",
    "        df = df.join(temp)\n",
    "           \n",
    "    \n",
    "    # daily basis data\n",
    "    # attach daily positive cases\n",
    "    caseDf = pd.read_csv(\"data/FOPH/data/COVID19Cases_geoRegion.csv\")\n",
    "    caseDf = caseDf.loc[caseDf[\"geoRegion\"]==cantonId]\n",
    "    caseDf = caseDf.set_index('datum')\n",
    "    interestedCols = ['entries','inz_entries','inzsumTotal']\n",
    "    caseDf = caseDf[interestedCols]\n",
    "    caseDf.columns = [\"case_\"+e for e in interestedCols]\n",
    "    caseDf.index = pd.to_datetime(caseDf.index) \n",
    "    df = df.join(caseDf)\n",
    "    \n",
    "    # attach daily hospital cases\n",
    "    hospDf = pd.read_csv(\"data/FOPH/data/COVID19Hosp_geoRegion.csv\")\n",
    "    hospDf = hospDf.loc[hospDf[\"geoRegion\"]==cantonId]\n",
    "    hospDf = hospDf.set_index('datum')\n",
    "    interestedCols = ['inz_entries','inzsumTotal']\n",
    "    hospDf = hospDf[interestedCols]\n",
    "    hospDf.columns = [\"hosp_\"+e for e in interestedCols]\n",
    "    hospDf.index = pd.to_datetime(hospDf.index) \n",
    "    df = df.join(hospDf)\n",
    "    \n",
    "    # attach daily death cases\n",
    "    deathDf = pd.read_csv(\"data/FOPH/data/COVID19Death_geoRegion.csv\")\n",
    "    deathDf = deathDf.loc[deathDf[\"geoRegion\"]==cantonId]\n",
    "    deathDf = deathDf.set_index('datum')\n",
    "    interestedCols = ['inz_entries','inzsumTotal']\n",
    "    deathDf = deathDf[interestedCols]\n",
    "    deathDf.columns = [\"death_\"+e for e in interestedCols]\n",
    "    deathDf.index = pd.to_datetime(deathDf.index)\n",
    "    df = df.join(deathDf)\n",
    "    \n",
    "    \n",
    "    # attach daily test\n",
    "    testDf = pd.read_csv(\"data/FOPH/data/COVID19Test_geoRegion_all.csv\")\n",
    "    testDf = testDf.loc[testDf[\"geoRegion\"]==cantonId]\n",
    "    testDf = testDf.set_index('datum')\n",
    "    pop = testDf['pop'][0]\n",
    "    interestedCols = ['entries','inz_entries','inzsumTotal']\n",
    "    testDf = testDf[interestedCols]\n",
    "    testDf.columns = [\"test_\"+e for e in interestedCols]\n",
    "    testDf.index = pd.to_datetime(testDf.index)\n",
    "    df = df.join(testDf)\n",
    "    # compute rest of test entries\n",
    "    '''\n",
    "    totalTestsInSwitzerland = pd.DataFrame(index=pd.date_range(start=datetime.datetime(2020, 2, 15), end=datetime.datetime(2020, 5, 22)))\n",
    "    temp = pd.read_csv(\"data/FOPH/data/COVID19Test_geoRegion_all.csv\")\n",
    "    temp = temp[temp['geoRegion']=='CHFL']\n",
    "    temp = temp.set_index('datum')\n",
    "    temp.index = pd.to_datetime(temp.index)\n",
    "    totalTestsInSwitzerland = totalTestsInSwitzerland.join(temp)  \n",
    "    totalTestsInSwitzerland = totalTestsInSwitzerland[['entries']]\n",
    "    totalTestsInSwitzerland.fillna(method='bfill', inplace=True)\n",
    "    testsByCanton = pd.read_csv(\"data/FOPH/data/COVID19Test_geoRegion_all.csv\")\n",
    "    testsByCanton = testsByCanton.set_index('datum')\n",
    "    testsByCanton.index = pd.to_datetime(testsByCanton.index)\n",
    "    sumSwitzerland = testsByCanton.loc[testsByCanton[\"geoRegion\"]=='CHFL'][['entries']]\n",
    "    sumCanton = testsByCanton.loc[testsByCanton[\"geoRegion\"]==cantonId][['entries']]\n",
    "    cantonalTestFraction = sumCanton['2020-05-23':'2020-06-05'].sum(axis=0).values[0]/sumSwitzerland['2020-05-23':'2020-06-05'].sum(axis=0).values[0]\n",
    "    #multiply this with cantonal test quotient\n",
    "    computedMissingEntries = totalTestsInSwitzerland['2020-02-15':'2020-05-22']*cantonalTestFraction \n",
    "    computedMissingEntries.rename(columns = {\"entries\":'test_entries'}, inplace = True)\n",
    "    df[['test_entries']] = df[['test_entries']].fillna(computedMissingEntries[['test_entries']])\n",
    "    # compute rest of test incidence\n",
    "    missingTestIncidents = 100000*(df[['test_entries']]/pop)\n",
    "    missingTestIncidents.rename(columns = {\"test_entries\":'test_inz_entries'}, inplace = True)\n",
    "    df[['test_inz_entries']] = df[['test_inz_entries']].fillna(missingTestIncidents)   \n",
    "    '''\n",
    "    \n",
    "    # attach daily R-values\n",
    "    rvalueDf = pd.read_csv(\"data/FOPH/data/COVID19Re_geoRegion.csv\")\n",
    "    rvalueDf = rvalueDf.loc[rvalueDf[\"geoRegion\"]==cantonId]\n",
    "    rvalueDf = rvalueDf.set_index('date')\n",
    "    interestedCols = ['median_R_mean','median_R_highHPD','median_R_lowHPD']\n",
    "    rvalueDf = rvalueDf[interestedCols]\n",
    "    rvalueDf.index = pd.to_datetime(rvalueDf.index)\n",
    "    df = df.join(rvalueDf)\n",
    "    \n",
    "    # attach google mobility data\n",
    "    mobDf2020 = pd.read_csv(\"data/GoogleMobility/2020_CH_Region_Mobility_Report.csv\")\n",
    "    mobDf2021 = pd.read_csv(\"data/GoogleMobility/2021_CH_Region_Mobility_Report.csv\")\n",
    "    mobDf2020 = mobDf2020.loc[mobDf2020[\"sub_region_1\"]==googleMobDict[cantonId]].set_index('date')\n",
    "    mobDf2021 = mobDf2021.loc[mobDf2021[\"sub_region_1\"]==googleMobDict[cantonId]].set_index('date')\n",
    "    interestedCols = ['retail_and_recreation_percent_change_from_baseline',\n",
    "                  'grocery_and_pharmacy_percent_change_from_baseline',\n",
    "                  'parks_percent_change_from_baseline',\n",
    "                  'transit_stations_percent_change_from_baseline',\n",
    "                  'workplaces_percent_change_from_baseline',\n",
    "                  'residential_percent_change_from_baseline'\n",
    "                 ]\n",
    "    mobDf2020 = mobDf2020[interestedCols]\n",
    "    mobDf2021 = mobDf2021[interestedCols]\n",
    "    mobDf2020.index = pd.to_datetime(mobDf2020.index)\n",
    "    mobDf2021.index = pd.to_datetime(mobDf2021.index)\n",
    "    mobDf = mobDf2020.append(mobDf2021)\n",
    "    df = df.join(mobDf)\n",
    "    \n",
    "    # attach KOF strigency index\n",
    "    kofDf = pd.read_csv(\"data/KOF/KOFStrigencyIndex.csv\")\n",
    "    kofDf = kofDf.set_index('date')\n",
    "    kofDf = kofDf[[\"ch.kof.stringency.\"+cantonId.lower()+\".stringency_plus\"]]\n",
    "    kofDf.rename(columns = {\"ch.kof.stringency.\"+cantonId.lower()+\".stringency_plus\":'kofStrigency'}, inplace = True)\n",
    "    kofDf.index = pd.to_datetime(kofDf.index)\n",
    "    df = df.join(kofDf)\n",
    "    \n",
    "    # attach all measures\n",
    "    measuresDf = pd.read_csv(\"data/measures/\"+cantonId+\".csv\")\n",
    "    measuresDf = measuresDf.set_index('Time')\n",
    "    measuresDf.index = pd.to_datetime(measuresDf.index)\n",
    "    df = df.join(measuresDf)\n",
    "    \n",
    "    \n",
    "    # attach holidays & vacations \n",
    "    holy = pd.read_csv(\"data/HolidayVacation/HolidayVacation.csv\").set_index(\"date\")[[cantonId]]\n",
    "    holy.rename(columns = {cantonId:'isHoliday'}, inplace = True)\n",
    "    holy.index = pd.to_datetime(holy.index)\n",
    "    df = df.join(holy)\n",
    "   \n",
    "    \n",
    "    # attach intervista mobility data\n",
    "    averageAndMedian = pd.read_csv(\"data/IntervistaMobility/Mittelwerte_und_Median_pro_Tag.csv\", encoding=\"mac_roman\")\n",
    "    averageAndMedian = averageAndMedian.loc[(averageAndMedian[\"Beschreibung\"] == \"Distanz\") & (averageAndMedian[\"Typ\"] == \"Median\")]\n",
    "    averageAndMedian = averageAndMedian.set_index(\"Datum\")\n",
    "    averageAndMedian = averageAndMedian[['D-CH','F-CH', 'I-CH']]\n",
    "    averageAndMedian.index = pd.to_datetime(averageAndMedian.index)\n",
    "    D_CH = ['AG','AI','AR', 'BE','BL', 'BS','LU','GR','NW', 'OW', 'SG', 'SH', 'SO', 'SZ', 'TG','GL','UR','ZG','ZH']\n",
    "    F_CH = ['FR', 'GE', 'JU', 'VD', 'VS', 'NE']\n",
    "    #'TI'\n",
    "    if cantonId in D_CH:\n",
    "        df = df.join(averageAndMedian[['D-CH']])\n",
    "        df.rename(columns = {'D-CH':'intervistaMob'}, inplace = True) \n",
    "    elif cantonId in F_CH:\n",
    "        df = df.join(averageAndMedian[['F-CH']])\n",
    "        df.rename(columns = {'F-CH':'intervistaMob'}, inplace = True) \n",
    "    else:\n",
    "        # cantonId = TI\n",
    "        df = df.join(averageAndMedian[['I-CH']])\n",
    "        df.rename(columns = {'I-CH':'intervistaMob'}, inplace = True)\n",
    "        \n",
    "   \n",
    "    # attach neighbor incidents (WEEKLY)\n",
    "    neigbors = {\n",
    "      'AG': ['BL','SO','BE','LU','ZH','Baden-Wurttemberg','ZG'],\n",
    "      'AI': ['AR','SG'],\n",
    "      'AR': ['AI','SG'],\n",
    "      'BE': ['AG','SO','JU','NE','FR','VD','VS','UR','NW','OW','LU'], \n",
    "      'BL': ['AG','BS','SO','Baden-Wurttemberg','Grand Est'], \n",
    "      'BS': ['BL','Baden-Wurttemberg','Grand Est'], \n",
    "      'FR': ['BE','VD','NE'], \n",
    "      'GE': ['VD','Auvergne Rhone Alpes'], \n",
    "      'GL': ['SG','SZ','UR','GR'], \n",
    "      'GR': ['SG','GL','UR','TI','Vorarlberg','Lombardia','Liechtenstein'], \n",
    "      'JU': ['BL','SO','BE','NE','Grand Est','Bourgogne Franche Comte'], \n",
    "      'LU': ['AG','BE','NW', 'OW','ZG','SZ'], \n",
    "      'NE': ['JU','BE','VD','FR','Bourgogne Franche Comte'], \n",
    "      'NW': ['OW','BE','LU','SZ','UR'], \n",
    "      'OW': ['NW','LU','BE','UR'], \n",
    "      'SG': ['AI','AR','TG','ZH','SZ','GL','GR','Vorarlberg','Liechtenstein'], \n",
    "      'SH': ['TG','ZH','Baden-Wurttemberg'], \n",
    "      'SO': ['BE','JU','BL','AG','Grand Est'],\n",
    "      'SZ': ['ZG','ZH','SG','GL','LU','NW','UR'], \n",
    "      'TG': ['SH','ZH','SG','Baden-Wurttemberg'], \n",
    "      'TI': ['UR','GR','Piemonte','Lombardia'], \n",
    "      'UR': ['TI','VS','GR','BE','NW','OW','SZ','GL'], \n",
    "      'VD': ['NE','GE','FR','VS','BE','Auvergne Rhone Alpes','Bourgogne Franche Comte'], \n",
    "      'VS': ['VD','BE','UR','Piemonte','Auvergne Rhone Alpes'], \n",
    "      'ZG': ['ZH','AG','LU','SZ'],\n",
    "      'ZH': ['SH','AG','ZG','SZ','SG','TG','Baden-Wurttemberg']\n",
    "    }\n",
    "    foph = pd.read_csv(\"data/FOPH/data/COVID19Cases_geoRegion.csv\", parse_dates=True)\n",
    "    # we have to convert each date string to datetime to merge the dataframes later\n",
    "    foph = foph.set_index('datum')\n",
    "    foph.index = pd.to_datetime(foph.index)\n",
    "    foph['rate_14_day_per_100k'] = (100000*foph['sum14d']) / foph['pop']\n",
    "    foph = foph[['geoRegion','rate_14_day_per_100k']]\n",
    "    foph.index = pd.to_datetime(foph.index)\n",
    "    ecdc = pd.read_csv(\"data/ECDC/ECDCsubnationalcaseweekly.csv\")\n",
    "    ecdc[['year_week']] = ecdc[['year_week']].applymap(lambda x: mondaysByWeekNr[x[0:4]+x[5:7]]) # mondaysByWeekNr[x[0:4]+x[6:8]]\n",
    "    ecdc = ecdc.set_index(\"year_week\")\n",
    "    ecdc = ecdc[['region_name','rate_14_day_per_100k']]\n",
    "    ecdc.index = pd.to_datetime(ecdc.index)\n",
    "    temp = pd.DataFrame(index=pd.date_range(start=start, end=end))\n",
    "    for n in neigbors[cantonId]:\n",
    "        if len(n) != 2:\n",
    "            internationalRegion = ecdc.loc[ecdc['region_name']==n][['rate_14_day_per_100k']]\n",
    "            temp[[n]] = internationalRegion\n",
    "            temp[[n]] = temp[[n]].interpolate(method='linear')\n",
    "            temp.loc[temp.index[0],[n]] = 0\n",
    "        else:\n",
    "            nationalRegion = foph.loc[foph['geoRegion']==n][['rate_14_day_per_100k']]\n",
    "            temp[[n]] = nationalRegion\n",
    "            temp.loc[temp.index[0],[n]] = 0\n",
    "            temp[[n]] = temp[[n]].interpolate(method='linear')\n",
    "    imputer = KNNImputer(n_neighbors=10, weights=\"distance\")\n",
    "    temp = pd.DataFrame(imputer.fit_transform(temp.values), index=temp.index, columns=temp.columns)\n",
    "    temp.columns = ['incidence_'+col for col in temp.columns]\n",
    "    temp['meanNeighborIncidence'] = temp.mean(axis=1)\n",
    "    temp['maxNeighborIncidence'] = temp.max(axis=1)\n",
    "    df = df.join(temp[['meanNeighborIncidence','maxNeighborIncidence']]) \n",
    "    \n",
    "    \n",
    "    # compute statistic weather for missing values\n",
    "    '''\n",
    "    statWeathDf = pd.read_csv(\"static_data/statistical_historicweather/statistical_\"+cantonId+\".csv\")\n",
    "    statWeathDf['date'] = statWeathDf.apply(lambda row: datetime.datetime(2020,int(row[\"month\"]),int(row[\"day\"])), axis=1)\n",
    "    statWeathDf = statWeathDf.set_index('date')\n",
    "    statWeathDf = statWeathDf[['temp.average_min','temp.average_max','clouds.mean','precipitation.mean']]\n",
    "    statWeathDf.columns = ['temp_min', 'temp_max', 'clouds', 'precipitation']\n",
    "    weather = statWeathDf\n",
    "    '''\n",
    "    \n",
    "    # compute historic weather from stored data\n",
    "    storedWeathDf = pd.read_csv(\"static_data/historicweather_from_19_03_2020_to_17_03_2021/\"+cantonId+\".csv\")\n",
    "    storedWeathDf = storedWeathDf.set_index('dt')\n",
    "    storedWeathDf = storedWeathDf[['main.temp_min','main.temp_max','clouds.all', 'rain.1h','snow.1h']]\n",
    "    storedWeathDf = storedWeathDf.fillna(0)\n",
    "    storedWeathDf['precipitation'] = storedWeathDf[['rain.1h','snow.1h']].sum(axis=1)\n",
    "    storedWeathDf = storedWeathDf[['main.temp_min','main.temp_max','clouds.all','precipitation']] \n",
    "\n",
    "    startDate = datetime.datetime(2020, 3, 19)\n",
    "    endDate = datetime.datetime(2021, 2, 3)\n",
    "    temp = pd.DataFrame(index=pd.date_range(start=startDate, end=endDate), columns=storedWeathDf.columns)\n",
    "    for day in pd.date_range(start=startDate, end=endDate):   \n",
    "        oneDay = storedWeathDf.filter(like=day.strftime('%Y-%m-%d'), axis=0)\n",
    "        temp.loc[day] = {'main.temp_min': oneDay['main.temp_min'].min(), \n",
    "                         'main.temp_max': oneDay['main.temp_max'].max(), \n",
    "                         'clouds.all': oneDay['clouds.all'].mean(),\n",
    "                         'precipitation': oneDay['precipitation'].sum()}\n",
    "    storedWeathDf = temp\n",
    "    storedWeathDf.columns = ['temp_min', 'temp_max', 'clouds', 'precipitation']\n",
    "    #weather = weather.append(storedWeathDf)\n",
    "    weather = storedWeathDf\n",
    "\n",
    "    # compute historic weather from recently loaded weather update\n",
    "    updateWeathDf = pd.read_csv(\"data/historicweatherupdate/\"+cantonId+\".csv\")\n",
    "    updateWeathDf = updateWeathDf.set_index('dt')\n",
    "    if 'snow.1h' in updateWeathDf.columns:\n",
    "        updateWeathDf = updateWeathDf[['main.temp_min','main.temp_max','clouds.all', 'rain.1h','snow.1h']]\n",
    "    else:\n",
    "        updateWeathDf['snow.1h'] = 0\n",
    "        updateWeathDf = updateWeathDf[['main.temp_min','main.temp_max','clouds.all', 'rain.1h','snow.1h']]\n",
    "    updateWeathDf = updateWeathDf.fillna(0)\n",
    "    updateWeathDf['precipitation'] = updateWeathDf[['rain.1h','snow.1h']].sum(axis=1)\n",
    "    updateWeathDf = updateWeathDf[['main.temp_min','main.temp_max','clouds.all','precipitation']] \n",
    "\n",
    "    endDate = datetime.datetime.strptime(updateWeathDf.index[-1], '%Y-%m-%d %H:%M:%S')\n",
    "    endDate = endDate.replace(hour=0, minute=0)\n",
    "    startDate = datetime.datetime(2021, 2, 4)\n",
    "    temp = pd.DataFrame(index=pd.date_range(start=startDate, end=endDate), columns=updateWeathDf.columns)\n",
    "    for day in pd.date_range(start=startDate, end=endDate):   \n",
    "        oneDay = updateWeathDf.filter(like=day.strftime('%Y-%m-%d'), axis=0)\n",
    "        temp.loc[day] = {'main.temp_min': oneDay['main.temp_min'].min(), \n",
    "                         'main.temp_max': oneDay['main.temp_max'].max(), \n",
    "                         'clouds.all': oneDay['clouds.all'].mean(),\n",
    "                         'precipitation': oneDay['precipitation'].sum()}\n",
    "    updateWeathDf = temp\n",
    "    updateWeathDf.columns = ['temp_min', 'temp_max', 'clouds', 'precipitation']\n",
    "    weather = weather.append(updateWeathDf)  \n",
    "    df = df.join(weather)\n",
    "    \n",
    "     \n",
    "    df.index.names = [\"date\"]\n",
    "    if not os.path.exists('data/merged'):\n",
    "        os.makedirs('data/merged')\n",
    "    df[start:end].to_csv('data/merged/'+cantonId+'.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laden-resistance",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# fill the missing data\n",
    "\n",
    "mergedDict = {}\n",
    "for cantonId in cantonKeys: \n",
    "    mergedDict[cantonId] = pd.read_csv(\"data/merged/\"+cantonId+\".csv\").set_index('date')[start:end]\n",
    "    mergedDict[cantonId].index = pd.to_datetime(mergedDict[cantonId].index)\n",
    "    \n",
    "interpolMet = 'linear'\n",
    "originalDict = mergedDict.copy()\n",
    "\n",
    "for cantonId in cantonKeys:\n",
    "    filled = mergedDict[cantonId].copy()\n",
    "    \n",
    "    # compute statistic weather for missing values\n",
    "    statWeathDf = pd.read_csv(\"static_data/statistical_historicweather/statistical_\"+cantonId+\".csv\")\n",
    "    statWeathDf['date'] = statWeathDf.apply(lambda row: datetime.datetime(2020,int(row[\"month\"]),int(row[\"day\"])), axis=1)\n",
    "    statWeathDf = statWeathDf.set_index('date')\n",
    "    statWeathDf = statWeathDf[['temp.average_min','temp.average_max','clouds.mean','precipitation.mean']]\n",
    "    statWeathDf.columns = ['temp_min', 'temp_max', 'clouds', 'precipitation']\n",
    "    filled[['temp_min', 'temp_max', 'clouds', 'precipitation']] = filled[['temp_min', 'temp_max', 'clouds', 'precipitation']].fillna(statWeathDf)\n",
    "    \n",
    "    # fill missing test entries\n",
    "    totalTestsInSwitzerland = pd.DataFrame(index=pd.date_range(start=datetime.datetime(2020, 2, 15), end=datetime.datetime(2020, 5, 22)))\n",
    "    temp = pd.read_csv(\"data/FOPH/data/COVID19Test_geoRegion_all.csv\")\n",
    "    temp = temp[temp['geoRegion']=='CHFL']\n",
    "    temp = temp.set_index('datum')\n",
    "    temp.index = pd.to_datetime(temp.index)\n",
    "    totalTestsInSwitzerland = totalTestsInSwitzerland.join(temp)  \n",
    "    totalTestsInSwitzerland = totalTestsInSwitzerland[['entries']]\n",
    "    totalTestsInSwitzerland.fillna(method='bfill', inplace=True)\n",
    "    testsByCanton = pd.read_csv(\"data/FOPH/data/COVID19Test_geoRegion_all.csv\")\n",
    "    testsByCanton = testsByCanton.set_index('datum')\n",
    "    testsByCanton.index = pd.to_datetime(testsByCanton.index)\n",
    "    sumSwitzerland = testsByCanton.loc[testsByCanton[\"geoRegion\"]=='CHFL'][['entries']]\n",
    "    sumCanton = testsByCanton.loc[testsByCanton[\"geoRegion\"]==cantonId][['entries']]\n",
    "    cantonalTestFraction = sumCanton['2020-05-23':'2020-06-05'].sum(axis=0).values[0]/sumSwitzerland['2020-05-23':'2020-06-05'].sum(axis=0).values[0]\n",
    "    #multiply this with cantonal test quotient\n",
    "    computedMissingEntries = totalTestsInSwitzerland['2020-02-15':'2020-05-22']*cantonalTestFraction \n",
    "    computedMissingEntries.rename(columns = {\"entries\":'test_entries'}, inplace = True)\n",
    "    filled[['test_entries']] = filled[['test_entries']].fillna(computedMissingEntries[['test_entries']])\n",
    "    # compute rest of test incidence\n",
    "    missingTestIncidents = 100000*(filled[['test_entries']]/pop)\n",
    "    missingTestIncidents.rename(columns = {\"test_entries\":'test_inz_entries'}, inplace = True)\n",
    "    filled[['test_inz_entries']] = filled[['test_inz_entries']].fillna(missingTestIncidents)  \n",
    "    \n",
    "    \n",
    "    # fill missing vaccine data\n",
    "    vaccine = ['AtLeastOneDosePersons per100PersonsTotal','COVID19FullyVaccPersons per100PersonsTotal']\n",
    "    filled.loc['2020-12-18',vaccine] = 0 #on 19. december 2020 swissmedic approved the first vaccine\n",
    "    filled[vaccine] = filled[vaccine].interpolate(method=interpolMet)\n",
    "    filled[vaccine] = filled[vaccine].fillna(method='ffill')\n",
    "    filled[vaccine] = filled[vaccine].fillna(0)\n",
    "     \n",
    "    # fill missing total hospital capacities\n",
    "    hospitalCols = ['ICU_Capacity','ICU_FreeCapacity','Total_Capacity','Total_FreeCapacity']\n",
    "    filled[hospitalCols] = filled[hospitalCols].interpolate(method=interpolMet)\n",
    "    filled[hospitalCols] = filled[hospitalCols].fillna(method='bfill')\n",
    "    # because of Appenzell Innerrhoden\n",
    "    #filled[hospitalCols] = filled[hospitalCols].fillna(0)\n",
    "\n",
    "    # fill in missing hospital capacity datanearest\n",
    "    # we make the assumption that at the beginning there were no covid patients\n",
    "    filled.loc['2020-02-15',['Total_Covid19Patients','ICU_Covid19Patients']] = 0\n",
    "    patientCols = ['Total_Covid19Patients','Total_NonCovid19Patients','Total_AllPatients','ICU_Covid19Patients',\n",
    "                   'ICU_NonCovid19Patients','ICU_AllPatients']\n",
    "    filled[patientCols] = filled[patientCols].interpolate(method=interpolMet)\n",
    "    filled[['Total_NonCovid19Patients','ICU_NonCovid19Patients']] = filled[['Total_NonCovid19Patients','ICU_NonCovid19Patients']].fillna(method='bfill')\n",
    "    # Covid19Patients + NonCovid19Patients = AllPatients\n",
    "    filled['Total_AllPatients'].fillna(filled[['Total_Covid19Patients','Total_NonCovid19Patients']].sum(axis=1), inplace=True)\n",
    "    filled['ICU_AllPatients'].fillna(filled[['ICU_Covid19Patients','ICU_NonCovid19Patients']].sum(axis=1), inplace=True)\n",
    "    # because of Appenzell Innerrhoden\n",
    "    #filled[patientCols] = filled[patientCols].fillna(0)\n",
    "    \n",
    "    # fill in missing Google mobility data\n",
    "    googleMobilityCols = ['retail_and_recreation_percent_change_from_baseline','grocery_and_pharmacy_percent_change_from_baseline',\n",
    "    'parks_percent_change_from_baseline','transit_stations_percent_change_from_baseline','workplaces_percent_change_from_baseline'\n",
    "    ,'residential_percent_change_from_baseline']\n",
    "    \n",
    "    # THIS IS ONLY FOR A PLOT\n",
    "    '''\n",
    "    missingDf = filled.copy()\n",
    "    missingDf = missingDf.reset_index()\n",
    "    plt.figure(figsize=(9,4))\n",
    "    missingDf = missingDf[['date','workplaces_percent_change_from_baseline']]\n",
    "    plt.scatter(missingDf['date'],\n",
    "            missingDf['workplaces_percent_change_from_baseline'],\n",
    "           marker='o',\n",
    "           color='blue',\n",
    "           label='workplace mobility AI',\n",
    "           alpha=1)\n",
    "    plt.xlabel('date')\n",
    "    plt.ylabel('percent change from baseline')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    missingDf = missingDf.set_index('date')\n",
    "    missingDf['swiss workplace average'] = float('nan')\n",
    "    '''\n",
    "    \n",
    "    # use the swiss average when possible\n",
    "    for year in [2020,2021]:\n",
    "        googleMobCH = pd.read_csv(\"data/GoogleMobility/\"+str(year)+\"_CH_Region_Mobility_Report.csv\")\n",
    "        googleMobCH = googleMobCH.loc[googleMobCH[\"sub_region_1\"].isna()].set_index('date')[googleMobilityCols]\n",
    "        googleMobCH.index = pd.to_datetime(googleMobCH.index)\n",
    "        for col in googleMobilityCols:\n",
    "            \n",
    "            filled[col] = filled[col].fillna(googleMobCH[col])\n",
    "            \n",
    "            # THIS IS ONLY FOR A PLOT\n",
    "            '''\n",
    "            if col == 'workplaces_percent_change_from_baseline':\n",
    "                display(googleMobCH[col])\n",
    "                missingDf['swiss workplace average'] = missingDf['swiss workplace average'].fillna(googleMobCH[col])\n",
    "            '''\n",
    "              \n",
    "    # for the rest interpolate\n",
    "    filled[googleMobilityCols] = filled[googleMobilityCols].interpolate(method=interpolMet)\n",
    "    \n",
    "    # THIS IS ONLY FOR A PLOT\n",
    "    '''\n",
    "    missingDf['complement'] = missingDf[missingDf['workplaces_percent_change_from_baseline'].isnull()]['swiss workplace average']\n",
    "    missingDf = missingDf.reset_index()\n",
    "    plt.figure(figsize=(9,4))\n",
    "    plt.scatter(missingDf['date'],\n",
    "            missingDf['workplaces_percent_change_from_baseline'],\n",
    "           marker='o',\n",
    "           color='blue',\n",
    "           label='workplace mobility AI',\n",
    "           alpha=1)\n",
    "    plt.scatter(missingDf['date'],\n",
    "            missingDf['complement'],\n",
    "           marker='o',\n",
    "           color='red',\n",
    "           label='workplace mobility Swiss average',\n",
    "           alpha=1)\n",
    "    plt.xlabel('date')\n",
    "    plt.ylabel('percent change from baseline')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    '''\n",
    "    \n",
    "    # fill in missing Intervista mobility data \n",
    "    filled[['intervistaMob']] = filled[['intervistaMob']].interpolate(method=interpolMet)\n",
    "    filled['intervistaMob'].fillna(method='ffill', inplace=True)\n",
    "        \n",
    "    # fill in missing r values\n",
    "    rvalues = ['median_R_mean','median_R_highHPD','median_R_lowHPD']\n",
    "    filled[rvalues] = filled[rvalues].interpolate(method=interpolMet)\n",
    "    # move them 14 days forward to match r erstimation value with publication date\n",
    "    filled[rvalues] = filled[rvalues].shift(periods=14)\n",
    "    filled[rvalues] = filled[rvalues].fillna(method='ffill')\n",
    "    filled[rvalues] = filled[rvalues].fillna(method='bfill')\n",
    "  \n",
    "    # fill in missing variants\n",
    "    # first detected case for variants of concerne in Switzerland is 2020-10-14\n",
    "    #variants = ['lower_ci_day','upper_ci_day','anteil_pos']\n",
    "    #filled.loc['2020-10-13',['lower_ci_day','anteil_pos']] = 0\n",
    "    #filled.loc['2020-10-13',['upper_ci_day']] = 100\n",
    "    variants = ['prct_'+v for v in variantList]\n",
    "    filled[variants] = filled[variants].interpolate(method=interpolMet)\n",
    "    filled[variants] = filled[variants].fillna(method='ffill')\n",
    "    filled[variants] = filled[variants].fillna(method='bfill')\n",
    "\n",
    "    # fill in daily incoming missing data\n",
    "    zeroAndInterpolate = ['case_entries','case_inz_entries','hosp_inz_entries',\n",
    "                          'death_inz_entries','case_inzsumTotal','hosp_inzsumTotal','death_inzsumTotal',\n",
    "                          'test_inzsumTotal','Cases inz_entries 0 - 9',\n",
    "                          'Cases inz_entries 10 - 19','Cases inz_entries 20 - 29','Cases inz_entries 30 - 39',\n",
    "                          'Cases inz_entries 40 - 49','Cases inz_entries 50 - 59','Cases inz_entries 60 - 69',\n",
    "                          'Cases inz_entries 70 - 79','Cases inz_entries 80+','Cases inz_entries male',\n",
    "                          'Cases inz_entries female','Cases inzsumTotal 0 - 9','Cases inzsumTotal 10 - 19',\n",
    "                          'Cases inzsumTotal 20 - 29','Cases inzsumTotal 30 - 39','Cases inzsumTotal 40 - 49',\n",
    "                          'Cases inzsumTotal 50 - 59','Cases inzsumTotal 60 - 69','Cases inzsumTotal 70 - 79',\n",
    "                          'Cases inzsumTotal 80+','Cases inzsumTotal male','Cases inzsumTotal female',\n",
    "                          'Death inz_entries 0 - 9',\n",
    "                          'Death inz_entries 10 - 19','Death inz_entries 20 - 29','Death inz_entries 30 - 39',\n",
    "                          'Death inz_entries 40 - 49','Death inz_entries 50 - 59','Death inz_entries 60 - 69',\n",
    "                          'Death inz_entries 70 - 79','Death inz_entries 80+','Death inz_entries male',\n",
    "                          'Death inz_entries female','Death inzsumTotal 0 - 9','Death inzsumTotal 10 - 19',\n",
    "                          'Death inzsumTotal 20 - 29','Death inzsumTotal 30 - 39','Death inzsumTotal 40 - 49',\n",
    "                          'Death inzsumTotal 50 - 59','Death inzsumTotal 60 - 69','Death inzsumTotal 70 - 79',\n",
    "                          'Death inzsumTotal 80+','Death inzsumTotal male','Death inzsumTotal female',\n",
    "                          'Hosp inz_entries 0 - 9','Hosp inz_entries 10 - 19',\n",
    "                          'Hosp inz_entries 20 - 29','Hosp inz_entries 30 - 39','Hosp inz_entries 40 - 49',\n",
    "                          'Hosp inz_entries 50 - 59','Hosp inz_entries 60 - 69','Hosp inz_entries 70 - 79',\n",
    "                          'Hosp inz_entries 80+','Hosp inz_entries male','Hosp inz_entries female',\n",
    "                          'Hosp inzsumTotal 0 - 9','Hosp inzsumTotal 10 - 19','Hosp inzsumTotal 20 - 29',\n",
    "                          'Hosp inzsumTotal 30 - 39','Hosp inzsumTotal 40 - 49','Hosp inzsumTotal 50 - 59',\n",
    "                          'Hosp inzsumTotal 60 - 69','Hosp inzsumTotal 70 - 79','Hosp inzsumTotal 80+',\n",
    "                          'Hosp inzsumTotal male','Hosp inzsumTotal female']\n",
    "    filled.loc['2020-02-15',zeroAndInterpolate] = 0\n",
    "    filled[zeroAndInterpolate] = filled[zeroAndInterpolate].interpolate(method=interpolMet)\n",
    "    \n",
    "    tests = ['test_entries','test_inz_entries']\n",
    "    filled[tests] = filled[tests].fillna(method='ffill')\n",
    "    \n",
    "    \n",
    "    if not os.path.exists('data/filled'):\n",
    "        os.makedirs('data/filled')\n",
    "    filled.to_csv('data/filled/'+cantonId+'.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enclosed-colombia",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# check complete data if there are any NaNs left\n",
    "for cantonId in cantonKeys:\n",
    "    filled = pd.read_csv('data/filled/'+cantonId+'.csv')\n",
    "    for col in filled.columns:\n",
    "        if filled[col].isna().sum() != 0:\n",
    "            print(cantonId+\" \"+col+\" (#NaN/#NotNaN): (\" + str(filled[col].isna().sum())+\"/\"+str(filled[col].notna().sum())+\")\")\n",
    "            #display(filled[[col]])\n",
    "            #dict[cantonId][col].plot(kind='line',y=[col], figsize=(20,10))\n",
    "            #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floral-genome",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# plotting original data vs filled data\n",
    "#for col in dict['AG'].columns:\n",
    "#    comparingDf = pd.concat([dict['AG'][[col]],originalData[[\"original_\"+col]]], axis=1)\n",
    "    #comparingDf[['original_'+col]].reset_index().plot(kind='scatter', x=['date'], y=['original_'+col], figsize=(20,10))\n",
    "    #comparingDf['2020-02-15':'2021-04-05'].plot(kind='line',y=[col], figsize=(20,10))\n",
    "#for col in pd.read_csv(\"data/filled/\"+'AG'+\".csv\").set_index('date').columns:\n",
    "#    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seasonal-bradford",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# FEATURE ENGINEERING\n",
    "filledDict = {}\n",
    "\n",
    "for cantonId in cantonKeys:\n",
    "    filledDict[cantonId] = pd.read_csv(\"data/filled/\"+cantonId+\".csv\").set_index('date')\n",
    "    \n",
    "    dailyFeatures = filledDict[cantonId].copy()\n",
    "    dailyFeatures.index = pd.to_datetime(dailyFeatures.index) \n",
    "\n",
    "    # summarize mask mandatories\n",
    "    maskMandatories = [ 'Mask mandatory in publicly accessible establishments/ spaces (shops etc.)',\n",
    "                       'Mask mandatory in public transport','Masks mandatory in schools','Masks mandatory at work']\n",
    "    dailyFeatures[['maskMandatories']] = dailyFeatures[maskMandatories].sum(axis=1)\n",
    "    dailyFeatures.drop(maskMandatories, axis=1, inplace=True)\n",
    "    \n",
    "    # r value accuracy\n",
    "    dailyFeatures[['R_error']] = dailyFeatures['median_R_highHPD']-dailyFeatures['median_R_lowHPD']\n",
    "    dailyFeatures.drop(['median_R_highHPD','median_R_lowHPD'],axis=1, inplace=True)\n",
    "    \n",
    "    # test positivity rate\n",
    "    dailyFeatures[['testPositvity_7dayAverage']] = (dailyFeatures['case_entries'].rolling(window=7, min_periods=1).mean()/dailyFeatures['test_entries'].rolling(window=7, min_periods=1).mean()).rolling(window=7, min_periods=1).mean()\n",
    "\n",
    "\n",
    "    # hospital capacities\n",
    "    hospCap = [ 'ICU_AllPatients',\n",
    "     'ICU_Covid19Patients',\n",
    "     'ICU_Capacity',\n",
    "     'Total_AllPatients',\n",
    "     'Total_Covid19Patients',\n",
    "     'Total_Capacity',\n",
    "     'ICU_NonCovid19Patients',\n",
    "     'ICU_FreeCapacity',\n",
    "     'Total_NonCovid19Patients',\n",
    "     'Total_FreeCapacity']\n",
    "    staticCantonal = pd.read_excel(\"static_data/staticCantonalData.xlsx\").set_index('canton').transpose()\n",
    "    dailyFeatures[[col + \"_inz\" for col in hospCap]] = 100000*(dailyFeatures[hospCap]/staticCantonal.loc[[cantonId]]['residents'][0])\n",
    "    dailyFeatures.drop(hospCap,axis=1, inplace=True)\n",
    "\n",
    "    # summing up age groups\n",
    "    for c1 in ['Cases','Death','Hosp']:\n",
    "        for c2 in ['inz_entries','inzsumTotal']:\n",
    "            dailyFeatures[[c1+\" \"+c2+\" \"+\"0 - 19\"]] = dailyFeatures[[c1+\" \"+c2+\" \"+\"0 - 9\",c1+\" \"+c2+\" \"+\"10 - 19\"]].sum(axis=1)\n",
    "            dailyFeatures[[c1+\" \"+c2+\" \"+\"20 - 39\"]] = dailyFeatures[[c1+\" \"+c2+\" \"+\"20 - 29\",c1+\" \"+c2+\" \"+\"30 - 39\"]].sum(axis=1)\n",
    "            dailyFeatures[[c1+\" \"+c2+\" \"+\"40 - 59\"]] = dailyFeatures[[c1+\" \"+c2+\" \"+\"40 - 49\",c1+\" \"+c2+\" \"+\"50 - 59\"]].sum(axis=1)\n",
    "            dailyFeatures[[c1+\" \"+c2+\" \"+\"60+\"]] = dailyFeatures[[c1+\" \"+c2+\" \"+\"60 - 69\",c1+\" \"+c2+\" \"+\"70 - 79\",c1+\" \"+c2+\" \"+\"80+\"]].sum(axis=1)\n",
    "            for agegroup in [\"0 - 9\",\"10 - 19\",\"20 - 29\",\"30 - 39\",\"40 - 49\",\"50 - 59\",\"60 - 69\",\"70 - 79\",\"80+\"]:\n",
    "                dailyFeatures.drop([c1+\" \"+c2+\" \"+agegroup], axis=1, inplace=True)\n",
    "        \n",
    "        \n",
    "    toBeSmoothed = ['case_inz_entries',\n",
    "                    'hosp_inz_entries',\n",
    "                    'death_inz_entries',\n",
    "                    'test_entries',\n",
    "                    'test_inz_entries',\n",
    "                    'retail_and_recreation_percent_change_from_baseline',\n",
    "                     'grocery_and_pharmacy_percent_change_from_baseline',\n",
    "                     'parks_percent_change_from_baseline',\n",
    "                     'transit_stations_percent_change_from_baseline',\n",
    "                     'workplaces_percent_change_from_baseline',\n",
    "                     'residential_percent_change_from_baseline',\n",
    "                     'intervistaMob']\n",
    "    for f in toBeSmoothed:\n",
    "        dailyFeatures[[f+\"_7dayAverage\"]] = dailyFeatures[[f]].rolling(window=7, min_periods=1).mean()\n",
    "\n",
    "  \n",
    "    #display([c for c in dailyFeatures.columns])\n",
    "    if not os.path.exists('data/dailyFeatures'):\n",
    "        os.makedirs('data/dailyFeatures')\n",
    "    dailyFeatures.to_csv('data/dailyFeatures/'+cantonId+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powerful-pearl",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cantonId in cantonKeys:\n",
    "    data = pd.read_csv('data/dailyFeatures/'+cantonId+'.csv')\n",
    "    for col in data.columns:\n",
    "        if data[col].isna().sum() != 0:\n",
    "            display(cantonId)\n",
    "            print(col+\" (#NaN/#NotNaN): (\" + str(data[col].isna().sum())+\"/\"+str(data[col].notna().sum())+\")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infectious-nursery",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# CONSTRUCTING INPUT/OUTPUT INTERVALS\n",
    "\n",
    "weeksIn = 3\n",
    "weeksOut = 4\n",
    "\n",
    "listOfInputIntervals = []\n",
    "listOfOutputIntervals = []\n",
    "for e in pd.date_range(start=pd.Timestamp(start),end=pd.Timestamp(end), freq='D'):\n",
    "    if (e+timedelta(days = (weeksIn*7)+(weeksOut*7)-1) <= pd.Timestamp(end).date()):\n",
    "        tempInputList = []\n",
    "        for week in range(0,weeksIn):\n",
    "            tempInputList.append(((e+timedelta(days = week*7)).date(),(e+timedelta(days = ((week+1)*7)-1)).date()))\n",
    "        listOfInputIntervals.append(tempInputList)\n",
    "        \n",
    "        tempOutputList = []\n",
    "        for week in range(0,weeksOut):\n",
    "            tempOutputList.append(((e+timedelta(days = weeksIn*7+week*7)).date(),(e+timedelta(days = weeksIn*7+((week+1)*7)-1)).date()))\n",
    "        listOfOutputIntervals.append(tempOutputList)                      \n",
    "    \n",
    "\n",
    "display(listOfInputIntervals)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aquatic-ranking",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "startTimer = time.time()\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "dailyFeaturesDict = {} \n",
    "\n",
    "# average features\n",
    "# features which will be averaged over the whole input interval\n",
    "\n",
    "    \n",
    "# direct features\n",
    "# features which will be direct input for every day of the input interval\n",
    "# attention: this can potentially increase the number of input features significantly\n",
    "# added features are len(directFea)*daysIn\n",
    "# only add features for which have a large variance from one day to another day\n",
    "\n",
    "# direct feautres = output features\n",
    "\n",
    "outputFeatures = ['case_inz_entries_7dayAverage',\n",
    "                  'hosp_inz_entries_7dayAverage',\n",
    "                  'death_inz_entries_7dayAverage',\n",
    "                  'testPositvity_7dayAverage',\n",
    "                  'transit_stations_percent_change_from_baseline_7dayAverage',\n",
    "                  'workplaces_percent_change_from_baseline_7dayAverage'                        \n",
    "]\n",
    "\n",
    "# CONSTRUCTING ACTUAL INPUTS\n",
    "for cantonId in cantonKeys:\n",
    "    display(cantonId)\n",
    "    dailyFeaturesDict[cantonId] = pd.read_csv(\"data/dailyFeatures/\"+cantonId+\".csv\").set_index('date')\n",
    "    dailyFeaturesDict[cantonId].index = pd.to_datetime(dailyFeaturesDict[cantonId].index)\n",
    "    averageFeatures = dailyFeaturesDict[cantonId].columns\n",
    "    \n",
    "    # we remove the 7 day average features from averagefeatures list because we do not need their mean again\n",
    "    averageFeatures = [e for e in averageFeatures if e not in outputFeatures]\n",
    "\n",
    "    \n",
    "    features = pd.DataFrame()\n",
    "    \n",
    "    # construction of input features\n",
    "    listCounter = 0\n",
    "    \n",
    "    for inputList in listOfInputIntervals: # for every input week set\n",
    "        # we create a feature row which we append at the end to the features dataframe of the canton\n",
    "        featureRow = pd.DataFrame()\n",
    "        \n",
    "        weekNumber = 0\n",
    "        \n",
    "        for inputTuple in inputList: # for all input weeks\n",
    "            timeFrame = dailyFeaturesDict[cantonId][inputTuple[0]:inputTuple[1]]\n",
    "\n",
    "            # add the mean of the week for all features\n",
    "            temp = timeFrame[averageFeatures].mean().to_frame().transpose()\n",
    "            temp.columns = [f + \"_weekMean_\" + str(weekNumber) for f in averageFeatures]\n",
    "            \n",
    "            \n",
    "            for f in outputFeatures: # for all output features\n",
    "                y = timeFrame[[f]].values\n",
    "             \n",
    "                # add last value of the input week\n",
    "                temp[[f +\"_last_\"+str(weekNumber)]] = y[6][0]\n",
    "                \n",
    "                \n",
    "                # short term trend\n",
    "                temp[[f +\"_shortTrend_\"+str(weekNumber)]] = (y[6][0]-y[0][0])/7\n",
    "                \n",
    "                \n",
    "                s = np.sort(y, axis=None)\n",
    "                # add range of the input week\n",
    "                #temp[[f +\"_quartilerange_\"+str(weekNumber)]] = (s[4]+s[5]/2)-(s[1]+s[2]/2)\n",
    "                temp[[f +\"_range_\"+str(weekNumber)]] = s[6]-s[0]\n",
    "                # add median of the input week\n",
    "                temp[[f +\"_median_\"+str(weekNumber)]] = s[3]\n",
    "                \n",
    "            \n",
    "            featureRow = pd.concat([featureRow, temp], axis = 1)\n",
    "            weekNumber = weekNumber + 1\n",
    "        \n",
    "        # add the sum of the KOF strigency index from start of the pandemic until the last day of the input\n",
    "        featureRow[['kofStrigency sumtotal']] = dailyFeaturesDict[cantonId][start:inputList[-1][1]][['kofStrigency']].sum()[0]\n",
    "        \n",
    "        # longterm trend\n",
    "        wholeTimeFrame = dailyFeaturesDict[cantonId][inputList[0][0]:inputList[-1][1]]\n",
    "        for f in outputFeatures: # for every output feature\n",
    "            y = wholeTimeFrame[[f]].values\n",
    "            featureRow[[f +\"_longtermTrend\"]] = (y[20][0]-y[0][0])/21 \n",
    "        \n",
    "        # limited future features (only one future week, but only week mean)\n",
    "        limitedFutureFeatures = ['temp_min','temp_max','clouds','precipitation', 'kofStrigency',\n",
    "                         'Borders','Events','Gatherings/private events',\n",
    "                         'Demonstrations','Primary (includes kindergarten) and lower secondary school',\n",
    "                         'Upper secondary school, vocational schools and higher education',\n",
    "                         'Universities and other educational establishments',\n",
    "                         'Mountain railways',\n",
    "                         'Homeworking',\n",
    "                         'Restaurants',\n",
    "                         'Discos/Nightclubs',\n",
    "                         'Shops/Markets',\n",
    "                         'Penalties',\n",
    "                         'Cultural, entertainment and recreational facilities',\n",
    "                         'Sport/Wellness facilities',\n",
    "                         'Sport activities',\n",
    "                         'Religious services',\n",
    "                         'Singing allowed']\n",
    "        ot = listOfOutputIntervals[listCounter][0]  #select only the first future week\n",
    "        timeFrameOutput = dailyFeaturesDict[cantonId][ot[0]:ot[1]]\n",
    "        temp = timeFrameOutput[limitedFutureFeatures].mean().to_frame().transpose()\n",
    "        temp.columns = [f + \"_futureWeekMean_0\" for f in limitedFutureFeatures]\n",
    "        featureRow = pd.concat([featureRow, temp], axis = 1)\n",
    "        \n",
    "        # limited future features direct (only one future week, but all 7 entires)\n",
    "        limitedFutureFeaturesDirect = ['temp_min','temp_max','clouds','precipitation']\n",
    "        ot = listOfOutputIntervals[listCounter][0]  #select only the first future week\n",
    "        timeFrameOutput = dailyFeaturesDict[cantonId][ot[0]:ot[1]]\n",
    "        temp = pd.DataFrame(timeFrameOutput[limitedFutureFeaturesDirect].values.flatten(order='F')).transpose()\n",
    "        temp.columns =  [f + '_future_day_'+str(d) for f in limitedFutureFeaturesDirect for d in range(0,7)]\n",
    "        featureRow = pd.concat([featureRow, temp], axis = 1)\n",
    "        \n",
    "        # unlimited future features (as much weeks as selected output weeks, but only week mean)\n",
    "        unlimitedFutureFeatures = ['isHoliday']\n",
    "        futureWeekNumber = 0\n",
    "        for outPutTuple in listOfOutputIntervals[listCounter]:\n",
    "            timeFrameOutput = dailyFeaturesDict[cantonId][outPutTuple[0]:outPutTuple[1]]\n",
    "            temp = timeFrameOutput[unlimitedFutureFeatures].mean().to_frame().transpose()\n",
    "            temp.columns = [f + \"_futureWeekMean_\" + str(futureWeekNumber) for f in unlimitedFutureFeatures]\n",
    "            featureRow = pd.concat([featureRow, temp], axis = 1)\n",
    "            futureWeekNumber = futureWeekNumber + 1\n",
    "        \n",
    "        # increase the list counter to get the next corresponding output\n",
    "        listCounter = listCounter + 1\n",
    "        \n",
    "        # add the last input day date\n",
    "        lastInputDay = inputList[weeksIn-1][1]\n",
    "        featureRow['lastInputDay'] = lastInputDay\n",
    "        \n",
    "        \n",
    "        features = features.append(featureRow, ignore_index=True)\n",
    "          \n",
    "    staticCantonal = pd.read_excel(\"static_data/staticCantonalData.xlsx\").set_index('canton').transpose()\n",
    "    \n",
    "    # households\n",
    "    for h in ['2PersonHouseholds','3PersonHouseholds', '4PersonHouseholds', '5PersonHouseholds','6+PersonHouseholds']:\n",
    "        features[[h+\"_perc\"]] = staticCantonal.loc[[cantonId]][h][0]/staticCantonal.loc[[cantonId]]['totalHousholds'][0]\n",
    "    features[['averageHousehold']] = staticCantonal.loc[[cantonId]]['residents'][0]/staticCantonal.loc[[cantonId]]['totalHousholds'][0]\n",
    "    # add static features\n",
    "    staticFeatures = ['percentage 65 years or over','urbanPopulationPercent','homeownershipPercent', \n",
    "                      'livingSpaceInm2','carsPer1000inhabitants', 'publicTransportationPercent',\n",
    "                      'privateMotorisedTransportPercent','DoctorsPer100Kinhabitants','residentsPerKm2']\n",
    "    for f in staticFeatures:\n",
    "        features[[f]] = staticCantonal.loc[[cantonId]][f][0]\n",
    "    # construct settlement area feature\n",
    "    residents = staticCantonal.loc[[cantonId]]['residents'][0]\n",
    "    settlementArea = staticCantonal.loc[[cantonId]]['areaInKm2'][0]*(staticCantonal.loc[[cantonId]]['settlementAreaPercent'][0]/100)\n",
    "    features[['residentsPerKm2SettlementArea']] = residents/settlementArea\n",
    "    \n",
    "    \n",
    "    residentsInSwitzerland = staticCantonal[['residents']].sum()[0]\n",
    "    residentsInCanton = staticCantonal.loc[[cantonId]]['residents'][0]\n",
    "    features[['population_perc']] = residentsInCanton/residentsInSwitzerland\n",
    "    \n",
    "    temp = pd.DataFrame([i for i in range(0,features.shape[0])],columns =['daysSincePandemicStart'])\n",
    "    features = pd.concat([features, temp], axis = 1) \n",
    "    \n",
    "    #display([col for col in features])\n",
    "    #display(features)\n",
    "    if not os.path.exists('data/features'):\n",
    "        os.makedirs('data/features')\n",
    "    features.to_csv('data/features/'+cantonId+'.csv', index=False)\n",
    "\n",
    "display(\"----------End of evaluating (%s)----------\" % (time.time() - startTimer))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prime-institution",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# outputs \n",
    "outputCols = outputFeatures\n",
    "# CONSTRUCTING CORRECT OUTPUTS\n",
    "for cantonId in cantonKeys:\n",
    "    outputs = pd.DataFrame()\n",
    "    for outputList in listOfOutputIntervals:\n",
    "        outputRow = pd.DataFrame()\n",
    "        weekNumber = 0\n",
    "        for outputTuple in outputList:\n",
    "            temp = dailyFeaturesDict[cantonId][outputTuple[0]:outputTuple[1]][outputCols] \n",
    "            temp = temp.iloc[-1].to_frame().transpose() #this uses the last value of the output week\n",
    "            #temp = temp.mean().to_frame().transpose() #this uses the mean of the output week\n",
    "            temp = temp.reset_index(drop=True)\n",
    "            temp.columns = [\"output_\"+f + \"_\" + str(weekNumber) for f in outputCols]\n",
    "            weekNumber = weekNumber + 1\n",
    "            outputRow = pd.concat([outputRow, temp], axis = 1)\n",
    "        outputs = outputs.append(outputRow, ignore_index=True)\n",
    "    \n",
    "    if not os.path.exists('data/outputs'):\n",
    "        os.makedirs('data/outputs')\n",
    "    outputs.to_csv('data/outputs/'+cantonId+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "diverse-confidentiality",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BE',\n",
       " 'GL',\n",
       " 'SH',\n",
       " 'GE',\n",
       " 'TI',\n",
       " 'LU',\n",
       " 'FR',\n",
       " 'JU',\n",
       " 'TG',\n",
       " 'SO',\n",
       " 'AR',\n",
       " 'GR',\n",
       " 'NE',\n",
       " 'AG',\n",
       " 'OW',\n",
       " 'BL',\n",
       " 'BS',\n",
       " 'NW',\n",
       " 'ZG',\n",
       " 'UR',\n",
       " 'AI',\n",
       " 'VD',\n",
       " 'ZH',\n",
       " 'VS',\n",
       " 'SG',\n",
       " 'SZ']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['ZG',\n",
       " 'SO',\n",
       " 'NE',\n",
       " 'GR',\n",
       " 'GL',\n",
       " 'AG',\n",
       " 'VD',\n",
       " 'TG',\n",
       " 'BE',\n",
       " 'GE',\n",
       " 'VS',\n",
       " 'AI',\n",
       " 'FR',\n",
       " 'SG',\n",
       " 'TI',\n",
       " 'SZ',\n",
       " 'NW',\n",
       " 'AR',\n",
       " 'LU',\n",
       " 'BS',\n",
       " 'JU',\n",
       " 'SH',\n",
       " 'BL',\n",
       " 'UR',\n",
       " 'ZH',\n",
       " 'OW']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['BL',\n",
       " 'AR',\n",
       " 'GL',\n",
       " 'JU',\n",
       " 'GR',\n",
       " 'VD',\n",
       " 'UR',\n",
       " 'VS',\n",
       " 'SO',\n",
       " 'OW',\n",
       " 'SZ',\n",
       " 'BS',\n",
       " 'TG',\n",
       " 'ZH',\n",
       " 'GE',\n",
       " 'NE',\n",
       " 'ZG',\n",
       " 'SH',\n",
       " 'NW',\n",
       " 'BE',\n",
       " 'TI',\n",
       " 'FR',\n",
       " 'SG',\n",
       " 'AI',\n",
       " 'AG',\n",
       " 'LU']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['BS',\n",
       " 'BL',\n",
       " 'SZ',\n",
       " 'TI',\n",
       " 'NW',\n",
       " 'ZH',\n",
       " 'SG',\n",
       " 'VD',\n",
       " 'OW',\n",
       " 'NE',\n",
       " 'TG',\n",
       " 'FR',\n",
       " 'BE',\n",
       " 'JU',\n",
       " 'SO',\n",
       " 'AG',\n",
       " 'LU',\n",
       " 'AI',\n",
       " 'AR',\n",
       " 'ZG',\n",
       " 'GE',\n",
       " 'UR',\n",
       " 'VS',\n",
       " 'GL',\n",
       " 'SH',\n",
       " 'GR']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Complete: 11674'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Training set: 8646(0.7406201816001371)'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Test set 1: 702(0.060133630289532294)'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Test set 2: 842(0.0721260921706356)'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Validation set 1: 698(0.05979098852150077)'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Validation set 2: 786(0.06732910741819428)'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# categorizes data into test sets, validation sets and training set\n",
    "import random\n",
    "intervalSize = 14 #two week intervals for test/validation set 2\n",
    "# test set 1 are the newest 28 days\n",
    "# validation set 1 are the second newest 28 days\n",
    "\n",
    "dfList = []\n",
    "\n",
    "\n",
    "#for cantonId in cantonKeys:\n",
    "for cantonId in cantonKeys:\n",
    "    inputs = pd.read_csv('data/features/'+cantonId+'.csv')\n",
    "    outputs = pd.read_csv('data/outputs/'+cantonId+'.csv')\n",
    "    length = inputs.shape[0]\n",
    "    \n",
    "    df = pd.concat([inputs,outputs], axis=1)\n",
    "    \n",
    "    df[['cantonId']] = cantonId\n",
    "    # mark all as train set (default)\n",
    "    df[['category']] = 'train'\n",
    "    \n",
    "    # mark test set 1\n",
    "    df.loc[(length-28):,['category']] = 'test 1'\n",
    "    \n",
    "    # mark validation set 1\n",
    "    df.loc[(length-56):(length-28),['category']] = 'validation 1'\n",
    "    \n",
    "    dfList.append((cantonId,df))\n",
    "\n",
    "    \n",
    "def hasConflict(list1, list2):\n",
    "    for i in range(0, len(list1)):\n",
    "        if list1[i] == list2[i]:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def sortListBasedOnSpecificOrder(inputList, orderingList):\n",
    "    newList = []\n",
    "    for e in orderingList:\n",
    "        # search in input list\n",
    "        for i in inputList:\n",
    "            if i[0] == e:\n",
    "                #found\n",
    "                newList.append(i)\n",
    "    return newList\n",
    "\n",
    "def printFirstsInTuple(inputList):\n",
    "    for e in inputList:\n",
    "        print(e[0])\n",
    "    print(\" \")\n",
    "    \n",
    "\n",
    "cantons = cantonKeys.copy()\n",
    "\n",
    "\n",
    "random.shuffle(cantons) \n",
    "testPerm1 = cantons.copy()\n",
    "display(testPerm1)\n",
    "\n",
    "\n",
    "testPerm2 = cantons.copy()\n",
    "while (hasConflict(testPerm1,testPerm2)):\n",
    "    random.shuffle(cantons)\n",
    "    testPerm2 = cantons.copy()\n",
    "display(testPerm2)\n",
    "\n",
    "\n",
    "validPerm1 = cantons.copy()\n",
    "while (hasConflict(validPerm1,testPerm1) or hasConflict(validPerm1,testPerm2)):\n",
    "    random.shuffle(cantons)\n",
    "    validPerm1 = cantons.copy()\n",
    "display(validPerm1)\n",
    "\n",
    "validPerm2 = cantons.copy()\n",
    "while (hasConflict(validPerm2,testPerm1) or hasConflict(validPerm2,testPerm2) or hasConflict(validPerm2,validPerm1)):\n",
    "    random.shuffle(cantons)\n",
    "    validPerm2 = cantons.copy()\n",
    "display(validPerm2)\n",
    "\n",
    "\n",
    "#printFirstsInTuple(dfList)\n",
    "\n",
    "dfList = sortListBasedOnSpecificOrder(dfList, testPerm1)\n",
    "#printFirstsInTuple(dfList)\n",
    "\n",
    "\n",
    "length = dfList[0][1].shape[0]\n",
    "counter = 0\n",
    "cantonCounter = 0\n",
    "while (counter+intervalSize-1)  < length-28:\n",
    "    dfList[cantonCounter % 26][1].loc[counter:counter+intervalSize-1,['category']] = 'test 2'\n",
    "    counter = counter + intervalSize\n",
    "    cantonCounter = cantonCounter + 1\n",
    "dfList[cantonCounter % 26][1].loc[counter:length-29,['category']] = 'test 2'\n",
    "# remove first and add at the end\n",
    "temp = dfList.pop(0)\n",
    "dfList.append(temp)\n",
    "    \n",
    " \n",
    "dfList = sortListBasedOnSpecificOrder(dfList, testPerm2)\n",
    "\n",
    "counter = 0\n",
    "cantonCounter = 0\n",
    "while (counter+intervalSize-1)  < length-28:\n",
    "    dfList[cantonCounter % 26][1].loc[counter:counter+intervalSize-1,['category']] = 'test 2'\n",
    "    counter = counter + intervalSize\n",
    "    cantonCounter = cantonCounter + 1\n",
    "dfList[cantonCounter % 26][1].loc[counter:length-29,['category']] = 'test 2'\n",
    "# remove first and add at the end\n",
    "temp = dfList.pop(0)\n",
    "dfList.append(temp)\n",
    "\n",
    "\n",
    "dfList = sortListBasedOnSpecificOrder(dfList, validPerm1)\n",
    "\n",
    "# we go through the whole time intervall 2\n",
    "counter = 0 \n",
    "cantonCounter = 0\n",
    "while (counter+intervalSize-1)  < length-56:\n",
    "    dfList[cantonCounter % 26][1].loc[counter:counter+intervalSize-1,['category']] = 'validation 2'\n",
    "    counter = counter + intervalSize\n",
    "    cantonCounter = cantonCounter + 1  \n",
    "dfList[cantonCounter % 26][1].loc[counter:length-57,['category']] = 'validation 2'\n",
    "# remove first and add at the end\n",
    "temp = dfList.pop(0)\n",
    "dfList.append(temp)\n",
    "    \n",
    "dfList = sortListBasedOnSpecificOrder(dfList, validPerm2)   \n",
    "    \n",
    "counter = 0\n",
    "cantonCounter = 0\n",
    "while (counter+intervalSize-1)  < length-56:\n",
    "    dfList[cantonCounter % 26][1].loc[counter:counter+intervalSize-1,['category']] = 'validation 2'\n",
    "    counter = counter + intervalSize\n",
    "    cantonCounter = cantonCounter + 1  \n",
    "dfList[cantonCounter % 26][1].loc[counter:length-57,['category']] = 'validation 2'\n",
    "# remove first and add at the end\n",
    "temp = dfList.pop(0)\n",
    "dfList.append(temp)\n",
    "\n",
    "dfList = [y for (x,y) in dfList]\n",
    "\n",
    "data = pd.concat(dfList, axis=0, ignore_index=True)\n",
    "completeNumber = data.shape[0]\n",
    "trainNumber = data[data['category']=='train'].shape[0]\n",
    "test1Number = data[data['category']=='test 1'].shape[0]\n",
    "test2Number = data[data['category']=='test 2'].shape[0]\n",
    "valid1Number = data[data['category']=='validation 1'].shape[0]\n",
    "valid2Number = data[data['category']=='validation 2'].shape[0]\n",
    "display(\"Complete: \"+ str(completeNumber) )\n",
    "display(\"Training set: \"+ str(trainNumber)+ \"(\"+str(trainNumber/completeNumber)+\")\")\n",
    "display(\"Test set 1: \"+ str(test1Number)+ \"(\"+str(test1Number/completeNumber)+\")\")\n",
    "display(\"Test set 2: \"+ str(test2Number)+ \"(\"+str(test2Number/completeNumber)+\")\")\n",
    "display(\"Validation set 1: \"+ str(valid1Number)+ \"(\"+str(valid1Number/completeNumber)+\")\")\n",
    "display(\"Validation set 2: \"+ str(valid2Number)+ \"(\"+str(valid2Number/completeNumber)+\")\")\n",
    "\n",
    "data.to_csv('completedata.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
