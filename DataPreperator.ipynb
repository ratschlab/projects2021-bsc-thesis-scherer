{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 916,
   "id": "animal-abortion",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "pd.options.display.max_rows = 10\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "cantonKeys = ['AG','AI','AR', 'BE', 'BL', 'BS', 'FR', 'GE', 'GL', 'GR', 'JU', 'LU', 'NE', 'NW', 'OW', 'SG', 'SH', 'SO', 'SZ', 'TG', 'TI', 'UR', 'VD', 'VS', 'ZG','ZH']\n",
    "\n",
    "dict = {}\n",
    "\n",
    "# we discard the first days of 2020 and future data\n",
    "for cantonId in cantonKeys: #use '2021-04-05' for testing and yesterday for production\n",
    "    dict[cantonId] = pd.read_csv(\"data/merged/\"+cantonId+\".csv\").set_index('date')['2020-02-15':'2021-04-05']\n",
    "    dict[cantonId].index = pd.to_datetime(dict[cantonId].index)\n",
    "    \n",
    "interpolMet = 'linear'\n",
    "originalDict = dict.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 917,
   "id": "laden-resistance",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "yesterday = str(datetime.date.today()- timedelta(days = 1))\n",
    "for cantonId in cantonKeys:\n",
    "  \n",
    "    # interpolate weekly FOPH dataCases entries 0 - 9\n",
    "    for c1 in ['Cases','Death','Hosp']:\n",
    "        for c2 in ['entries','inz_entries','inzsumTotal']:\n",
    "            for c3 in ['0 - 9','10 - 19','20 - 29','30 - 39','40 - 49','50 - 59','60 - 69','70 - 79','80+']:\n",
    "                # fill first row with 0\n",
    "                dict[cantonId].loc['2020-02-15',c1 + \" \" + c2 + \" \" + c3] = 0\n",
    "                # interpolate the rest\n",
    "                dict[cantonId][[c1 + \" \" + c2 + \" \" + c3]] = dict[cantonId][[c1 + \" \" + c2 + \" \" + c3]].interpolate(method=interpolMet)\n",
    "            for c4 in ['male','female']:\n",
    "                # fill first row with 0\n",
    "                dict[cantonId].loc['2020-02-15',c1 + \" \" + c2 + \" \" + c4] = 0\n",
    "                # interpolate the rest\n",
    "                dict[cantonId][[c1 + \" \" + c2 + \" \" + c4]] = dict[cantonId][[c1 + \" \" + c2 + \" \" + c4]].interpolate(method=interpolMet)\n",
    "    \n",
    "    \n",
    "    # fill missing vaccine data\n",
    "    vaccine = ['VaccDosesAdministered sumTotal','VaccDosesAdministered per100PersonsTotal', \n",
    "               'FullyVaccPersons sumTotal', 'FullyVaccPersons per100PersonsTotal']\n",
    "    dict[cantonId].loc['2020-02-15',vaccine] = 0\n",
    "    dict[cantonId][vaccine] = dict[cantonId][vaccine].fillna(method='ffill')\n",
    "    \n",
    "    # fill missing total hospital capacities\n",
    "    hospitalCols = ['ICU_Capacity','ICU_FreeCapacity','Total_Capacity','Total_FreeCapacity']\n",
    "    dict[cantonId][hospitalCols] = dict[cantonId][hospitalCols].interpolate(method=interpolMet)\n",
    "    dict[cantonId]['ICU_Capacity'].fillna(method='bfill', inplace=True)\n",
    "    dict[cantonId]['ICU_FreeCapacity'].fillna(method='bfill', inplace=True)\n",
    "    dict[cantonId]['Total_Capacity'].fillna(method='bfill', inplace=True)\n",
    "    dict[cantonId]['Total_FreeCapacity'].fillna(method='bfill', inplace=True)\n",
    "\n",
    "    # ICU_Covid19Patients + ICU_NonCovid19Patients = ICU_AllPatients \n",
    "    dict[cantonId].loc['2020-02-15','ICU_Covid19Patients'] = 0\n",
    "    ICUpatientsCols = ['ICU_Covid19Patients','ICU_NonCovid19Patients','ICU_AllPatients']\n",
    "    dict[cantonId][ICUpatientsCols] = dict[cantonId][ICUpatientsCols].interpolate(method=interpolMet)\n",
    "    dict[cantonId]['ICU_NonCovid19Patients'].fillna(method='bfill', inplace=True)\n",
    "    dict[cantonId]['ICU_AllPatients'].fillna(dict[cantonId][['ICU_Covid19Patients','ICU_NonCovid19Patients']].sum(axis=1), inplace=True)\n",
    "\n",
    "    # Total_Covid19Patients + Total_NonCovid19Patients = Total_AllPatients\n",
    "    dict[cantonId].loc['2020-02-15','Total_Covid19Patients'] = 0\n",
    "    patientCols = ['Total_Covid19Patients','Total_NonCovid19Patients','Total_AllPatients']\n",
    "    dict[cantonId][patientCols] = dict[cantonId][patientCols].interpolate(method=interpolMet)\n",
    "    dict[cantonId]['Total_NonCovid19Patients'].fillna(method='bfill', inplace=True)\n",
    "    dict[cantonId]['Total_AllPatients'].fillna(dict[cantonId][['Total_Covid19Patients','Total_NonCovid19Patients']].sum(axis=1), inplace=True)\n",
    "    \n",
    "    # fill in missing Google mobility data\n",
    "    googleMobilityCols = ['retail_and_recreation_percent_change_from_baseline','grocery_and_pharmacy_percent_change_from_baseline',\n",
    "    'parks_percent_change_from_baseline','transit_stations_percent_change_from_baseline','workplaces_percent_change_from_baseline'\n",
    "    ,'residential_percent_change_from_baseline']\n",
    "    googleMobCH = pd.read_csv(\"data/GoogleMobility/2020_CH_Region_Mobility_Report.csv\")\n",
    "    googleMobCH = googleMobCH.loc[googleMobCH[\"sub_region_1\"].isna()].set_index('date')[googleMobilityCols]\n",
    "    googleMobCH.index = pd.to_datetime(googleMobCH.index)\n",
    "    for col in googleMobilityCols:\n",
    "        dict[cantonId][col].fillna(googleMobCH[col], inplace=True)\n",
    "    dict[cantonId][googleMobilityCols] = dict[cantonId][googleMobilityCols].interpolate(method=interpolMet)\n",
    "    dict[cantonId][googleMobilityCols] = dict[cantonId][googleMobilityCols].fillna(method='ffill')\n",
    "    \n",
    "    # fill in missing Intervista mobility data \n",
    "    dict[cantonId][['intervistaMob']] = dict[cantonId][['intervistaMob']].interpolate(method=interpolMet)\n",
    "    dict[cantonId]['intervistaMob'].fillna(method='ffill', inplace=True)\n",
    "    \n",
    "    # fill in missing neighbor incidence   \n",
    "    dict[cantonId].loc['2020-02-15', 'meanNeighborIncidence'] = 0\n",
    "    dict[cantonId].loc['2020-02-15', 'maxNeighborIncidence'] = 0\n",
    "    dict[cantonId][['meanNeighborIncidence','maxNeighborIncidence']] = dict[cantonId][['meanNeighborIncidence','maxNeighborIncidence']].interpolate(method=interpolMet)\n",
    "        \n",
    "    # fill in missing r values\n",
    "    rvalues = ['median_R_mean','median_R_highHPD','median_R_lowHPD']\n",
    "    dict[cantonId][rvalues] = dict[cantonId][rvalues].interpolate(method=interpolMet)  \n",
    "    dict[cantonId][rvalues] = dict[cantonId][rvalues].fillna(method='ffill')\n",
    "    dict[cantonId][rvalues] = dict[cantonId][rvalues].fillna(method='bfill')\n",
    "  \n",
    "    # fill in missing variants\n",
    "    variants = ['lower_ci_day','upper_ci_day','anteil_pos']\n",
    "    # first detected case for variants of concerne in Switzerland is 2020-10-14\n",
    "    dict[cantonId].loc['2020-10-13',['lower_ci_day','anteil_pos']] = 0\n",
    "    dict[cantonId].loc['2020-10-13',['upper_ci_day']] = 100\n",
    "    dict[cantonId][variants] = dict[cantonId][variants].interpolate(method=interpolMet)\n",
    "    dict[cantonId][variants] = dict[cantonId][variants].fillna(method='ffill')\n",
    "    dict[cantonId][variants] = dict[cantonId][variants].fillna(method='bfill')\n",
    "\n",
    "    # fill in daily incoming missing data\n",
    "    # entries\n",
    "    dict[cantonId].loc['2020-02-15',['case_entries','hosp_entries','death_entries','case_inz_entries','hosp_inz_entries','death_inz_entries',\n",
    "              'case_inzsumTotal','hosp_inzsumTotal','death_inzsumTotal']] = 0\n",
    "    dict[cantonId][entries] = dict[cantonId][entries].interpolate(method=interpolMet)\n",
    "\n",
    "    dict[cantonId].loc['2020-02-15','test_inzsumTotal'] = 0\n",
    "    dict[cantonId][['test_inzsumTotal']] = dict[cantonId][['test_inzsumTotal']].interpolate(method=interpolMet)\n",
    "    \n",
    "    if not os.path.exists('data/filled'):\n",
    "        os.makedirs('data/filled')\n",
    "    dict[cantonId].to_csv('data/filled/'+cantonId+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 918,
   "id": "enclosed-colombia",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for cantonId in cantonKeys:\n",
    "    for col in dict[cantonId].columns:\n",
    "        if dict[cantonId][col].isna().sum() != 0:\n",
    "            print(cantonId+\" \"+col+\" (#NaN/total): (\" + str(dict[cantonId][col].isna().sum())+\"/\"+str(dict[cantonId][col].notna().sum())+\")\")\n",
    "            #dict[cantonId][col].plot(kind='line',y=[col], figsize=(20,10))\n",
    "            #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 919,
   "id": "commercial-transsexual",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ncol = 'Cases entries male'\\ndisplay(np.array(originalDict['AG'][col]))\\noriginalDict['AG'].plot(kind='scatter', x='date', y=[col], figsize=(20,10))\\ndict['AG'].plot(kind='line',y=[col], figsize=(20,10))\\nplt.show()\\n\""
      ]
     },
     "execution_count": 919,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "col = 'Cases entries male'\n",
    "display(np.array(originalDict['AG'][col]))\n",
    "originalDict['AG'].plot(kind='scatter', x='date', y=[col], figsize=(20,10))\n",
    "dict['AG'].plot(kind='line',y=[col], figsize=(20,10))\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radical-eight",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 920,
   "id": "floral-genome",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# plotting original data\n",
    "for col in dict['AG'].columns:\n",
    "    comparingDf = pd.concat([dict['AG'][[col]],originalData[[\"original_\"+col]]], axis=1)\n",
    "    #comparingDf[['original_'+col]].reset_index().plot(kind='scatter', x=['date'], y=['original_'+col], figsize=(20,10))\n",
    "    #comparingDf['2020-02-15':'2021-04-05'].plot(kind='line',y=[col], figsize=(20,10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 921,
   "id": "consolidated-colon",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#display([col for col in pd.read_csv(\"data/filled/AG.csv\").columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 922,
   "id": "proof-adobe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#display([col for col in features.columns])\n",
    "#display(len([col for col in features.columns]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 923,
   "id": "seasonal-bradford",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature engineering\n",
    "filledDict = {}\n",
    "\n",
    "# window size\n",
    "w = 7\n",
    "\n",
    "for cantonId in cantonKeys:\n",
    "    d = pd.read_csv(\"data/filled/\"+cantonId+\".csv\")\n",
    "    d = d.set_index('date')\n",
    "    filledDict[cantonId] = d\n",
    "    \n",
    "    dailyFeatures = filledDict[cantonId].copy()\n",
    "\n",
    "    # summarize mask mandatories\n",
    "    maskMandatories = [ 'Mask mandatory in publicly accessible establishments/ spaces (shops etc.)',\n",
    "                       'Mask mandatory in public transport','Masks mandatory in schools','Masks mandatory at work']\n",
    "    dailyFeatures[['maskMandatories']] = dailyFeatures[maskMandatories].sum(axis=1)\n",
    "    dailyFeatures.drop(maskMandatories, axis=1, inplace=True)\n",
    "\n",
    "    dailyFeatures[['googleMobility']] = dailyFeatures[['retail_and_recreation_percent_change_from_baseline',\n",
    "                                                       'grocery_and_pharmacy_percent_change_from_baseline',\n",
    "                                                       'parks_percent_change_from_baseline',\n",
    "                                                       'transit_stations_percent_change_from_baseline',\n",
    "                                                       'workplaces_percent_change_from_baseline',\n",
    "                                                       'residential_percent_change_from_baseline']].mean(axis=1)\n",
    "\n",
    "    # r value accuracy\n",
    "    dailyFeatures[['R_error']] = dailyFeatures['median_R_highHPD']-dailyFeatures['median_R_lowHPD']\n",
    "    dailyFeatures.drop(['median_R_highHPD','median_R_lowHPD'],axis=1, inplace=True)\n",
    "\n",
    "    # variants accuracy\n",
    "    #features[['anteil_pos','upper_ci_day','lower_ci_day']] = features[['anteil_pos','upper_ci_day','lower_ci_day']].rolling(window=w).mean()\n",
    "    dailyFeatures[['variant_error']] = dailyFeatures['upper_ci_day']-dailyFeatures['lower_ci_day']\n",
    "    dailyFeatures.drop(['upper_ci_day','lower_ci_day'],axis=1, inplace=True)\n",
    "\n",
    "    # vaccine\n",
    "    dailyFeatures.drop(['VaccDosesAdministered sumTotal','FullyVaccPersons sumTotal'],axis=1, inplace=True)\n",
    "    vaccine = ['VaccDosesAdministered per100PersonsTotal',\n",
    "               'FullyVaccPersons per100PersonsTotal']\n",
    "    #features[vaccine] = features[vaccine].rolling(window=w).mean()\n",
    "    \n",
    "    # test positivity rate\n",
    "    #display(len(features[['case_entries']]))\n",
    "    #display(len(features[['test_entries']]))\n",
    "    dailyFeatures[['testPositvity']] = dailyFeatures['case_entries']/dailyFeatures['test_entries']\n",
    "\n",
    "    # remove absolut values which are included in the incidenc rates\n",
    "    absVal = ['Cases entries 0 - 9','Cases entries 10 - 19','Cases entries 20 - 29','Cases entries 30 - 39',\n",
    "              'Cases entries 40 - 49','Cases entries 50 - 59','Cases entries 60 - 69','Cases entries 70 - 79',\n",
    "              'Cases entries 80+','Death entries 0 - 9','Death entries 10 - 19','Death entries 20 - 29',\n",
    "              'Death entries 30 - 39','Death entries 40 - 49','Death entries 50 - 59','Death entries 60 - 69',\n",
    "              'Death entries 70 - 79','Death entries 80+', 'Hosp entries 0 - 9','Hosp entries 10 - 19',\n",
    "              'Hosp entries 20 - 29','Hosp entries 30 - 39','Hosp entries 40 - 49','Hosp entries 50 - 59',\n",
    "              'Hosp entries 60 - 69','Hosp entries 70 - 79','Hosp entries 80+', 'Cases entries female',\n",
    "              'Cases entries male','Death entries female','Death entries male', 'Hosp entries female',\n",
    "              'Hosp entries male','case_entries','hosp_entries','death_entries','test_entries']\n",
    "    dailyFeatures.drop(absVal,axis=1, inplace=True)\n",
    "\n",
    "    # hospital capacities\n",
    "    hospCap = [ 'ICU_AllPatients',\n",
    "     'ICU_Covid19Patients',\n",
    "     'ICU_Capacity',\n",
    "     'Total_AllPatients',\n",
    "     'Total_Covid19Patients',\n",
    "     'Total_Capacity',\n",
    "     'ICU_NonCovid19Patients',\n",
    "     'ICU_FreeCapacity',\n",
    "     'Total_NonCovid19Patients',\n",
    "     'Total_FreeCapacity']\n",
    "    staticCantonal = pd.read_excel(\"static_data/staticCantonalData.xlsx\").set_index('canton').transpose()\n",
    "    dailyFeatures[[col + \"_inz\" for col in hospCap]] = 100000*(dailyFeatures[hospCap]/staticCantonal.loc[[cantonId]]['residents'][0])\n",
    "    dailyFeatures.drop(hospCap,axis=1, inplace=True)\n",
    "\n",
    "    if not os.path.exists('data/dailyFeatures'):\n",
    "        os.makedirs('data/dailyFeatures')\n",
    "    dailyFeatures.to_csv('data/dailyFeatures/'+cantonId+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 924,
   "id": "atomic-mentor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Cases inz_entries 0 - 9', 'Cases inz_entries 10 - 19',\n",
       "       'Cases inz_entries 20 - 29', 'Cases inz_entries 30 - 39',\n",
       "       'Cases inz_entries 40 - 49', 'Cases inz_entries 50 - 59',\n",
       "       'Cases inz_entries 60 - 69', 'Cases inz_entries 70 - 79',\n",
       "       'Cases inz_entries 80+', 'Cases inzsumTotal 0 - 9',\n",
       "       ...\n",
       "       'ICU_AllPatients_inz', 'ICU_Covid19Patients_inz', 'ICU_Capacity_inz',\n",
       "       'Total_AllPatients_inz', 'Total_Covid19Patients_inz',\n",
       "       'Total_Capacity_inz', 'ICU_NonCovid19Patients_inz',\n",
       "       'ICU_FreeCapacity_inz', 'Total_NonCovid19Patients_inz',\n",
       "       'Total_FreeCapacity_inz'],\n",
       "      dtype='object', length=126)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "display(dailyFeatures.reset_index().drop(['date'], axis=1).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "psychological-tongue",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo: add future weather as a feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infectious-nursery",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "future-share",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
