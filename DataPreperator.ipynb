{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "jewish-namibia",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "from datetime import date, timedelta, timezone\n",
    "from sklearn.impute import KNNImputer\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.options.display.max_rows = 10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cantonKeys = ['AG','AI','AR', 'BE', 'BL', 'BS', 'FR', 'GE', 'GL', 'GR', 'JU', 'LU', 'NE', 'NW', 'OW', 'SG', 'SH', 'SO', 'SZ', 'TG', 'TI', 'UR', 'VD', 'VS', 'ZG','ZH']\n",
    "googleMobDict = dict(zip(cantonKeys,[\"Aargau\",\"Appenzell Innerrhoden\",\"Appenzell Ausserrhoden\",\"Canton of Bern\",\"Basel-Landschaft\",\"Basel City\",\n",
    "                                                    \"Fribourg\",\"Geneva\",\"Glarus\",\"Grisons\",\"Jura\",\"Lucerne\",\"Neuch√¢tel\",\"Nidwalden\",\"Obwalden\",\"St. Gallen\",\n",
    "                                                    \"Schaffhausen\",\"Solothurn\",\"Schwyz\",\"Thurgau\",\"Ticino\",\"Uri\",\"Vaud\",\"Valais\",\"Canton of Zug\",\"Zurich\"]))\n",
    "def getDays(year, offset):\n",
    "   d = date(year, 1, 1)                    \n",
    "   d += timedelta(days = offset - d.weekday())  \n",
    "   while d.year == year:\n",
    "      yield d\n",
    "      d += timedelta(days = 7)\n",
    "\n",
    "listOfMondays = []\n",
    "for year in [2020,2021]:\n",
    "    for day in getDays(year, 7):\n",
    "       listOfMondays.append(day)\n",
    "    \n",
    "def addZero(x):\n",
    "    if len(x)==1:\n",
    "        return \"0\"+x\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "yearWeek = [str(x.isocalendar()[0])+addZero(str(x.isocalendar()[1])) for x in listOfMondays]\n",
    "\n",
    "mondaysByWeekNr = dict(zip(yearWeek,listOfMondays))\n",
    "\n",
    "# we discard the first days of 2020 and future data\n",
    "yesterday = datetime.date.today()-timedelta(days = 1)\n",
    "yesterdayStr = str(yesterday)\n",
    "start = '2020-02-15'\n",
    "end = '2021-04-20' #use '2021-04-05' for testing and yesterdayStr for production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlimited-accessory",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# MERGE THE COLLECTED DATA TO ONE FILE FOR EACH CANTON\n",
    "\n",
    "for cantonId in cantonKeys:\n",
    "    df = pd.DataFrame(index=pd.date_range(start=datetime.datetime(2020, 1, 1), end=datetime.datetime(2021, 12, 31)))\n",
    "    \n",
    "    # weekly age classified FOPH data\n",
    "    for category in ['Cases','Death','Hosp']: \n",
    "        age = pd.read_csv(\"data/FOPH/data/COVID19\"+category+\"_geoRegion_AKL10_w.csv\")\n",
    "        age[['datum']] = age[['datum']].applymap(lambda x: mondaysByWeekNr[str(x)])\n",
    "        age = age.loc[age['geoRegion']==cantonId]\n",
    "        age = age[['datum','altersklasse_covid19','inz_entries','inzsumTotal']]\n",
    "        age = age.pivot(index=\"datum\", columns=\"altersklasse_covid19\")\n",
    "        age.columns = [category+\" \"+' '.join(col) for col in age.columns.values]\n",
    "        age = age.drop([category+' inz_entries Unbekannt', category+' inzsumTotal Unbekannt'], axis=1)\n",
    "        df = df.join(age)\n",
    "    \n",
    "    # weekly gender classified FOPH data\n",
    "    for category in ['Cases','Death','Hosp']: \n",
    "        gender = pd.read_csv(\"data/FOPH/data/COVID19\"+category+\"_geoRegion_sex_w.csv\")\n",
    "        gender[['datum']] = gender[['datum']].applymap(lambda x: mondaysByWeekNr[str(x)])\n",
    "        gender = gender.loc[gender['geoRegion']==cantonId][['datum','sex','inz_entries','inzsumTotal']]\n",
    "        gender = gender.pivot(index=\"datum\", columns=\"sex\")\n",
    "        gender.columns = [category+\" \"+' '.join(col) for col in gender.columns.values]\n",
    "        gender = gender.drop([category+' inz_entries unknown', category+' inzsumTotal unknown'],axis=1)\n",
    "        df = df.join(gender)\n",
    "    \n",
    "    # weekly vaccination age classified FOPH data \n",
    "    for category in ['VaccDosesAdministered','FullyVaccPersons']: \n",
    "        vacc = pd.read_csv(\"data/FOPH/data/COVID19\"+category+\".csv\")\n",
    "        vacc = vacc.loc[vacc['geoRegion']==cantonId]\n",
    "        vacc = vacc[['date','sumTotal','per100PersonsTotal']]\n",
    "        vacc = vacc.set_index('date')\n",
    "        vacc.columns = [category+\" \"+col for col in vacc.columns]\n",
    "        vacc.index = pd.to_datetime(vacc.index) \n",
    "        df = df.join(vacc)\n",
    "    \n",
    "    # daily hospital capacity FOPH data\n",
    "    capacity = pd.read_csv(\"data/FOPH/data/COVID19HospCapacity_geoRegion.csv\")\n",
    "    capacity = capacity.loc[capacity['geoRegion']==cantonId]\n",
    "    capacity = capacity.set_index('date').sort_index()\n",
    "    capacity = capacity[['ICU_AllPatients','ICU_Covid19Patients','ICU_Capacity','Total_AllPatients',\n",
    "                         'Total_Covid19Patients','Total_Capacity','ICU_NonCovid19Patients','ICU_FreeCapacity',\n",
    "                         'Total_NonCovid19Patients','Total_FreeCapacity','type_variant']]\n",
    "    capacity = capacity.drop_duplicates()\n",
    "    capacity = capacity.loc[capacity['type_variant']=='fp7d']\n",
    "    capacity = capacity.drop(['type_variant'], axis=1)\n",
    "    capacity.index = pd.to_datetime(capacity.index) \n",
    "    df = df.join(capacity)\n",
    "    \n",
    "    \n",
    "    # percentage of virus variants\n",
    "    variants = pd.read_csv(\"data/FOPH/data/COVID19Variants.csv\", low_memory=False)\n",
    "    variants = variants.loc[(variants['geoRegion']==cantonId) & (variants['variant_type']=='voc_digitally_reported') & (variants['data_quality']=='sufficient') ] #(variants['variant_type']=='voc_known')\n",
    "    variants = variants.set_index('date').sort_index()\n",
    "    variants = variants[['lower_ci_day','upper_ci_day','anteil_pos']]\n",
    "    variants.index = pd.to_datetime(variants.index) \n",
    "    df = df.join(variants)\n",
    "    \n",
    "    \n",
    "    # daily basis data\n",
    "    # attach daily positive cases\n",
    "    caseDf = pd.read_csv(\"data/FOPH/data/COVID19Cases_geoRegion.csv\")\n",
    "    caseDf = caseDf.loc[caseDf[\"geoRegion\"]==cantonId]\n",
    "    caseDf = caseDf.set_index('datum')\n",
    "    interestedCols = ['inz_entries','inzsumTotal']\n",
    "    caseDf = caseDf[interestedCols]\n",
    "    caseDf.columns = [\"case_\"+e for e in interestedCols]\n",
    "    caseDf.index = pd.to_datetime(caseDf.index) \n",
    "    df = df.join(caseDf)\n",
    "    \n",
    "    # attach daily hospital cases\n",
    "    hospDf = pd.read_csv(\"data/FOPH/data/COVID19Hosp_geoRegion.csv\")\n",
    "    hospDf = hospDf.loc[hospDf[\"geoRegion\"]==cantonId]\n",
    "    hospDf = hospDf.set_index('datum')\n",
    "    interestedCols = ['inz_entries','inzsumTotal']\n",
    "    hospDf = hospDf[interestedCols]\n",
    "    hospDf.columns = [\"hosp_\"+e for e in interestedCols]\n",
    "    hospDf.index = pd.to_datetime(hospDf.index) \n",
    "    df = df.join(hospDf)\n",
    "    \n",
    "    # attach daily death cases\n",
    "    deathDf = pd.read_csv(\"data/FOPH/data/COVID19Death_geoRegion.csv\")\n",
    "    deathDf = deathDf.loc[deathDf[\"geoRegion\"]==cantonId]\n",
    "    deathDf = deathDf.set_index('datum')\n",
    "    interestedCols = ['inz_entries','inzsumTotal']\n",
    "    deathDf = deathDf[interestedCols]\n",
    "    deathDf.columns = [\"death_\"+e for e in interestedCols]\n",
    "    deathDf.index = pd.to_datetime(deathDf.index)\n",
    "    df = df.join(deathDf)\n",
    "    \n",
    "    \n",
    "    # attach daily test\n",
    "    testDf = pd.read_csv(\"data/FOPH/data/COVID19Test_geoRegion_all.csv\")\n",
    "    testDf = testDf.loc[testDf[\"geoRegion\"]==cantonId]\n",
    "    testDf = testDf.set_index('datum')\n",
    "    pop = testDf['pop'][0]\n",
    "    interestedCols = ['entries','inz_entries','inzsumTotal']\n",
    "    testDf = testDf[interestedCols]\n",
    "    testDf.columns = [\"test_\"+e for e in interestedCols]\n",
    "    testDf.index = pd.to_datetime(testDf.index)\n",
    "    df = df.join(testDf)\n",
    "    # compute rest of test entries\n",
    "    totalTestsInSwitzerland = pd.DataFrame(index=pd.date_range(start=datetime.datetime(2020, 2, 15), end=datetime.datetime(2020, 5, 22)))\n",
    "    temp = pd.read_csv(\"static_data/historicTests.csv\")\n",
    "    temp = temp.set_index('Datum')   \n",
    "    temp.index = pd.to_datetime(temp.index)\n",
    "    totalTestsInSwitzerland = totalTestsInSwitzerland.join(temp)  \n",
    "    totalTestsInSwitzerland = totalTestsInSwitzerland[['Tests pro Tag']]\n",
    "    totalTestsInSwitzerland.fillna(method='bfill', inplace=True)\n",
    "    testsByCanton = pd.read_csv(\"data/FOPH/data/COVID19Test_geoRegion_all.csv\")\n",
    "    testsByCanton = testsByCanton.set_index('datum')\n",
    "    testsByCanton.index = pd.to_datetime(testsByCanton.index)\n",
    "    sumSwitzerland = testsByCanton.loc[testsByCanton[\"geoRegion\"]=='CH'][['entries']]\n",
    "    sumCanton = testsByCanton.loc[testsByCanton[\"geoRegion\"]==cantonId][['entries']]\n",
    "    cantonalTestFraction = sumCanton['2020-05-23':'2020-06-05'].sum(axis=0).values[0]/sumSwitzerland['2020-05-23':'2020-06-05'].sum(axis=0).values[0]\n",
    "    #multiply this with cantonal test quotient\n",
    "    computedMissingEntries = totalTestsInSwitzerland['2020-02-15':'2020-05-22']*cantonalTestFraction \n",
    "    computedMissingEntries.rename(columns = {\"Tests pro Tag\":'test_entries'}, inplace = True)\n",
    "    df[['test_entries']] = df[['test_entries']].fillna(computedMissingEntries[['test_entries']])\n",
    "    # compute rest of test incidence\n",
    "    missingTestIncidents = 100000*(df[['test_entries']]/pop)\n",
    "    missingTestIncidents.rename(columns = {\"test_entries\":'test_inz_entries'}, inplace = True)\n",
    "    df[['test_inz_entries']] = df[['test_inz_entries']].fillna(missingTestIncidents)   \n",
    "    \n",
    "    # attach daily R-values\n",
    "    rvalueDf = pd.read_csv(\"data/FOPH/data/COVID19Re_geoRegion.csv\")\n",
    "    rvalueDf = rvalueDf.loc[rvalueDf[\"geoRegion\"]==cantonId]\n",
    "    rvalueDf = rvalueDf.set_index('date')\n",
    "    interestedCols = ['median_R_mean','median_R_highHPD','median_R_lowHPD']\n",
    "    rvalueDf = rvalueDf[interestedCols]\n",
    "    rvalueDf.index = pd.to_datetime(rvalueDf.index)\n",
    "    df = df.join(rvalueDf)\n",
    "    \n",
    "    # attach google mobility data\n",
    "    mobDf2020 = pd.read_csv(\"data/GoogleMobility/2020_CH_Region_Mobility_Report.csv\")\n",
    "    mobDf2021 = pd.read_csv(\"data/GoogleMobility/2021_CH_Region_Mobility_Report.csv\")\n",
    "    mobDf2020 = mobDf2020.loc[mobDf2020[\"sub_region_1\"]==googleMobDict[cantonId]].set_index('date')\n",
    "    mobDf2021 = mobDf2021.loc[mobDf2021[\"sub_region_1\"]==googleMobDict[cantonId]].set_index('date')\n",
    "    interestedCols = ['retail_and_recreation_percent_change_from_baseline',\n",
    "                  'grocery_and_pharmacy_percent_change_from_baseline',\n",
    "                  'parks_percent_change_from_baseline',\n",
    "                  'transit_stations_percent_change_from_baseline',\n",
    "                  'workplaces_percent_change_from_baseline',\n",
    "                  'residential_percent_change_from_baseline'\n",
    "                 ]\n",
    "    mobDf2020 = mobDf2020[interestedCols]\n",
    "    mobDf2021 = mobDf2021[interestedCols]\n",
    "    mobDf2020.index = pd.to_datetime(mobDf2020.index)\n",
    "    mobDf2021.index = pd.to_datetime(mobDf2021.index)\n",
    "    mobDf = mobDf2020.append(mobDf2021)\n",
    "    df = df.join(mobDf)\n",
    "    \n",
    "    # attach KOF strigency index\n",
    "    kofDf = pd.read_csv(\"data/KOF/KOFStrigencyIndex.csv\")\n",
    "    kofDf = kofDf.set_index('date')\n",
    "    kofDf = kofDf[[\"ch.kof.stringency.\"+cantonId.lower()+\".stringency_plus\"]]\n",
    "    kofDf.rename(columns = {\"ch.kof.stringency.\"+cantonId.lower()+\".stringency_plus\":'kofStrigency'}, inplace = True)\n",
    "    kofDf.index = pd.to_datetime(kofDf.index)\n",
    "    df = df.join(kofDf)\n",
    "    \n",
    "    # attach all measures\n",
    "    measuresDf = pd.read_csv(\"data/measures/\"+cantonId+\".csv\")\n",
    "    measuresDf = measuresDf.set_index('Time')\n",
    "    measuresDf.index = pd.to_datetime(measuresDf.index)\n",
    "    df = df.join(measuresDf)\n",
    "    \n",
    "    \n",
    "    # attach holidays & vacations \n",
    "    holy = pd.read_csv(\"data/HolidayVacation/HolidayVacation.csv\").set_index(\"date\")[[cantonId]]\n",
    "    holy.rename(columns = {cantonId:'isHoliday'}, inplace = True)\n",
    "    holy.index = pd.to_datetime(holy.index)\n",
    "    df = df.join(holy)\n",
    "   \n",
    "    \n",
    "    # attach intervista mobility data\n",
    "    averageAndMedian = pd.read_csv(\"data/IntervistaMobility/Mittelwerte_und_Median_pro_Tag.csv\", encoding=\"mac_roman\")\n",
    "    averageAndMedian = averageAndMedian.loc[(averageAndMedian[\"Beschreibung\"] == \"Distanz\") & (averageAndMedian[\"Typ\"] == \"Median\")]\n",
    "    averageAndMedian = averageAndMedian.set_index(\"Datum\")\n",
    "    averageAndMedian = averageAndMedian[['Alter_15-29', 'Alter_30-64', 'Alter_65-79','M‚Ä∞nnlich', 'Weiblich', \n",
    "                                         'St‚Ä∞dtisch', 'L‚Ä∞ndlich', 'Erwerbst‚Ä∞tig','In_Ausbildung', 'Nicht_Erwerbst‚Ä∞tig',\n",
    "                                         'Auto_Ja','Auto_Nein', 'HaushaltsgrÀÜsse_1_Person', 'HaushaltsgrÀÜsse_2_Personen',\n",
    "                                         'HaushaltsgrÀÜsse_3+_Personen', 'Kinder_Ja', 'Kinder_Nein', 'D-CH','F-CH', 'I-CH']]\n",
    "    averageAndMedian.index = pd.to_datetime(averageAndMedian.index)\n",
    "    D_CH = ['AG','AI','AR', 'BE','BL', 'BS','LU','GR','NW', 'OW', 'SG', 'SH', 'SO', 'SZ', 'TG','GL','UR','ZG','ZH']\n",
    "    F_CH = ['FR', 'GE', 'JU', 'VD', 'VS', 'NE']\n",
    "    #'TI'\n",
    "    if cantonId in D_CH:\n",
    "        df = df.join(averageAndMedian[['D-CH']])\n",
    "        df.rename(columns = {'D-CH':'intervistaMob'}, inplace = True) \n",
    "    elif cantonId in F_CH:\n",
    "        df = df.join(averageAndMedian[['F-CH']])\n",
    "        df.rename(columns = {'F-CH':'intervistaMob'}, inplace = True) \n",
    "    else:\n",
    "        # cantonId = TI\n",
    "        df = df.join(averageAndMedian[['I-CH']])\n",
    "        df.rename(columns = {'I-CH':'intervistaMob'}, inplace = True)\n",
    "        \n",
    "   \n",
    "    # attach neighbor incidents (WEEKLY)\n",
    "    neigbors = {\n",
    "      'AG': ['BL','SO','BE','LU','ZH','Baden-Wurttemberg','ZG'],\n",
    "      'AI': ['AR','SG'],\n",
    "      'AR': ['AI','SG'],\n",
    "      'BE': ['AG','SO','JU','NE','FR','VD','VS','UR','NW','OW','LU'], \n",
    "      'BL': ['AG','BS','SO','Baden-Wurttemberg','Grand Est'], \n",
    "      'BS': ['BL','Baden-Wurttemberg','Grand Est'], \n",
    "      'FR': ['BE','VD','NE'], \n",
    "      'GE': ['VD','Auvergne Rhone Alpes'], \n",
    "      'GL': ['SG','SZ','UR','GR'], \n",
    "      'GR': ['SG','GL','UR','TI','Vorarlberg','Lombardia','Liechtenstein'], \n",
    "      'JU': ['BL','SO','BE','NE','Grand Est','Bourgogne Franche Comte'], \n",
    "      'LU': ['AG','BE','NW', 'OW','ZG','SZ'], \n",
    "      'NE': ['JU','BE','VD','FR','Bourgogne Franche Comte'], \n",
    "      'NW': ['OW','BE','LU','SZ','UR'], \n",
    "      'OW': ['NW','LU','BE','UR'], \n",
    "      'SG': ['AI','AR','TG','ZH','SZ','GL','GR','Vorarlberg','Liechtenstein'], \n",
    "      'SH': ['TG','ZH','Baden-Wurttemberg'], \n",
    "      'SO': ['BE','JU','BL','AG','Grand Est'],\n",
    "      'SZ': ['ZG','ZH','SG','GL','LU','NW','UR'], \n",
    "      'TG': ['SH','ZH','SG','Baden-Wurttemberg'], \n",
    "      'TI': ['UR','GR','Piemonte','Lombardia'], \n",
    "      'UR': ['TI','VS','GR','BE','NW','OW','SZ','GL'], \n",
    "      'VD': ['NE','GE','FR','VS','BE','Auvergne Rhone Alpes','Bourgogne Franche Comte'], \n",
    "      'VS': ['VD','BE','UR','Piemonte','Auvergne Rhone Alpes'], \n",
    "      'ZG': ['ZH','AG','LU','SZ'],\n",
    "      'ZH': ['SH','AG','ZG','SZ','SG','TG','Baden-Wurttemberg']\n",
    "    }\n",
    "    foph = pd.read_csv(\"data/FOPH/data/COVID19Cases_geoRegion.csv\", parse_dates=True)\n",
    "    # we have to convert each date string to datetime to merge the dataframes later\n",
    "    foph = foph.set_index('datum')\n",
    "    foph.index = pd.to_datetime(foph.index)\n",
    "    foph['rate_14_day_per_100k'] = (100000*foph['sum14d']) / foph['pop']\n",
    "    foph = foph[['geoRegion','rate_14_day_per_100k']]\n",
    "    foph.index = pd.to_datetime(foph.index)\n",
    "    ecdc = pd.read_csv(\"data/ECDC/ECDCsubnationalcaseweekly.csv\")\n",
    "    ecdc[['year_week']] = ecdc[['year_week']].applymap(lambda x: mondaysByWeekNr[x[0:4]+x[6:8]]) #datetime.datetime.strptime(x.replace('-','')+' MON', '%YW%U %a').date())\n",
    "    ecdc = ecdc.set_index(\"year_week\")\n",
    "    ecdc = ecdc[['region_name','rate_14_day_per_100k']]\n",
    "    ecdc.index = pd.to_datetime(ecdc.index)\n",
    "    temp = pd.DataFrame(index=pd.date_range(start=start, end=end))\n",
    "    for n in neigbors[cantonId]:\n",
    "        if len(n) != 2:\n",
    "            internationalRegion = ecdc.loc[ecdc['region_name']==n][['rate_14_day_per_100k']]\n",
    "            temp[[n]] = internationalRegion\n",
    "            temp[[n]] = temp[[n]].interpolate(method='linear')\n",
    "            temp.loc[temp.index[0],[n]] = 0\n",
    "        else:\n",
    "            nationalRegion = foph.loc[foph['geoRegion']==n][['rate_14_day_per_100k']]\n",
    "            temp[[n]] = nationalRegion\n",
    "            temp.loc[temp.index[0],[n]] = 0\n",
    "            temp[[n]] = temp[[n]].interpolate(method='linear')\n",
    "    imputer = KNNImputer(n_neighbors=10, weights=\"distance\")\n",
    "    temp = pd.DataFrame(imputer.fit_transform(temp.values), index=temp.index, columns=temp.columns)\n",
    "    temp.columns = ['incidence_'+col for col in temp.columns]\n",
    "    temp['meanNeighborIncidence'] = temp.mean(axis=1)\n",
    "    temp['maxNeighborIncidence'] = temp.max(axis=1)\n",
    "    df = df.join(temp[['meanNeighborIncidence','maxNeighborIncidence']]) \n",
    "    \n",
    "    \n",
    "    # compute statistic weather for missing values\n",
    "    statWeathDf = pd.read_csv(\"static_data/statistical_historicweather/statistical_\"+cantonId+\".csv\")\n",
    "    statWeathDf['date'] = statWeathDf.apply(lambda row: datetime.datetime(2020,int(row[\"month\"]),int(row[\"day\"])), axis=1)\n",
    "    statWeathDf = statWeathDf.set_index('date')\n",
    "    statWeathDf = statWeathDf[['temp.average_min','temp.average_max','clouds.mean','precipitation.mean']]\n",
    "    statWeathDf.columns = ['temp_min', 'temp_max', 'clouds', 'precipitation']\n",
    "    weather = statWeathDf\n",
    "    \n",
    "    # compute historic weather from stored data\n",
    "    storedWeathDf = pd.read_csv(\"static_data/historicweather_from_19_03_2020_to_17_03_2021/\"+cantonId+\".csv\")\n",
    "    storedWeathDf = storedWeathDf.set_index('dt')\n",
    "    storedWeathDf = storedWeathDf[['main.temp_min','main.temp_max','clouds.all', 'rain.1h','snow.1h']]\n",
    "    storedWeathDf = storedWeathDf.fillna(0)\n",
    "    storedWeathDf['precipitation'] = storedWeathDf[['rain.1h','snow.1h']].sum(axis=1)\n",
    "    storedWeathDf = storedWeathDf[['main.temp_min','main.temp_max','clouds.all','precipitation']] \n",
    "\n",
    "    startDate = datetime.datetime(2020, 3, 19)\n",
    "    endDate = datetime.datetime(2021, 2, 3)\n",
    "    temp = pd.DataFrame(index=pd.date_range(start=startDate, end=endDate), columns=storedWeathDf.columns)\n",
    "    for day in pd.date_range(start=startDate, end=endDate):   \n",
    "        oneDay = storedWeathDf.filter(like=day.strftime('%Y-%m-%d'), axis=0)\n",
    "        temp.loc[day] = {'main.temp_min': oneDay['main.temp_min'].min(), \n",
    "                         'main.temp_max': oneDay['main.temp_max'].max(), \n",
    "                         'clouds.all': oneDay['clouds.all'].mean(),\n",
    "                         'precipitation': oneDay['precipitation'].sum()}\n",
    "    storedWeathDf = temp\n",
    "    storedWeathDf.columns = ['temp_min', 'temp_max', 'clouds', 'precipitation']\n",
    "    weather = weather.append(storedWeathDf)\n",
    "\n",
    "    # compute historic weather from recently loaded weather update\n",
    "    updateWeathDf = pd.read_csv(\"data/historicweatherupdate/\"+cantonId+\".csv\")\n",
    "    updateWeathDf = updateWeathDf.set_index('dt')\n",
    "    updateWeathDf = updateWeathDf[['main.temp_min','main.temp_max','clouds.all', 'rain.1h','snow.1h']]\n",
    "    updateWeathDf = updateWeathDf.fillna(0)\n",
    "    updateWeathDf['precipitation'] = updateWeathDf[['rain.1h','snow.1h']].sum(axis=1)\n",
    "    updateWeathDf = updateWeathDf[['main.temp_min','main.temp_max','clouds.all','precipitation']] \n",
    "\n",
    "    endDate = datetime.datetime.strptime(updateWeathDf.index[-1], '%Y-%m-%d %H:%M:%S')\n",
    "    endDate = endDate.replace(hour=0, minute=0)\n",
    "    startDate = datetime.datetime(2021, 2, 4)\n",
    "    temp = pd.DataFrame(index=pd.date_range(start=startDate, end=endDate), columns=updateWeathDf.columns)\n",
    "    for day in pd.date_range(start=startDate, end=endDate):   \n",
    "        oneDay = updateWeathDf.filter(like=day.strftime('%Y-%m-%d'), axis=0)\n",
    "        temp.loc[day] = {'main.temp_min': oneDay['main.temp_min'].min(), \n",
    "                         'main.temp_max': oneDay['main.temp_max'].max(), \n",
    "                         'clouds.all': oneDay['clouds.all'].mean(),\n",
    "                         'precipitation': oneDay['precipitation'].sum()}\n",
    "    updateWeathDf = temp\n",
    "    updateWeathDf.columns = ['temp_min', 'temp_max', 'clouds', 'precipitation']\n",
    "    weather = weather.append(updateWeathDf)  \n",
    "    df = df.join(weather)\n",
    "       \n",
    "    df.index.names = [\"date\"]\n",
    "    if not os.path.exists('data/merged'):\n",
    "        os.makedirs('data/merged')\n",
    "    df[start:end].to_csv('data/merged/'+cantonId+'.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laden-resistance",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# fill the missing data\n",
    "\n",
    "mergedDict = {}\n",
    "for cantonId in cantonKeys: \n",
    "    mergedDict[cantonId] = pd.read_csv(\"data/merged/\"+cantonId+\".csv\").set_index('date')[start:end]\n",
    "    mergedDict[cantonId].index = pd.to_datetime(mergedDict[cantonId].index)\n",
    "    \n",
    "interpolMet = 'linear'\n",
    "originalDict = mergedDict.copy()\n",
    "#cantonKeys = ['AG']\n",
    "for cantonId in cantonKeys:\n",
    "    filled = mergedDict[cantonId].copy()\n",
    "    \n",
    "    # fill missing vaccine data\n",
    "    vaccine = ['VaccDosesAdministered sumTotal','VaccDosesAdministered per100PersonsTotal', \n",
    "               'FullyVaccPersons sumTotal', 'FullyVaccPersons per100PersonsTotal']\n",
    "    filled.loc['2020-02-15',vaccine] = 0\n",
    "    filled[vaccine] = filled[vaccine].fillna(method='ffill')\n",
    "    \n",
    "    # fill missing total hospital capacities\n",
    "    hospitalCols = ['ICU_Capacity','ICU_FreeCapacity','Total_Capacity','Total_FreeCapacity']\n",
    "    filled[hospitalCols] = filled[hospitalCols].interpolate(method=interpolMet)\n",
    "    filled[hospitalCols] = filled[hospitalCols].fillna(method='bfill')  \n",
    "\n",
    "    # fill in missing hospital capacity datanearest\n",
    "    # we make the assumption that at the beginning there were no covid patients\n",
    "    filled.loc['2020-02-15',['Total_Covid19Patients','ICU_Covid19Patients']] = 0\n",
    "    patientCols = ['Total_Covid19Patients','Total_NonCovid19Patients','Total_AllPatients','ICU_Covid19Patients',\n",
    "                   'ICU_NonCovid19Patients','ICU_AllPatients']\n",
    "    filled[patientCols] = filled[patientCols].interpolate(method=interpolMet)\n",
    "    filled[['Total_NonCovid19Patients','ICU_NonCovid19Patients']] = filled[['Total_NonCovid19Patients','ICU_NonCovid19Patients']].fillna(method='bfill')\n",
    "    # Covid19Patients + NonCovid19Patients = AllPatients\n",
    "    filled['Total_AllPatients'].fillna(filled[['Total_Covid19Patients','Total_NonCovid19Patients']].sum(axis=1), inplace=True)\n",
    "    filled['ICU_AllPatients'].fillna(filled[['ICU_Covid19Patients','ICU_NonCovid19Patients']].sum(axis=1), inplace=True)\n",
    "    \n",
    "    # fill in missing Google mobility data\n",
    "    googleMobilityCols = ['retail_and_recreation_percent_change_from_baseline','grocery_and_pharmacy_percent_change_from_baseline',\n",
    "    'parks_percent_change_from_baseline','transit_stations_percent_change_from_baseline','workplaces_percent_change_from_baseline'\n",
    "    ,'residential_percent_change_from_baseline']\n",
    "    # use the swiss average when possible\n",
    "    googleMobCH = pd.read_csv(\"data/GoogleMobility/2020_CH_Region_Mobility_Report.csv\")\n",
    "    googleMobCH = googleMobCH.loc[googleMobCH[\"sub_region_1\"].isna()].set_index('date')[googleMobilityCols]\n",
    "    googleMobCH.index = pd.to_datetime(googleMobCH.index)\n",
    "    for col in googleMobilityCols:\n",
    "        filled[col] = filled[col].fillna(googleMobCH[col])\n",
    "    # for the rest interpolate\n",
    "    filled[googleMobilityCols] = filled[googleMobilityCols].interpolate(method=interpolMet)\n",
    "    \n",
    "    # fill in missing Intervista mobility data \n",
    "    filled[['intervistaMob']] = filled[['intervistaMob']].interpolate(method=interpolMet)\n",
    "    filled['intervistaMob'].fillna(method='ffill', inplace=True)\n",
    "        \n",
    "    # fill in missing r values\n",
    "    rvalues = ['median_R_mean','median_R_highHPD','median_R_lowHPD']\n",
    "    filled[rvalues] = filled[rvalues].interpolate(method=interpolMet)\n",
    "    # move them 14 days forward\n",
    "    filled[rvalues] = filled[rvalues].shift(periods=14)\n",
    "    filled[rvalues] = filled[rvalues].fillna(method='ffill')\n",
    "    filled[rvalues] = filled[rvalues].fillna(method='bfill')\n",
    "  \n",
    "    # fill in missing variants\n",
    "    # first detected case for variants of concerne in Switzerland is 2020-10-14\n",
    "    variants = ['lower_ci_day','upper_ci_day','anteil_pos']\n",
    "    filled.loc['2020-10-13',['lower_ci_day','anteil_pos']] = 0\n",
    "    filled.loc['2020-10-13',['upper_ci_day']] = 100\n",
    "    filled[variants] = filled[variants].interpolate(method=interpolMet)\n",
    "    filled[variants] = filled[variants].fillna(method='ffill')\n",
    "    filled[variants] = filled[variants].fillna(method='bfill')\n",
    "\n",
    "    # fill in daily incoming missing data\n",
    "    zeroAndInterpolate = ['case_entries','hosp_entries','death_entries','case_inz_entries','hosp_inz_entries',\n",
    "                          'death_inz_entries','case_inzsumTotal','hosp_inzsumTotal','death_inzsumTotal',\n",
    "                          'test_inzsumTotal',\n",
    "                          'Cases entries 0 - 9','Cases entries 10 - 19','Cases entries 20 - 29',\n",
    "                          'Cases entries 30 - 39','Cases entries 40 - 49','Cases entries 50 - 59',\n",
    "                          'Cases entries 60 - 69','Cases entries 70 - 79','Cases entries 80+',\n",
    "                          'Cases entries male','Cases entries female','Cases inz_entries 0 - 9',\n",
    "                          'Cases inz_entries 10 - 19','Cases inz_entries 20 - 29','Cases inz_entries 30 - 39',\n",
    "                          'Cases inz_entries 40 - 49','Cases inz_entries 50 - 59','Cases inz_entries 60 - 69',\n",
    "                          'Cases inz_entries 70 - 79','Cases inz_entries 80+','Cases inz_entries male',\n",
    "                          'Cases inz_entries female','Cases inzsumTotal 0 - 9','Cases inzsumTotal 10 - 19',\n",
    "                          'Cases inzsumTotal 20 - 29','Cases inzsumTotal 30 - 39','Cases inzsumTotal 40 - 49',\n",
    "                          'Cases inzsumTotal 50 - 59','Cases inzsumTotal 60 - 69','Cases inzsumTotal 70 - 79',\n",
    "                          'Cases inzsumTotal 80+','Cases inzsumTotal male','Cases inzsumTotal female',\n",
    "                          'Death entries 0 - 9','Death entries 10 - 19','Death entries 20 - 29',\n",
    "                          'Death entries 30 - 39','Death entries 40 - 49','Death entries 50 - 59',\n",
    "                          'Death entries 60 - 69','Death entries 70 - 79','Death entries 80+',\n",
    "                          'Death entries male','Death entries female','Death inz_entries 0 - 9',\n",
    "                          'Death inz_entries 10 - 19','Death inz_entries 20 - 29','Death inz_entries 30 - 39',\n",
    "                          'Death inz_entries 40 - 49','Death inz_entries 50 - 59','Death inz_entries 60 - 69',\n",
    "                          'Death inz_entries 70 - 79','Death inz_entries 80+','Death inz_entries male',\n",
    "                          'Death inz_entries female','Death inzsumTotal 0 - 9','Death inzsumTotal 10 - 19',\n",
    "                          'Death inzsumTotal 20 - 29','Death inzsumTotal 30 - 39','Death inzsumTotal 40 - 49',\n",
    "                          'Death inzsumTotal 50 - 59','Death inzsumTotal 60 - 69','Death inzsumTotal 70 - 79',\n",
    "                          'Death inzsumTotal 80+','Death inzsumTotal male','Death inzsumTotal female',\n",
    "                          'Hosp entries 0 - 9','Hosp entries 10 - 19','Hosp entries 20 - 29',\n",
    "                          'Hosp entries 30 - 39','Hosp entries 40 - 49','Hosp entries 50 - 59',\n",
    "                          'Hosp entries 60 - 69','Hosp entries 70 - 79','Hosp entries 80+','Hosp entries male',\n",
    "                          'Hosp entries female','Hosp inz_entries 0 - 9','Hosp inz_entries 10 - 19',\n",
    "                          'Hosp inz_entries 20 - 29','Hosp inz_entries 30 - 39','Hosp inz_entries 40 - 49',\n",
    "                          'Hosp inz_entries 50 - 59','Hosp inz_entries 60 - 69','Hosp inz_entries 70 - 79',\n",
    "                          'Hosp inz_entries 80+','Hosp inz_entries male','Hosp inz_entries female',\n",
    "                          'Hosp inzsumTotal 0 - 9','Hosp inzsumTotal 10 - 19','Hosp inzsumTotal 20 - 29',\n",
    "                          'Hosp inzsumTotal 30 - 39','Hosp inzsumTotal 40 - 49','Hosp inzsumTotal 50 - 59',\n",
    "                          'Hosp inzsumTotal 60 - 69','Hosp inzsumTotal 70 - 79','Hosp inzsumTotal 80+',\n",
    "                          'Hosp inzsumTotal male','Hosp inzsumTotal female']\n",
    "    filled.loc['2020-02-15',zeroAndInterpolate] = 0\n",
    "    filled[zeroAndInterpolate] = filled[zeroAndInterpolate].interpolate(method=interpolMet)\n",
    "    \n",
    "    if not os.path.exists('data/filled'):\n",
    "        os.makedirs('data/filled')\n",
    "    filled.to_csv('data/filled/'+cantonId+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enclosed-colombia",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# check complete data if there are any NaNs left\n",
    "for cantonId in cantonKeys:\n",
    "    filled = pd.read_csv('data/filled/'+cantonId+'.csv')\n",
    "    for col in filled.columns:\n",
    "        if filled[col].isna().sum() != 0:\n",
    "            print(cantonId+\" \"+col+\" (#NaN/#NotNaN): (\" + str(filled[col].isna().sum())+\"/\"+str(filled[col].notna().sum())+\")\")\n",
    "            #dict[cantonId][col].plot(kind='line',y=[col], figsize=(20,10))\n",
    "            #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floral-genome",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plotting original data vs filled data\n",
    "#for col in dict['AG'].columns:\n",
    "#    comparingDf = pd.concat([dict['AG'][[col]],originalData[[\"original_\"+col]]], axis=1)\n",
    "    #comparingDf[['original_'+col]].reset_index().plot(kind='scatter', x=['date'], y=['original_'+col], figsize=(20,10))\n",
    "    #comparingDf['2020-02-15':'2021-04-05'].plot(kind='line',y=[col], figsize=(20,10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seasonal-bradford",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# FEATURE ENGINEERING\n",
    "filledDict = {}\n",
    "\n",
    "for cantonId in cantonKeys:\n",
    "    filledDict[cantonId] = pd.read_csv(\"data/filled/\"+cantonId+\".csv\").set_index('date')\n",
    "    \n",
    "    dailyFeatures = filledDict[cantonId].copy()\n",
    "    dailyFeatures.index = pd.to_datetime(dailyFeatures.index) \n",
    "\n",
    "    # summarize mask mandatories\n",
    "    maskMandatories = [ 'Mask mandatory in publicly accessible establishments/ spaces (shops etc.)',\n",
    "                       'Mask mandatory in public transport','Masks mandatory in schools','Masks mandatory at work']\n",
    "    dailyFeatures[['maskMandatories']] = dailyFeatures[maskMandatories].sum(axis=1)\n",
    "    dailyFeatures.drop(maskMandatories, axis=1, inplace=True)\n",
    "    \n",
    "    # r value accuracy\n",
    "    dailyFeatures[['R_error']] = dailyFeatures['median_R_highHPD']-dailyFeatures['median_R_lowHPD']\n",
    "    dailyFeatures.drop(['median_R_highHPD','median_R_lowHPD'],axis=1, inplace=True)\n",
    "\n",
    "    # variants accuracy\n",
    "    #features[['anteil_pos','upper_ci_day','lower_ci_day']] = features[['anteil_pos','upper_ci_day','lower_ci_day']].rolling(window=w).mean()\n",
    "    dailyFeatures[['variant_error']] = dailyFeatures['upper_ci_day']-dailyFeatures['lower_ci_day']\n",
    "    dailyFeatures.drop(['upper_ci_day','lower_ci_day'],axis=1, inplace=True)\n",
    "\n",
    "    # vaccine\n",
    "    dailyFeatures.drop(['VaccDosesAdministered sumTotal','FullyVaccPersons sumTotal'],axis=1, inplace=True)\n",
    "    vaccine = ['VaccDosesAdministered per100PersonsTotal',\n",
    "               'FullyVaccPersons per100PersonsTotal']\n",
    "    #features[vaccine] = features[vaccine].rolling(window=w).mean()\n",
    "    \n",
    "    # test positivity rate\n",
    "    dailyFeatures[['testPositvity']] = dailyFeatures['case_entries']/dailyFeatures['test_entries']\n",
    "    dailyFeatures.drop(['test_entries'],axis=1, inplace=True)\n",
    "\n",
    "    # hospital capacities\n",
    "    hospCap = [ 'ICU_AllPatients',\n",
    "     'ICU_Covid19Patients',\n",
    "     'ICU_Capacity',\n",
    "     'Total_AllPatients',\n",
    "     'Total_Covid19Patients',\n",
    "     'Total_Capacity',\n",
    "     'ICU_NonCovid19Patients',\n",
    "     'ICU_FreeCapacity',\n",
    "     'Total_NonCovid19Patients',\n",
    "     'Total_FreeCapacity']\n",
    "    staticCantonal = pd.read_excel(\"static_data/staticCantonalData.xlsx\").set_index('canton').transpose()\n",
    "    dailyFeatures[[col + \"_inz\" for col in hospCap]] = 100000*(dailyFeatures[hospCap]/staticCantonal.loc[[cantonId]]['residents'][0])\n",
    "    dailyFeatures.drop(hospCap,axis=1, inplace=True)\n",
    "    \n",
    "    if not os.path.exists('data/dailyFeatures'):\n",
    "        os.makedirs('data/dailyFeatures')\n",
    "    dailyFeatures.to_csv('data/dailyFeatures/'+cantonId+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "infectious-nursery",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(datetime.date(2020, 2, 15), datetime.date(2020, 2, 28)),\n",
       " (datetime.date(2020, 2, 16), datetime.date(2020, 2, 29)),\n",
       " (datetime.date(2020, 2, 17), datetime.date(2020, 3, 1)),\n",
       " (datetime.date(2020, 2, 18), datetime.date(2020, 3, 2)),\n",
       " (datetime.date(2020, 2, 19), datetime.date(2020, 3, 3)),\n",
       " (datetime.date(2020, 2, 20), datetime.date(2020, 3, 4)),\n",
       " (datetime.date(2020, 2, 21), datetime.date(2020, 3, 5)),\n",
       " (datetime.date(2020, 2, 22), datetime.date(2020, 3, 6)),\n",
       " (datetime.date(2020, 2, 23), datetime.date(2020, 3, 7)),\n",
       " (datetime.date(2020, 2, 24), datetime.date(2020, 3, 8)),\n",
       " (datetime.date(2020, 2, 25), datetime.date(2020, 3, 9)),\n",
       " (datetime.date(2020, 2, 26), datetime.date(2020, 3, 10)),\n",
       " (datetime.date(2020, 2, 27), datetime.date(2020, 3, 11)),\n",
       " (datetime.date(2020, 2, 28), datetime.date(2020, 3, 12)),\n",
       " (datetime.date(2020, 2, 29), datetime.date(2020, 3, 13)),\n",
       " (datetime.date(2020, 3, 1), datetime.date(2020, 3, 14)),\n",
       " (datetime.date(2020, 3, 2), datetime.date(2020, 3, 15)),\n",
       " (datetime.date(2020, 3, 3), datetime.date(2020, 3, 16)),\n",
       " (datetime.date(2020, 3, 4), datetime.date(2020, 3, 17)),\n",
       " (datetime.date(2020, 3, 5), datetime.date(2020, 3, 18)),\n",
       " (datetime.date(2020, 3, 6), datetime.date(2020, 3, 19)),\n",
       " (datetime.date(2020, 3, 7), datetime.date(2020, 3, 20)),\n",
       " (datetime.date(2020, 3, 8), datetime.date(2020, 3, 21)),\n",
       " (datetime.date(2020, 3, 9), datetime.date(2020, 3, 22)),\n",
       " (datetime.date(2020, 3, 10), datetime.date(2020, 3, 23)),\n",
       " (datetime.date(2020, 3, 11), datetime.date(2020, 3, 24)),\n",
       " (datetime.date(2020, 3, 12), datetime.date(2020, 3, 25)),\n",
       " (datetime.date(2020, 3, 13), datetime.date(2020, 3, 26)),\n",
       " (datetime.date(2020, 3, 14), datetime.date(2020, 3, 27)),\n",
       " (datetime.date(2020, 3, 15), datetime.date(2020, 3, 28)),\n",
       " (datetime.date(2020, 3, 16), datetime.date(2020, 3, 29)),\n",
       " (datetime.date(2020, 3, 17), datetime.date(2020, 3, 30)),\n",
       " (datetime.date(2020, 3, 18), datetime.date(2020, 3, 31)),\n",
       " (datetime.date(2020, 3, 19), datetime.date(2020, 4, 1)),\n",
       " (datetime.date(2020, 3, 20), datetime.date(2020, 4, 2)),\n",
       " (datetime.date(2020, 3, 21), datetime.date(2020, 4, 3)),\n",
       " (datetime.date(2020, 3, 22), datetime.date(2020, 4, 4)),\n",
       " (datetime.date(2020, 3, 23), datetime.date(2020, 4, 5)),\n",
       " (datetime.date(2020, 3, 24), datetime.date(2020, 4, 6)),\n",
       " (datetime.date(2020, 3, 25), datetime.date(2020, 4, 7)),\n",
       " (datetime.date(2020, 3, 26), datetime.date(2020, 4, 8)),\n",
       " (datetime.date(2020, 3, 27), datetime.date(2020, 4, 9)),\n",
       " (datetime.date(2020, 3, 28), datetime.date(2020, 4, 10)),\n",
       " (datetime.date(2020, 3, 29), datetime.date(2020, 4, 11)),\n",
       " (datetime.date(2020, 3, 30), datetime.date(2020, 4, 12)),\n",
       " (datetime.date(2020, 3, 31), datetime.date(2020, 4, 13)),\n",
       " (datetime.date(2020, 4, 1), datetime.date(2020, 4, 14)),\n",
       " (datetime.date(2020, 4, 2), datetime.date(2020, 4, 15)),\n",
       " (datetime.date(2020, 4, 3), datetime.date(2020, 4, 16)),\n",
       " (datetime.date(2020, 4, 4), datetime.date(2020, 4, 17)),\n",
       " (datetime.date(2020, 4, 5), datetime.date(2020, 4, 18)),\n",
       " (datetime.date(2020, 4, 6), datetime.date(2020, 4, 19)),\n",
       " (datetime.date(2020, 4, 7), datetime.date(2020, 4, 20)),\n",
       " (datetime.date(2020, 4, 8), datetime.date(2020, 4, 21)),\n",
       " (datetime.date(2020, 4, 9), datetime.date(2020, 4, 22)),\n",
       " (datetime.date(2020, 4, 10), datetime.date(2020, 4, 23)),\n",
       " (datetime.date(2020, 4, 11), datetime.date(2020, 4, 24)),\n",
       " (datetime.date(2020, 4, 12), datetime.date(2020, 4, 25)),\n",
       " (datetime.date(2020, 4, 13), datetime.date(2020, 4, 26)),\n",
       " (datetime.date(2020, 4, 14), datetime.date(2020, 4, 27)),\n",
       " (datetime.date(2020, 4, 15), datetime.date(2020, 4, 28)),\n",
       " (datetime.date(2020, 4, 16), datetime.date(2020, 4, 29)),\n",
       " (datetime.date(2020, 4, 17), datetime.date(2020, 4, 30)),\n",
       " (datetime.date(2020, 4, 18), datetime.date(2020, 5, 1)),\n",
       " (datetime.date(2020, 4, 19), datetime.date(2020, 5, 2)),\n",
       " (datetime.date(2020, 4, 20), datetime.date(2020, 5, 3)),\n",
       " (datetime.date(2020, 4, 21), datetime.date(2020, 5, 4)),\n",
       " (datetime.date(2020, 4, 22), datetime.date(2020, 5, 5)),\n",
       " (datetime.date(2020, 4, 23), datetime.date(2020, 5, 6)),\n",
       " (datetime.date(2020, 4, 24), datetime.date(2020, 5, 7)),\n",
       " (datetime.date(2020, 4, 25), datetime.date(2020, 5, 8)),\n",
       " (datetime.date(2020, 4, 26), datetime.date(2020, 5, 9)),\n",
       " (datetime.date(2020, 4, 27), datetime.date(2020, 5, 10)),\n",
       " (datetime.date(2020, 4, 28), datetime.date(2020, 5, 11)),\n",
       " (datetime.date(2020, 4, 29), datetime.date(2020, 5, 12)),\n",
       " (datetime.date(2020, 4, 30), datetime.date(2020, 5, 13)),\n",
       " (datetime.date(2020, 5, 1), datetime.date(2020, 5, 14)),\n",
       " (datetime.date(2020, 5, 2), datetime.date(2020, 5, 15)),\n",
       " (datetime.date(2020, 5, 3), datetime.date(2020, 5, 16)),\n",
       " (datetime.date(2020, 5, 4), datetime.date(2020, 5, 17)),\n",
       " (datetime.date(2020, 5, 5), datetime.date(2020, 5, 18)),\n",
       " (datetime.date(2020, 5, 6), datetime.date(2020, 5, 19)),\n",
       " (datetime.date(2020, 5, 7), datetime.date(2020, 5, 20)),\n",
       " (datetime.date(2020, 5, 8), datetime.date(2020, 5, 21)),\n",
       " (datetime.date(2020, 5, 9), datetime.date(2020, 5, 22)),\n",
       " (datetime.date(2020, 5, 10), datetime.date(2020, 5, 23)),\n",
       " (datetime.date(2020, 5, 11), datetime.date(2020, 5, 24)),\n",
       " (datetime.date(2020, 5, 12), datetime.date(2020, 5, 25)),\n",
       " (datetime.date(2020, 5, 13), datetime.date(2020, 5, 26)),\n",
       " (datetime.date(2020, 5, 14), datetime.date(2020, 5, 27)),\n",
       " (datetime.date(2020, 5, 15), datetime.date(2020, 5, 28)),\n",
       " (datetime.date(2020, 5, 16), datetime.date(2020, 5, 29)),\n",
       " (datetime.date(2020, 5, 17), datetime.date(2020, 5, 30)),\n",
       " (datetime.date(2020, 5, 18), datetime.date(2020, 5, 31)),\n",
       " (datetime.date(2020, 5, 19), datetime.date(2020, 6, 1)),\n",
       " (datetime.date(2020, 5, 20), datetime.date(2020, 6, 2)),\n",
       " (datetime.date(2020, 5, 21), datetime.date(2020, 6, 3)),\n",
       " (datetime.date(2020, 5, 22), datetime.date(2020, 6, 4)),\n",
       " (datetime.date(2020, 5, 23), datetime.date(2020, 6, 5)),\n",
       " (datetime.date(2020, 5, 24), datetime.date(2020, 6, 6)),\n",
       " (datetime.date(2020, 5, 25), datetime.date(2020, 6, 7)),\n",
       " (datetime.date(2020, 5, 26), datetime.date(2020, 6, 8)),\n",
       " (datetime.date(2020, 5, 27), datetime.date(2020, 6, 9)),\n",
       " (datetime.date(2020, 5, 28), datetime.date(2020, 6, 10)),\n",
       " (datetime.date(2020, 5, 29), datetime.date(2020, 6, 11)),\n",
       " (datetime.date(2020, 5, 30), datetime.date(2020, 6, 12)),\n",
       " (datetime.date(2020, 5, 31), datetime.date(2020, 6, 13)),\n",
       " (datetime.date(2020, 6, 1), datetime.date(2020, 6, 14)),\n",
       " (datetime.date(2020, 6, 2), datetime.date(2020, 6, 15)),\n",
       " (datetime.date(2020, 6, 3), datetime.date(2020, 6, 16)),\n",
       " (datetime.date(2020, 6, 4), datetime.date(2020, 6, 17)),\n",
       " (datetime.date(2020, 6, 5), datetime.date(2020, 6, 18)),\n",
       " (datetime.date(2020, 6, 6), datetime.date(2020, 6, 19)),\n",
       " (datetime.date(2020, 6, 7), datetime.date(2020, 6, 20)),\n",
       " (datetime.date(2020, 6, 8), datetime.date(2020, 6, 21)),\n",
       " (datetime.date(2020, 6, 9), datetime.date(2020, 6, 22)),\n",
       " (datetime.date(2020, 6, 10), datetime.date(2020, 6, 23)),\n",
       " (datetime.date(2020, 6, 11), datetime.date(2020, 6, 24)),\n",
       " (datetime.date(2020, 6, 12), datetime.date(2020, 6, 25)),\n",
       " (datetime.date(2020, 6, 13), datetime.date(2020, 6, 26)),\n",
       " (datetime.date(2020, 6, 14), datetime.date(2020, 6, 27)),\n",
       " (datetime.date(2020, 6, 15), datetime.date(2020, 6, 28)),\n",
       " (datetime.date(2020, 6, 16), datetime.date(2020, 6, 29)),\n",
       " (datetime.date(2020, 6, 17), datetime.date(2020, 6, 30)),\n",
       " (datetime.date(2020, 6, 18), datetime.date(2020, 7, 1)),\n",
       " (datetime.date(2020, 6, 19), datetime.date(2020, 7, 2)),\n",
       " (datetime.date(2020, 6, 20), datetime.date(2020, 7, 3)),\n",
       " (datetime.date(2020, 6, 21), datetime.date(2020, 7, 4)),\n",
       " (datetime.date(2020, 6, 22), datetime.date(2020, 7, 5)),\n",
       " (datetime.date(2020, 6, 23), datetime.date(2020, 7, 6)),\n",
       " (datetime.date(2020, 6, 24), datetime.date(2020, 7, 7)),\n",
       " (datetime.date(2020, 6, 25), datetime.date(2020, 7, 8)),\n",
       " (datetime.date(2020, 6, 26), datetime.date(2020, 7, 9)),\n",
       " (datetime.date(2020, 6, 27), datetime.date(2020, 7, 10)),\n",
       " (datetime.date(2020, 6, 28), datetime.date(2020, 7, 11)),\n",
       " (datetime.date(2020, 6, 29), datetime.date(2020, 7, 12)),\n",
       " (datetime.date(2020, 6, 30), datetime.date(2020, 7, 13)),\n",
       " (datetime.date(2020, 7, 1), datetime.date(2020, 7, 14)),\n",
       " (datetime.date(2020, 7, 2), datetime.date(2020, 7, 15)),\n",
       " (datetime.date(2020, 7, 3), datetime.date(2020, 7, 16)),\n",
       " (datetime.date(2020, 7, 4), datetime.date(2020, 7, 17)),\n",
       " (datetime.date(2020, 7, 5), datetime.date(2020, 7, 18)),\n",
       " (datetime.date(2020, 7, 6), datetime.date(2020, 7, 19)),\n",
       " (datetime.date(2020, 7, 7), datetime.date(2020, 7, 20)),\n",
       " (datetime.date(2020, 7, 8), datetime.date(2020, 7, 21)),\n",
       " (datetime.date(2020, 7, 9), datetime.date(2020, 7, 22)),\n",
       " (datetime.date(2020, 7, 10), datetime.date(2020, 7, 23)),\n",
       " (datetime.date(2020, 7, 11), datetime.date(2020, 7, 24)),\n",
       " (datetime.date(2020, 7, 12), datetime.date(2020, 7, 25)),\n",
       " (datetime.date(2020, 7, 13), datetime.date(2020, 7, 26)),\n",
       " (datetime.date(2020, 7, 14), datetime.date(2020, 7, 27)),\n",
       " (datetime.date(2020, 7, 15), datetime.date(2020, 7, 28)),\n",
       " (datetime.date(2020, 7, 16), datetime.date(2020, 7, 29)),\n",
       " (datetime.date(2020, 7, 17), datetime.date(2020, 7, 30)),\n",
       " (datetime.date(2020, 7, 18), datetime.date(2020, 7, 31)),\n",
       " (datetime.date(2020, 7, 19), datetime.date(2020, 8, 1)),\n",
       " (datetime.date(2020, 7, 20), datetime.date(2020, 8, 2)),\n",
       " (datetime.date(2020, 7, 21), datetime.date(2020, 8, 3)),\n",
       " (datetime.date(2020, 7, 22), datetime.date(2020, 8, 4)),\n",
       " (datetime.date(2020, 7, 23), datetime.date(2020, 8, 5)),\n",
       " (datetime.date(2020, 7, 24), datetime.date(2020, 8, 6)),\n",
       " (datetime.date(2020, 7, 25), datetime.date(2020, 8, 7)),\n",
       " (datetime.date(2020, 7, 26), datetime.date(2020, 8, 8)),\n",
       " (datetime.date(2020, 7, 27), datetime.date(2020, 8, 9)),\n",
       " (datetime.date(2020, 7, 28), datetime.date(2020, 8, 10)),\n",
       " (datetime.date(2020, 7, 29), datetime.date(2020, 8, 11)),\n",
       " (datetime.date(2020, 7, 30), datetime.date(2020, 8, 12)),\n",
       " (datetime.date(2020, 7, 31), datetime.date(2020, 8, 13)),\n",
       " (datetime.date(2020, 8, 1), datetime.date(2020, 8, 14)),\n",
       " (datetime.date(2020, 8, 2), datetime.date(2020, 8, 15)),\n",
       " (datetime.date(2020, 8, 3), datetime.date(2020, 8, 16)),\n",
       " (datetime.date(2020, 8, 4), datetime.date(2020, 8, 17)),\n",
       " (datetime.date(2020, 8, 5), datetime.date(2020, 8, 18)),\n",
       " (datetime.date(2020, 8, 6), datetime.date(2020, 8, 19)),\n",
       " (datetime.date(2020, 8, 7), datetime.date(2020, 8, 20)),\n",
       " (datetime.date(2020, 8, 8), datetime.date(2020, 8, 21)),\n",
       " (datetime.date(2020, 8, 9), datetime.date(2020, 8, 22)),\n",
       " (datetime.date(2020, 8, 10), datetime.date(2020, 8, 23)),\n",
       " (datetime.date(2020, 8, 11), datetime.date(2020, 8, 24)),\n",
       " (datetime.date(2020, 8, 12), datetime.date(2020, 8, 25)),\n",
       " (datetime.date(2020, 8, 13), datetime.date(2020, 8, 26)),\n",
       " (datetime.date(2020, 8, 14), datetime.date(2020, 8, 27)),\n",
       " (datetime.date(2020, 8, 15), datetime.date(2020, 8, 28)),\n",
       " (datetime.date(2020, 8, 16), datetime.date(2020, 8, 29)),\n",
       " (datetime.date(2020, 8, 17), datetime.date(2020, 8, 30)),\n",
       " (datetime.date(2020, 8, 18), datetime.date(2020, 8, 31)),\n",
       " (datetime.date(2020, 8, 19), datetime.date(2020, 9, 1)),\n",
       " (datetime.date(2020, 8, 20), datetime.date(2020, 9, 2)),\n",
       " (datetime.date(2020, 8, 21), datetime.date(2020, 9, 3)),\n",
       " (datetime.date(2020, 8, 22), datetime.date(2020, 9, 4)),\n",
       " (datetime.date(2020, 8, 23), datetime.date(2020, 9, 5)),\n",
       " (datetime.date(2020, 8, 24), datetime.date(2020, 9, 6)),\n",
       " (datetime.date(2020, 8, 25), datetime.date(2020, 9, 7)),\n",
       " (datetime.date(2020, 8, 26), datetime.date(2020, 9, 8)),\n",
       " (datetime.date(2020, 8, 27), datetime.date(2020, 9, 9)),\n",
       " (datetime.date(2020, 8, 28), datetime.date(2020, 9, 10)),\n",
       " (datetime.date(2020, 8, 29), datetime.date(2020, 9, 11)),\n",
       " (datetime.date(2020, 8, 30), datetime.date(2020, 9, 12)),\n",
       " (datetime.date(2020, 8, 31), datetime.date(2020, 9, 13)),\n",
       " (datetime.date(2020, 9, 1), datetime.date(2020, 9, 14)),\n",
       " (datetime.date(2020, 9, 2), datetime.date(2020, 9, 15)),\n",
       " (datetime.date(2020, 9, 3), datetime.date(2020, 9, 16)),\n",
       " (datetime.date(2020, 9, 4), datetime.date(2020, 9, 17)),\n",
       " (datetime.date(2020, 9, 5), datetime.date(2020, 9, 18)),\n",
       " (datetime.date(2020, 9, 6), datetime.date(2020, 9, 19)),\n",
       " (datetime.date(2020, 9, 7), datetime.date(2020, 9, 20)),\n",
       " (datetime.date(2020, 9, 8), datetime.date(2020, 9, 21)),\n",
       " (datetime.date(2020, 9, 9), datetime.date(2020, 9, 22)),\n",
       " (datetime.date(2020, 9, 10), datetime.date(2020, 9, 23)),\n",
       " (datetime.date(2020, 9, 11), datetime.date(2020, 9, 24)),\n",
       " (datetime.date(2020, 9, 12), datetime.date(2020, 9, 25)),\n",
       " (datetime.date(2020, 9, 13), datetime.date(2020, 9, 26)),\n",
       " (datetime.date(2020, 9, 14), datetime.date(2020, 9, 27)),\n",
       " (datetime.date(2020, 9, 15), datetime.date(2020, 9, 28)),\n",
       " (datetime.date(2020, 9, 16), datetime.date(2020, 9, 29)),\n",
       " (datetime.date(2020, 9, 17), datetime.date(2020, 9, 30)),\n",
       " (datetime.date(2020, 9, 18), datetime.date(2020, 10, 1)),\n",
       " (datetime.date(2020, 9, 19), datetime.date(2020, 10, 2)),\n",
       " (datetime.date(2020, 9, 20), datetime.date(2020, 10, 3)),\n",
       " (datetime.date(2020, 9, 21), datetime.date(2020, 10, 4)),\n",
       " (datetime.date(2020, 9, 22), datetime.date(2020, 10, 5)),\n",
       " (datetime.date(2020, 9, 23), datetime.date(2020, 10, 6)),\n",
       " (datetime.date(2020, 9, 24), datetime.date(2020, 10, 7)),\n",
       " (datetime.date(2020, 9, 25), datetime.date(2020, 10, 8)),\n",
       " (datetime.date(2020, 9, 26), datetime.date(2020, 10, 9)),\n",
       " (datetime.date(2020, 9, 27), datetime.date(2020, 10, 10)),\n",
       " (datetime.date(2020, 9, 28), datetime.date(2020, 10, 11)),\n",
       " (datetime.date(2020, 9, 29), datetime.date(2020, 10, 12)),\n",
       " (datetime.date(2020, 9, 30), datetime.date(2020, 10, 13)),\n",
       " (datetime.date(2020, 10, 1), datetime.date(2020, 10, 14)),\n",
       " (datetime.date(2020, 10, 2), datetime.date(2020, 10, 15)),\n",
       " (datetime.date(2020, 10, 3), datetime.date(2020, 10, 16)),\n",
       " (datetime.date(2020, 10, 4), datetime.date(2020, 10, 17)),\n",
       " (datetime.date(2020, 10, 5), datetime.date(2020, 10, 18)),\n",
       " (datetime.date(2020, 10, 6), datetime.date(2020, 10, 19)),\n",
       " (datetime.date(2020, 10, 7), datetime.date(2020, 10, 20)),\n",
       " (datetime.date(2020, 10, 8), datetime.date(2020, 10, 21)),\n",
       " (datetime.date(2020, 10, 9), datetime.date(2020, 10, 22)),\n",
       " (datetime.date(2020, 10, 10), datetime.date(2020, 10, 23)),\n",
       " (datetime.date(2020, 10, 11), datetime.date(2020, 10, 24)),\n",
       " (datetime.date(2020, 10, 12), datetime.date(2020, 10, 25)),\n",
       " (datetime.date(2020, 10, 13), datetime.date(2020, 10, 26)),\n",
       " (datetime.date(2020, 10, 14), datetime.date(2020, 10, 27)),\n",
       " (datetime.date(2020, 10, 15), datetime.date(2020, 10, 28)),\n",
       " (datetime.date(2020, 10, 16), datetime.date(2020, 10, 29)),\n",
       " (datetime.date(2020, 10, 17), datetime.date(2020, 10, 30)),\n",
       " (datetime.date(2020, 10, 18), datetime.date(2020, 10, 31)),\n",
       " (datetime.date(2020, 10, 19), datetime.date(2020, 11, 1)),\n",
       " (datetime.date(2020, 10, 20), datetime.date(2020, 11, 2)),\n",
       " (datetime.date(2020, 10, 21), datetime.date(2020, 11, 3)),\n",
       " (datetime.date(2020, 10, 22), datetime.date(2020, 11, 4)),\n",
       " (datetime.date(2020, 10, 23), datetime.date(2020, 11, 5)),\n",
       " (datetime.date(2020, 10, 24), datetime.date(2020, 11, 6)),\n",
       " (datetime.date(2020, 10, 25), datetime.date(2020, 11, 7)),\n",
       " (datetime.date(2020, 10, 26), datetime.date(2020, 11, 8)),\n",
       " (datetime.date(2020, 10, 27), datetime.date(2020, 11, 9)),\n",
       " (datetime.date(2020, 10, 28), datetime.date(2020, 11, 10)),\n",
       " (datetime.date(2020, 10, 29), datetime.date(2020, 11, 11)),\n",
       " (datetime.date(2020, 10, 30), datetime.date(2020, 11, 12)),\n",
       " (datetime.date(2020, 10, 31), datetime.date(2020, 11, 13)),\n",
       " (datetime.date(2020, 11, 1), datetime.date(2020, 11, 14)),\n",
       " (datetime.date(2020, 11, 2), datetime.date(2020, 11, 15)),\n",
       " (datetime.date(2020, 11, 3), datetime.date(2020, 11, 16)),\n",
       " (datetime.date(2020, 11, 4), datetime.date(2020, 11, 17)),\n",
       " (datetime.date(2020, 11, 5), datetime.date(2020, 11, 18)),\n",
       " (datetime.date(2020, 11, 6), datetime.date(2020, 11, 19)),\n",
       " (datetime.date(2020, 11, 7), datetime.date(2020, 11, 20)),\n",
       " (datetime.date(2020, 11, 8), datetime.date(2020, 11, 21)),\n",
       " (datetime.date(2020, 11, 9), datetime.date(2020, 11, 22)),\n",
       " (datetime.date(2020, 11, 10), datetime.date(2020, 11, 23)),\n",
       " (datetime.date(2020, 11, 11), datetime.date(2020, 11, 24)),\n",
       " (datetime.date(2020, 11, 12), datetime.date(2020, 11, 25)),\n",
       " (datetime.date(2020, 11, 13), datetime.date(2020, 11, 26)),\n",
       " (datetime.date(2020, 11, 14), datetime.date(2020, 11, 27)),\n",
       " (datetime.date(2020, 11, 15), datetime.date(2020, 11, 28)),\n",
       " (datetime.date(2020, 11, 16), datetime.date(2020, 11, 29)),\n",
       " (datetime.date(2020, 11, 17), datetime.date(2020, 11, 30)),\n",
       " (datetime.date(2020, 11, 18), datetime.date(2020, 12, 1)),\n",
       " (datetime.date(2020, 11, 19), datetime.date(2020, 12, 2)),\n",
       " (datetime.date(2020, 11, 20), datetime.date(2020, 12, 3)),\n",
       " (datetime.date(2020, 11, 21), datetime.date(2020, 12, 4)),\n",
       " (datetime.date(2020, 11, 22), datetime.date(2020, 12, 5)),\n",
       " (datetime.date(2020, 11, 23), datetime.date(2020, 12, 6)),\n",
       " (datetime.date(2020, 11, 24), datetime.date(2020, 12, 7)),\n",
       " (datetime.date(2020, 11, 25), datetime.date(2020, 12, 8)),\n",
       " (datetime.date(2020, 11, 26), datetime.date(2020, 12, 9)),\n",
       " (datetime.date(2020, 11, 27), datetime.date(2020, 12, 10)),\n",
       " (datetime.date(2020, 11, 28), datetime.date(2020, 12, 11)),\n",
       " (datetime.date(2020, 11, 29), datetime.date(2020, 12, 12)),\n",
       " (datetime.date(2020, 11, 30), datetime.date(2020, 12, 13)),\n",
       " (datetime.date(2020, 12, 1), datetime.date(2020, 12, 14)),\n",
       " (datetime.date(2020, 12, 2), datetime.date(2020, 12, 15)),\n",
       " (datetime.date(2020, 12, 3), datetime.date(2020, 12, 16)),\n",
       " (datetime.date(2020, 12, 4), datetime.date(2020, 12, 17)),\n",
       " (datetime.date(2020, 12, 5), datetime.date(2020, 12, 18)),\n",
       " (datetime.date(2020, 12, 6), datetime.date(2020, 12, 19)),\n",
       " (datetime.date(2020, 12, 7), datetime.date(2020, 12, 20)),\n",
       " (datetime.date(2020, 12, 8), datetime.date(2020, 12, 21)),\n",
       " (datetime.date(2020, 12, 9), datetime.date(2020, 12, 22)),\n",
       " (datetime.date(2020, 12, 10), datetime.date(2020, 12, 23)),\n",
       " (datetime.date(2020, 12, 11), datetime.date(2020, 12, 24)),\n",
       " (datetime.date(2020, 12, 12), datetime.date(2020, 12, 25)),\n",
       " (datetime.date(2020, 12, 13), datetime.date(2020, 12, 26)),\n",
       " (datetime.date(2020, 12, 14), datetime.date(2020, 12, 27)),\n",
       " (datetime.date(2020, 12, 15), datetime.date(2020, 12, 28)),\n",
       " (datetime.date(2020, 12, 16), datetime.date(2020, 12, 29)),\n",
       " (datetime.date(2020, 12, 17), datetime.date(2020, 12, 30)),\n",
       " (datetime.date(2020, 12, 18), datetime.date(2020, 12, 31)),\n",
       " (datetime.date(2020, 12, 19), datetime.date(2021, 1, 1)),\n",
       " (datetime.date(2020, 12, 20), datetime.date(2021, 1, 2)),\n",
       " (datetime.date(2020, 12, 21), datetime.date(2021, 1, 3)),\n",
       " (datetime.date(2020, 12, 22), datetime.date(2021, 1, 4)),\n",
       " (datetime.date(2020, 12, 23), datetime.date(2021, 1, 5)),\n",
       " (datetime.date(2020, 12, 24), datetime.date(2021, 1, 6)),\n",
       " (datetime.date(2020, 12, 25), datetime.date(2021, 1, 7)),\n",
       " (datetime.date(2020, 12, 26), datetime.date(2021, 1, 8)),\n",
       " (datetime.date(2020, 12, 27), datetime.date(2021, 1, 9)),\n",
       " (datetime.date(2020, 12, 28), datetime.date(2021, 1, 10)),\n",
       " (datetime.date(2020, 12, 29), datetime.date(2021, 1, 11)),\n",
       " (datetime.date(2020, 12, 30), datetime.date(2021, 1, 12)),\n",
       " (datetime.date(2020, 12, 31), datetime.date(2021, 1, 13)),\n",
       " (datetime.date(2021, 1, 1), datetime.date(2021, 1, 14)),\n",
       " (datetime.date(2021, 1, 2), datetime.date(2021, 1, 15)),\n",
       " (datetime.date(2021, 1, 3), datetime.date(2021, 1, 16)),\n",
       " (datetime.date(2021, 1, 4), datetime.date(2021, 1, 17)),\n",
       " (datetime.date(2021, 1, 5), datetime.date(2021, 1, 18)),\n",
       " (datetime.date(2021, 1, 6), datetime.date(2021, 1, 19)),\n",
       " (datetime.date(2021, 1, 7), datetime.date(2021, 1, 20)),\n",
       " (datetime.date(2021, 1, 8), datetime.date(2021, 1, 21)),\n",
       " (datetime.date(2021, 1, 9), datetime.date(2021, 1, 22)),\n",
       " (datetime.date(2021, 1, 10), datetime.date(2021, 1, 23)),\n",
       " (datetime.date(2021, 1, 11), datetime.date(2021, 1, 24)),\n",
       " (datetime.date(2021, 1, 12), datetime.date(2021, 1, 25)),\n",
       " (datetime.date(2021, 1, 13), datetime.date(2021, 1, 26)),\n",
       " (datetime.date(2021, 1, 14), datetime.date(2021, 1, 27)),\n",
       " (datetime.date(2021, 1, 15), datetime.date(2021, 1, 28)),\n",
       " (datetime.date(2021, 1, 16), datetime.date(2021, 1, 29)),\n",
       " (datetime.date(2021, 1, 17), datetime.date(2021, 1, 30)),\n",
       " (datetime.date(2021, 1, 18), datetime.date(2021, 1, 31)),\n",
       " (datetime.date(2021, 1, 19), datetime.date(2021, 2, 1)),\n",
       " (datetime.date(2021, 1, 20), datetime.date(2021, 2, 2)),\n",
       " (datetime.date(2021, 1, 21), datetime.date(2021, 2, 3)),\n",
       " (datetime.date(2021, 1, 22), datetime.date(2021, 2, 4)),\n",
       " (datetime.date(2021, 1, 23), datetime.date(2021, 2, 5)),\n",
       " (datetime.date(2021, 1, 24), datetime.date(2021, 2, 6)),\n",
       " (datetime.date(2021, 1, 25), datetime.date(2021, 2, 7)),\n",
       " (datetime.date(2021, 1, 26), datetime.date(2021, 2, 8)),\n",
       " (datetime.date(2021, 1, 27), datetime.date(2021, 2, 9)),\n",
       " (datetime.date(2021, 1, 28), datetime.date(2021, 2, 10)),\n",
       " (datetime.date(2021, 1, 29), datetime.date(2021, 2, 11)),\n",
       " (datetime.date(2021, 1, 30), datetime.date(2021, 2, 12)),\n",
       " (datetime.date(2021, 1, 31), datetime.date(2021, 2, 13)),\n",
       " (datetime.date(2021, 2, 1), datetime.date(2021, 2, 14)),\n",
       " (datetime.date(2021, 2, 2), datetime.date(2021, 2, 15)),\n",
       " (datetime.date(2021, 2, 3), datetime.date(2021, 2, 16)),\n",
       " (datetime.date(2021, 2, 4), datetime.date(2021, 2, 17)),\n",
       " (datetime.date(2021, 2, 5), datetime.date(2021, 2, 18)),\n",
       " (datetime.date(2021, 2, 6), datetime.date(2021, 2, 19)),\n",
       " (datetime.date(2021, 2, 7), datetime.date(2021, 2, 20)),\n",
       " (datetime.date(2021, 2, 8), datetime.date(2021, 2, 21)),\n",
       " (datetime.date(2021, 2, 9), datetime.date(2021, 2, 22)),\n",
       " (datetime.date(2021, 2, 10), datetime.date(2021, 2, 23)),\n",
       " (datetime.date(2021, 2, 11), datetime.date(2021, 2, 24)),\n",
       " (datetime.date(2021, 2, 12), datetime.date(2021, 2, 25)),\n",
       " (datetime.date(2021, 2, 13), datetime.date(2021, 2, 26)),\n",
       " (datetime.date(2021, 2, 14), datetime.date(2021, 2, 27)),\n",
       " (datetime.date(2021, 2, 15), datetime.date(2021, 2, 28)),\n",
       " (datetime.date(2021, 2, 16), datetime.date(2021, 3, 1)),\n",
       " (datetime.date(2021, 2, 17), datetime.date(2021, 3, 2)),\n",
       " (datetime.date(2021, 2, 18), datetime.date(2021, 3, 3)),\n",
       " (datetime.date(2021, 2, 19), datetime.date(2021, 3, 4)),\n",
       " (datetime.date(2021, 2, 20), datetime.date(2021, 3, 5)),\n",
       " (datetime.date(2021, 2, 21), datetime.date(2021, 3, 6)),\n",
       " (datetime.date(2021, 2, 22), datetime.date(2021, 3, 7)),\n",
       " (datetime.date(2021, 2, 23), datetime.date(2021, 3, 8)),\n",
       " (datetime.date(2021, 2, 24), datetime.date(2021, 3, 9)),\n",
       " (datetime.date(2021, 2, 25), datetime.date(2021, 3, 10)),\n",
       " (datetime.date(2021, 2, 26), datetime.date(2021, 3, 11)),\n",
       " (datetime.date(2021, 2, 27), datetime.date(2021, 3, 12)),\n",
       " (datetime.date(2021, 2, 28), datetime.date(2021, 3, 13)),\n",
       " (datetime.date(2021, 3, 1), datetime.date(2021, 3, 14)),\n",
       " (datetime.date(2021, 3, 2), datetime.date(2021, 3, 15)),\n",
       " (datetime.date(2021, 3, 3), datetime.date(2021, 3, 16)),\n",
       " (datetime.date(2021, 3, 4), datetime.date(2021, 3, 17)),\n",
       " (datetime.date(2021, 3, 5), datetime.date(2021, 3, 18)),\n",
       " (datetime.date(2021, 3, 6), datetime.date(2021, 3, 19)),\n",
       " (datetime.date(2021, 3, 7), datetime.date(2021, 3, 20)),\n",
       " (datetime.date(2021, 3, 8), datetime.date(2021, 3, 21)),\n",
       " (datetime.date(2021, 3, 9), datetime.date(2021, 3, 22)),\n",
       " (datetime.date(2021, 3, 10), datetime.date(2021, 3, 23)),\n",
       " (datetime.date(2021, 3, 11), datetime.date(2021, 3, 24)),\n",
       " (datetime.date(2021, 3, 12), datetime.date(2021, 3, 25)),\n",
       " (datetime.date(2021, 3, 13), datetime.date(2021, 3, 26)),\n",
       " (datetime.date(2021, 3, 14), datetime.date(2021, 3, 27)),\n",
       " (datetime.date(2021, 3, 15), datetime.date(2021, 3, 28)),\n",
       " (datetime.date(2021, 3, 16), datetime.date(2021, 3, 29)),\n",
       " (datetime.date(2021, 3, 17), datetime.date(2021, 3, 30)),\n",
       " (datetime.date(2021, 3, 18), datetime.date(2021, 3, 31)),\n",
       " (datetime.date(2021, 3, 19), datetime.date(2021, 4, 1)),\n",
       " (datetime.date(2021, 3, 20), datetime.date(2021, 4, 2)),\n",
       " (datetime.date(2021, 3, 21), datetime.date(2021, 4, 3)),\n",
       " (datetime.date(2021, 3, 22), datetime.date(2021, 4, 4)),\n",
       " (datetime.date(2021, 3, 23), datetime.date(2021, 4, 5)),\n",
       " (datetime.date(2021, 3, 24), datetime.date(2021, 4, 6)),\n",
       " (datetime.date(2021, 3, 25), datetime.date(2021, 4, 7)),\n",
       " (datetime.date(2021, 3, 26), datetime.date(2021, 4, 8)),\n",
       " (datetime.date(2021, 3, 27), datetime.date(2021, 4, 9)),\n",
       " (datetime.date(2021, 3, 28), datetime.date(2021, 4, 10)),\n",
       " (datetime.date(2021, 3, 29), datetime.date(2021, 4, 11)),\n",
       " (datetime.date(2021, 3, 30), datetime.date(2021, 4, 12)),\n",
       " (datetime.date(2021, 3, 31), datetime.date(2021, 4, 13))]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "411"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "411"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CONSTRUCTING INPUT/OUTPUT INTERVALS\n",
    "\n",
    "daysIn = 14 # last n days of input\n",
    "daysOut = 7 # next n days of output\n",
    "\n",
    "listOfInputIntervals = []\n",
    "listOfOutputIntervals = []\n",
    "for e in pd.date_range(start=pd.Timestamp(start),end=pd.Timestamp(end), freq='D'):\n",
    "    if (e+timedelta(days = daysIn+daysOut-1) <= pd.Timestamp(end).date()):\n",
    "        listOfInputIntervals.append((e.date(),(e+timedelta(days = (daysIn-1))).date()))\n",
    "        listOfOutputIntervals.append(((e+timedelta(days = daysIn)).date(),(e+timedelta(days = daysIn+daysOut-1)).date()))\n",
    "    \n",
    "display(listOfInputIntervals)\n",
    "display(len(listOfInputIntervals))\n",
    "display(len(listOfOutputIntervals))\n",
    "#display(listOfOutputIntervals)\n",
    "\n",
    "\n",
    "dailyFeaturesDict = {} \n",
    "\n",
    "# average features\n",
    "# features which will be averaged over the whole input interval\n",
    "averageFeatures = ['Cases inz_entries 0 - 9','Cases inz_entries 10 - 19','Cases inz_entries 20 - 29',\n",
    "                  'Cases inz_entries 30 - 39','Cases inz_entries 40 - 49','Cases inz_entries 50 - 59',\n",
    "                  'Cases inz_entries 60 - 69','Cases inz_entries 70 - 79','Cases inz_entries 80+',\n",
    "                  'Cases inzsumTotal 0 - 9','Cases inzsumTotal 10 - 19','Cases inzsumTotal 20 - 29',\n",
    "                  'Cases inzsumTotal 30 - 39','Cases inzsumTotal 40 - 49','Cases inzsumTotal 50 - 59',\n",
    "                  'Cases inzsumTotal 60 - 69','Cases inzsumTotal 70 - 79','Cases inzsumTotal 80+',\n",
    "                  'Death inz_entries 0 - 9','Death inz_entries 10 - 19','Death inz_entries 20 - 29',\n",
    "                  'Death inz_entries 30 - 39','Death inz_entries 40 - 49','Death inz_entries 50 - 59',\n",
    "                  'Death inz_entries 60 - 69','Death inz_entries 70 - 79','Death inz_entries 80+',\n",
    "                  'Death inzsumTotal 0 - 9','Death inzsumTotal 10 - 19','Death inzsumTotal 20 - 29',\n",
    "                  'Death inzsumTotal 30 - 39','Death inzsumTotal 40 - 49','Death inzsumTotal 50 - 59',\n",
    "                  'Death inzsumTotal 60 - 69','Death inzsumTotal 70 - 79','Death inzsumTotal 80+',\n",
    "                  'Hosp inz_entries 0 - 9','Hosp inz_entries 10 - 19','Hosp inz_entries 20 - 29',\n",
    "                  'Hosp inz_entries 30 - 39','Hosp inz_entries 40 - 49','Hosp inz_entries 50 - 59',\n",
    "                  'Hosp inz_entries 60 - 69','Hosp inz_entries 70 - 79','Hosp inz_entries 80+',\n",
    "                  'Hosp inzsumTotal 0 - 9','Hosp inzsumTotal 10 - 19','Hosp inzsumTotal 20 - 29',\n",
    "                  'Hosp inzsumTotal 30 - 39','Hosp inzsumTotal 40 - 49','Hosp inzsumTotal 50 - 59',\n",
    "                  'Hosp inzsumTotal 60 - 69','Hosp inzsumTotal 70 - 79','Hosp inzsumTotal 80+',\n",
    "                  'Cases inz_entries female','Cases inz_entries male','Cases inzsumTotal female',\n",
    "                  'Cases inzsumTotal male','Death inz_entries female','Death inz_entries male',\n",
    "                  'Death inzsumTotal female','Death inzsumTotal male','Hosp inz_entries female',\n",
    "                  'Hosp inz_entries male','Hosp inzsumTotal female','Hosp inzsumTotal male', \n",
    "                  'VaccDosesAdministered per100PersonsTotal',\n",
    "                  'FullyVaccPersons per100PersonsTotal',\n",
    "                  'anteil_pos',\n",
    "                  'variant_error',\n",
    "                  'case_inzsumTotal','hosp_inzsumTotal','death_inzsumTotal','test_inzsumTotal','case_inz_entries',\n",
    "                  'hosp_inz_entries','death_inz_entries','test_inz_entries','testPositvity',\n",
    "                  'median_R_mean','R_error',\n",
    "                  'meanNeighborIncidence','maxNeighborIncidence',\n",
    "                  'kofStrigency',\n",
    "                  'Borders','Events','Gatherings/private events','Demonstrations',\n",
    "                  'Primary (includes kindergarten) and lower secondary school','Upper secondary school, vocational schools and higher education',\n",
    "                  'Universities and other educational establishments','Mountain railways','Homeworking','Restaurants',\n",
    "                  'Discos/Nightclubs','Shops/Markets','Penalties','Cultural, entertainment and recreational facilities',\n",
    "                  'Sport/Wellness facilities','Sport activities','Religious services','Singing allowed','maskMandatories',\n",
    "                  'ICU_AllPatients_inz','ICU_Covid19Patients_inz','ICU_Capacity_inz','Total_AllPatients_inz',\n",
    "                  'Total_Covid19Patients_inz','Total_Capacity_inz','ICU_NonCovid19Patients_inz','ICU_FreeCapacity_inz',\n",
    "                  'Total_NonCovid19Patients_inz','Total_FreeCapacity_inz',\n",
    "                  'isHoliday','retail_and_recreation_percent_change_from_baseline',\n",
    "                  'grocery_and_pharmacy_percent_change_from_baseline',\n",
    "                  'parks_percent_change_from_baseline',\n",
    "                  'transit_stations_percent_change_from_baseline',\n",
    "                  'workplaces_percent_change_from_baseline',\n",
    "                  'residential_percent_change_from_baseline',\n",
    "                  'intervistaMob',\n",
    "                  'temp_min','temp_max','clouds','precipitation']  \n",
    "avgColNames = [f + \"_\" + str(daysIn)+\"_dayMean \" for f in averageFeatures]\n",
    "    \n",
    "# direct features\n",
    "# features which will be direct input for every day of the input interval\n",
    "# attention: this can potentially increase the number of input features significantly\n",
    "# added features are len(directFea)*daysIn\n",
    "# only add features for which have a large variance from one day to another day\n",
    "directFeatures = ['retail_and_recreation_percent_change_from_baseline',\n",
    "                          'grocery_and_pharmacy_percent_change_from_baseline',\n",
    "                          'parks_percent_change_from_baseline',\n",
    "                          'transit_stations_percent_change_from_baseline',\n",
    "                          'workplaces_percent_change_from_baseline',\n",
    "                          'residential_percent_change_from_baseline',\n",
    "                          'intervistaMob',\n",
    "                          'temp_min','temp_max','clouds','precipitation']\n",
    "directColNames = [f+'_day_'+str(d) for f in directFeatures for d in range(0,daysIn)]\n",
    "\n",
    "# future features\n",
    "futureFeatures = ['temp_min','temp_max','clouds','precipitation']\n",
    "futureColNames = [f + '_future_day_'+str(d) for f in futureFeatures for d in range(0,daysOut)]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aquatic-ranking",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AG'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'AI'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'AR'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'BE'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'BL'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'BS'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'FR'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'GE'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'GL'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'GR'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'JU'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'LU'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'NE'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'NW'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'OW'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'SG'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'SH'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'SO'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'SZ'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'TG'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'TI'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'UR'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'VD'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'VS'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'ZG'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'ZH'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'----------End of evaluating (268.1441185474396)----------'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start = time.time()\n",
    "# CONSTRUCTING ACTUAL INPUTS\n",
    "for cantonId in cantonKeys:\n",
    "    dailyFeaturesDict[cantonId] = pd.read_csv(\"data/dailyFeatures/\"+cantonId+\".csv\").set_index('date')\n",
    "    dailyFeaturesDict[cantonId].index = pd.to_datetime(dailyFeaturesDict[cantonId].index)\n",
    "    \n",
    "    display(cantonId)\n",
    "    features = pd.DataFrame()\n",
    "       \n",
    "    # construction of input features\n",
    "    tupleCounter = 0\n",
    "    for t in listOfInputIntervals:\n",
    "        timeFrame = dailyFeaturesDict[cantonId][t[0]:t[1]]\n",
    "        #display([col for col in timeFrame.columns])\n",
    "        #display(timeFrame)\n",
    "        ot = listOfOutputIntervals[tupleCounter]\n",
    "        tupleCounter = tupleCounter + 1\n",
    "        timeFrameOutput = dailyFeaturesDict[cantonId][ot[0]:ot[1]]\n",
    "        \n",
    "        # average features\n",
    "        featureRow = timeFrame[averageFeatures].mean().to_frame().transpose()\n",
    "        featureRow.columns = avgColNames\n",
    "        \n",
    "\n",
    "        # direct features\n",
    "        #display(np.shape(timeFrame[directFeatures].values.flatten(order='F')))\n",
    "        featureRow[directColNames] = timeFrame[directFeatures].values.flatten(order='F')\n",
    "\n",
    "        # future features\n",
    "        featureRow[futureColNames] = timeFrameOutput[futureFeatures].values.flatten(order='F')\n",
    "        \n",
    "        features = features.append(featureRow, ignore_index=True)   \n",
    "    \n",
    "    \n",
    "    # static cantonal features\n",
    "    staticCantonal = pd.read_excel(\"static_data/staticCantonalData.xlsx\").set_index('canton').transpose()\n",
    "\n",
    "    # households\n",
    "    households = ['2PersonHouseholds','3PersonHouseholds', '4PersonHouseholds', \n",
    "              '5PersonHouseholds','6+PersonHouseholds']\n",
    "    for h in households:\n",
    "        features[[h+\"_perc\"]] = staticCantonal.loc[[cantonId]][h][0]/staticCantonal.loc[[cantonId]]['totalHousholds'][0]\n",
    "    features[['averageHousehold']] = staticCantonal.loc[[cantonId]]['residents'][0]/staticCantonal.loc[[cantonId]]['totalHousholds'][0]\n",
    "    # add static features\n",
    "    staticFeatures = ['percentage 65 years or over','urbanPopulationPercent','homeownershipPercent', \n",
    "                      'livingSpaceInm2','carsPer1000inhabitants', 'publicTransportationPercent',\n",
    "                      'privateMotorisedTransportPercent','DoctorsPer100Kinhabitants','residentsPerKm2']\n",
    "    for f in staticFeatures:\n",
    "        features[[f]] = staticCantonal.loc[[cantonId]][f][0]\n",
    "    # construct settlement area feature\n",
    "    residents = staticCantonal.loc[[cantonId]]['residents'][0]\n",
    "    settlementArea = staticCantonal.loc[[cantonId]]['areaInKm2'][0]*(staticCantonal.loc[[cantonId]]['settlementAreaPercent'][0]/100)\n",
    "    features[['residentsPerKm2SettlementArea']] = residents/settlementArea\n",
    "    \n",
    "    # add inputFrom and inputTo\n",
    "    #features[['inputFrom']] = [e[0] for e in listOfInputIntervals]\n",
    "    #features[['inputTo']] = [e[1] for e in listOfInputIntervals]\n",
    "    \n",
    "    if not os.path.exists('data/features'):\n",
    "        os.makedirs('data/features')\n",
    "    features.to_csv('data/features/'+cantonId+'.csv', index=False)\n",
    "\n",
    "display(\"----------End of evaluating (%s)----------\" % (time.time() - start))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "prime-institution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSTRUCTING ACTUAL OUTPUTS\n",
    "for cantonId in cantonKeys:\n",
    "    outputs = pd.DataFrame()\n",
    "    for t in listOfOutputIntervals:\n",
    "        outputCols = ['hosp_inz_entries',\n",
    "                      'death_inz_entries',\n",
    "                      'testPositvity',\n",
    "                      'workplaces_percent_change_from_baseline',\n",
    "                      'transit_stations_percent_change_from_baseline']\n",
    "        outputRow = dailyFeaturesDict[cantonId][t[0]:t[1]][outputCols].mean().to_frame().transpose()   \n",
    "        outputs = outputs.append(outputRow, ignore_index=True)\n",
    "    \n",
    "    # add outputFrom and outputTo\n",
    "    #outputs[['outputsFrom']] = [e[0] for e in listOfOutputIntervals]\n",
    "    #outputs[['outputsTo']] = [e[1] for e in listOfOutputIntervals]\n",
    "    \n",
    "    if not os.path.exists('data/outputs'):\n",
    "        os.makedirs('data/outputs')\n",
    "    outputs.to_csv('data/outputs/'+cantonId+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "considerable-cutting",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Complete: 10686'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Training set: 8870(0.8300580198390417)'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Test set 1: 702(0.06569343065693431)'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Test set 2: 384(0.035934868051656375)'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Validation set 1: 696(0.06513194834362718)'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Validation set 234(0.003181733108740408)'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# categorizes data into test sets, validation sets and training set\n",
    "import random\n",
    "intervalSize = 14 #two week intervals for test set 2\n",
    "\n",
    "dfList = []\n",
    "for cantonId in cantonKeys:\n",
    "    inputs = pd.read_csv('data/features/'+cantonId+'.csv')\n",
    "    outputs = pd.read_csv('data/outputs/'+cantonId+'.csv')\n",
    "    length = inputs.shape[0]\n",
    "    \n",
    "    df = pd.concat([inputs,outputs], axis=1)\n",
    "    \n",
    "    # mark all as train set (default)\n",
    "    df[['category']] = 'train'\n",
    "    \n",
    "    # mark test set 1\n",
    "    df.loc[(length-28):,['category']] = 'test 1'\n",
    "    \n",
    "    # mark validation set 1\n",
    "    df.loc[(length-56):(length-28),['category']] = 'validation 1'\n",
    "    \n",
    "    dfList.append(df)\n",
    " \n",
    "# mark test set 2\n",
    "random.shuffle(dfList)\n",
    "length = dfList[0].shape[0]\n",
    "counter = 0\n",
    "cantonCounter = 0\n",
    "while (counter+intervalSize-1)  < length-28:\n",
    "    dfList[cantonCounter % 26].loc[counter:counter+intervalSize-1,['category']] = 'test 2'\n",
    "    counter = counter + intervalSize\n",
    "    cantonCounter = cantonCounter + 1\n",
    "dfList[cantonCounter % 26].loc[counter:length-28,['category']] = 'test 2'\n",
    "\n",
    "\n",
    "# mark validation set 2\n",
    "# first check for conflicts between validation set 2 and test set 2\n",
    "counter = 0\n",
    "cantonCounter = 0\n",
    "found = False\n",
    "while not found:\n",
    "    random.shuffle(dfList)\n",
    "    found = True\n",
    "    while (counter+intervalSize-1)  < length-56:\n",
    "        if (dfList[cantonCounter % 26].loc[counter:counter+intervalSize-1,['category']] == 'test 2').any()[0]:\n",
    "            found = False\n",
    "            break\n",
    "        counter = counter + intervalSize\n",
    "        cantonCounter = cantonCounter + 1\n",
    "    if (dfList[cantonCounter % 26].loc[counter:length-57,['category']] == 'test 2').any()[0]:\n",
    "        found = False\n",
    "\n",
    "# at this point we know our permutation does not have a conflict\n",
    "while (counter+intervalSize-1)  < length-56:\n",
    "    dfList[cantonCounter % 26].loc[counter:counter+intervalSize-1,['category']] = 'validation 2'\n",
    "    counter = counter + intervalSize\n",
    "    cantonCounter = cantonCounter + 1  \n",
    "dfList[cantonCounter % 26].loc[counter:length-28,['category']] = 'validation 2'\n",
    "\n",
    "\n",
    "data = pd.concat(dfList, axis=0, ignore_index=True)\n",
    "completeNumber = data.shape[0]\n",
    "trainNumber = data[data['category']=='train'].shape[0]\n",
    "test1Number = data[data['category']=='test 1'].shape[0]\n",
    "test2Number = data[data['category']=='test 2'].shape[0]\n",
    "valid1Number = data[data['category']=='validation 1'].shape[0]\n",
    "valid2Number = data[data['category']=='validation 2'].shape[0]\n",
    "display(\"Complete: \"+ str(completeNumber) )\n",
    "display(\"Training set: \"+ str(trainNumber)+ \"(\"+str(trainNumber/completeNumber)+\")\")\n",
    "display(\"Test set 1: \"+ str(test1Number)+ \"(\"+str(test1Number/completeNumber)+\")\")\n",
    "display(\"Test set 2: \"+ str(test2Number)+ \"(\"+str(test2Number/completeNumber)+\")\")\n",
    "display(\"Validation set 1: \"+ str(valid1Number)+ \"(\"+str(valid1Number/completeNumber)+\")\")\n",
    "display(\"Validation set 2\"+ str(valid2Number)+ \"(\"+str(valid2Number/completeNumber)+\")\")\n",
    "\n",
    "\n",
    "data.to_csv('completedata.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fresh-engineer",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sensitive-berry",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# generate input for newest prediction\n",
    "t_end = yesterday\n",
    "t_start = yesterday-timedelta(days = daysIn+1)\n",
    "dailyFeaturesDict = {}\n",
    "\n",
    "\n",
    "for cantonId in cantonKeys:\n",
    "    dailyFeaturesDict[cantonId] = pd.read_csv(\"data/dailyFeatures/\"+cantonId+\".csv\").set_index('date')\n",
    "    dailyFeaturesDict[cantonId].index = pd.to_datetime(dailyFeaturesDict[cantonId].index)\n",
    "    \n",
    "    display(cantonId)\n",
    "    newest = pd.DataFrame()\n",
    "    \n",
    "    timeFrame = dailyFeaturesDict[cantonId][t_start:t_end]\n",
    "    #display(timeFrame)\n",
    "\n",
    "\n",
    "    # average features \n",
    "    newestRow = timeFrame[averageFeatures].mean().to_frame().transpose()\n",
    "    newestRow.columns = avgColNames\n",
    "\n",
    "\n",
    "    # direct features \n",
    "    #display(np.shape(timeFrame[directFeatures].values.flatten(order='F')))\n",
    "    newestRow[directColNames] = timeFrame[directFeatures].values.flatten(order='F')\n",
    "    \n",
    "    # future features\n",
    "    futureFeatures = ['temp_min','temp_max','clouds','precipitation']\n",
    "    # instead of the actual future weather we just take the weather forecast\n",
    "    forecast = pd.read_csv(\"data/weatherforecast/\"+cantonId+\".csv\")\n",
    "    \n",
    "    if ('rain' in forecast.columns) and ('snow' in forecast.columns):\n",
    "        forecast[['precipitation']] = forecast[['rain','snow']].sum(axis=1)\n",
    "    elif ('rain' in forecast.columns):\n",
    "        forecast[['precipitation']] = forecast[['rain']]\n",
    "    elif ('snow' in forecast.columns):\n",
    "        forecast[['precipitation']] = forecast[['snow']]\n",
    "    else:\n",
    "        forecast[['precipitation']] = 0\n",
    "    \n",
    "    forecast.rename(columns = {'temp.min':'temp_min','temp.max':'temp_max'}, inplace = True)\n",
    "    #display(forecast[futureFeatures][0:daysOut])\n",
    "    for ff in futureFeatures:\n",
    "        futureFe = forecast[futureFeatures][0:daysOut][[ff]].transpose()\n",
    "        futureFe.columns = [ff+'_future_day_'+str(d) for d in range(0,daysOut)]\n",
    "        futureFe = futureFe.reset_index().drop(['index'], axis=1)\n",
    "        newestRow = pd.concat([newestRow,futureFe], axis=1)\n",
    "    newest = newest.append(newestRow, ignore_index=True)\n",
    "    \n",
    "    \n",
    "    # static cantonal features\n",
    "    staticCantonal = pd.read_excel(\"static_data/staticCantonalData.xlsx\").set_index('canton').transpose()\n",
    "\n",
    "    # households\n",
    "    households = ['2PersonHouseholds','3PersonHouseholds', '4PersonHouseholds', \n",
    "              '5PersonHouseholds','6+PersonHouseholds']\n",
    "    for h in households:\n",
    "        newest[[h+\"_perc\"]] = staticCantonal.loc[[cantonId]][h][0]/staticCantonal.loc[[cantonId]]['totalHousholds'][0]\n",
    "    newest[['averageHousehold']] = staticCantonal.loc[[cantonId]]['residents'][0]/staticCantonal.loc[[cantonId]]['totalHousholds'][0]\n",
    "    # add static features\n",
    "    staticFeatures = ['percentage 65 years or over','urbanPopulationPercent','homeownershipPercent', \n",
    "                      'livingSpaceInm2','carsPer1000inhabitants', 'publicTransportationPercent',\n",
    "                      'privateMotorisedTransportPercent','DoctorsPer100Kinhabitants','residentsPerKm2']\n",
    "    for f in staticFeatures:\n",
    "        newest[[f]] = staticCantonal.loc[[cantonId]][f][0]\n",
    "    # construct settlement area feature\n",
    "    residents = staticCantonal.loc[[cantonId]]['residents'][0]\n",
    "    settlementArea = staticCantonal.loc[[cantonId]]['areaInKm2'][0]*(staticCantonal.loc[[cantonId]]['settlementAreaPercent'][0]/100)\n",
    "    newest[['residentsPerKm2SettlementArea']] = residents/settlementArea\n",
    "    \n",
    "    \n",
    "    if not os.path.exists('data/newest'):\n",
    "        os.makedirs('data/newest')\n",
    "    newest.to_csv('data/newest/'+cantonId+'.csv', index=False)\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behind-place",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spoken-coating",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outstanding-gardening",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "major-adoption",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternate-marking",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
