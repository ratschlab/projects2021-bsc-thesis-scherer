{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "from datetime import date, timedelta, timezone\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.options.display.max_rows = None\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cantonKeys = ['AG','AI','AR', 'BE', 'BL', 'BS', 'FR', 'GE', 'GL', 'GR', 'JU', 'LU', 'NE', 'NW', 'OW', 'SG', 'SH', 'SO', 'SZ', 'TG', 'TI', 'UR', 'VD', 'VS', 'ZG','ZH']\n",
    "googleMobDict = dict(zip(cantonKeys,[\"Aargau\",\"Appenzell Innerrhoden\",\"Appenzell Ausserrhoden\",\"Canton of Bern\",\"Basel-Landschaft\",\"Basel City\",\n",
    "                                                    \"Fribourg\",\"Geneva\",\"Glarus\",\"Grisons\",\"Jura\",\"Lucerne\",\"Neuch√¢tel\",\"Nidwalden\",\"Obwalden\",\"St. Gallen\",\n",
    "                                                    \"Schaffhausen\",\"Solothurn\",\"Schwyz\",\"Thurgau\",\"Ticino\",\"Uri\",\"Vaud\",\"Valais\",\"Canton of Zug\",\"Zurich\"]))\n",
    "def getDays(year, offset):\n",
    "   d = date(year, 1, 1)                    \n",
    "   d += timedelta(days = offset - d.weekday())  \n",
    "   while d.year == year:\n",
    "      yield d\n",
    "      d += timedelta(days = 7)\n",
    "\n",
    "listOfMondays = []\n",
    "for year in [2020,2021]:\n",
    "    for day in getDays(year, 7):\n",
    "       listOfMondays.append(day)\n",
    "    \n",
    "def addZero(x):\n",
    "    if len(x)==1:\n",
    "        return \"0\"+x\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "yearWeek = [str(x.isocalendar()[0])+addZero(str(x.isocalendar()[1])) for x in listOfMondays]\n",
    "\n",
    "mondaysByWeekNr = dict(zip(yearWeek,listOfMondays))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for cantonId in cantonKeys:\n",
    "    df = pd.DataFrame(index=pd.date_range(start=datetime.datetime(2020, 1, 1), end=datetime.datetime(2021, 12, 31)))\n",
    "    \n",
    "    \n",
    "    # weekly age classified FOPH data\n",
    "    for category in ['Cases','Death','Hosp']: \n",
    "        age = pd.read_csv(\"data/FOPH/data/COVID19\"+category+\"_geoRegion_AKL10_w.csv\")\n",
    "        age[['datum']] = age[['datum']].applymap(lambda x: mondaysByWeekNr[str(x)])\n",
    "        age = age.loc[age['geoRegion']==cantonId]\n",
    "        age = age[['datum','altersklasse_covid19','entries','prct','inz_entries','inzsumTotal']]\n",
    "        age = age.pivot(index=\"datum\", columns=\"altersklasse_covid19\")\n",
    "        age.columns = [category+\" \"+' '.join(col) for col in age.columns.values]\n",
    "        age = age.drop([category+' entries Unbekannt',category+' prct Unbekannt', category+' inz_entries Unbekannt', category+' inzsumTotal Unbekannt'], axis=1)\n",
    "        age = age.drop([category+' prct 0 - 9', category+' prct 10 - 19', category+' prct 20 - 29', \n",
    "                        category+' prct 30 - 39', category+' prct 40 - 49', category+' prct 50 - 59', \n",
    "                        category+' prct 60 - 69', category+' prct 70 - 79', category+' prct 80+'],axis=1)\n",
    "        df = df.join(age)\n",
    "    \n",
    "    # weekly gender classified FOPH data\n",
    "    for category in ['Cases','Death','Hosp']: \n",
    "        gender = pd.read_csv(\"data/FOPH/data/COVID19\"+category+\"_geoRegion_sex_w.csv\")\n",
    "        gender[['datum']] = gender[['datum']].applymap(lambda x: mondaysByWeekNr[str(x)])\n",
    "        gender = gender.loc[gender['geoRegion']==cantonId]\n",
    "        gender = gender[['datum','sex','entries','prct','inz_entries','inzsumTotal']]\n",
    "        gender = gender.pivot(index=\"datum\", columns=\"sex\")\n",
    "        gender.columns = [category+\" \"+' '.join(col) for col in gender.columns.values]\n",
    "        gender = gender.drop([category+' entries unknown',category+' inz_entries unknown', category+' inzsumTotal unknown', category+' prct unknown', category+' prct female', category+' prct male'],axis=1)\n",
    "        df = df.join(gender)\n",
    "    \n",
    "    # weekly vaccination age classified FOPH data \n",
    "    for category in ['VaccDosesAdministered','FullyVaccPersons']: \n",
    "        vacc = pd.read_csv(\"data/FOPH/data/COVID19\"+category+\".csv\")\n",
    "        vacc = vacc.loc[vacc['geoRegion']==cantonId]\n",
    "        vacc = vacc[['date','sumTotal','per100PersonsTotal']]\n",
    "        vacc = vacc.set_index('date')\n",
    "        vacc.columns = [category+\" \"+col for col in vacc.columns]\n",
    "        vacc.index = pd.to_datetime(vacc.index) \n",
    "        df = df.join(vacc)\n",
    "    \n",
    "    # daily hospital capacity FOPH data\n",
    "    capacity = pd.read_csv(\"data/FOPH/data/COVID19HospCapacity_geoRegion.csv\")\n",
    "    capacity = capacity.loc[capacity['geoRegion']==cantonId]\n",
    "    capacity = capacity.set_index('date').sort_index()\n",
    "    capacity = capacity[['ICU_AllPatients','ICU_Covid19Patients','ICU_Capacity','Total_AllPatients',\n",
    "                         'Total_Covid19Patients','Total_Capacity','ICU_NonCovid19Patients','ICU_FreeCapacity',\n",
    "                         'Total_NonCovid19Patients','Total_FreeCapacity','type_variant']]\n",
    "    capacity = capacity.drop_duplicates()\n",
    "    capacity = capacity.loc[capacity['type_variant']=='fp7d']\n",
    "    capacity = capacity.drop(['type_variant'], axis=1)\n",
    "    capacity.index = pd.to_datetime(capacity.index) \n",
    "    \n",
    "    df = df.join(capacity)\n",
    "    \n",
    "    # percentage of virus variants\n",
    "    variants = pd.read_csv(\"data/FOPH/data/COVID19Variants.csv\")\n",
    "    variants = variants.loc[(variants['geoRegion']==cantonId) & (variants['variant_type']=='voc_digitally_reported') & (variants['data_quality']=='sufficient') ] #(variants['variant_type']=='voc_known')\n",
    "    variants = variants.set_index('date').sort_index()\n",
    "    variants = variants[['lower_ci_day','upper_ci_day','anteil_pos']]\n",
    "    variants.index = pd.to_datetime(variants.index) \n",
    "    df = df.join(variants)\n",
    "    \n",
    "    \n",
    "    # daily basis data\n",
    "    \n",
    "    # attach daily positive cases\n",
    "    caseDf = pd.read_csv(\"data/FOPH/data/COVID19Cases_geoRegion.csv\")\n",
    "    caseDf = caseDf.loc[caseDf[\"geoRegion\"]==cantonId]\n",
    "    caseDf = caseDf.set_index('datum')\n",
    "    interestedCols = ['entries','inz_entries','inzsumTotal']\n",
    "    caseDf = caseDf[interestedCols]\n",
    "    caseDf.columns = [\"case_\"+e for e in interestedCols]\n",
    "    caseDf.index = pd.to_datetime(caseDf.index) \n",
    "    df = df.join(caseDf)\n",
    "    \n",
    "    # attach daily hospital cases\n",
    "    hospDf = pd.read_csv(\"data/FOPH/data/COVID19Hosp_geoRegion.csv\")\n",
    "    hospDf = hospDf.loc[hospDf[\"geoRegion\"]==cantonId]\n",
    "    hospDf = hospDf.set_index('datum')\n",
    "    interestedCols = ['entries','inz_entries','inzsumTotal']\n",
    "    hospDf = hospDf[interestedCols]\n",
    "    hospDf.columns = [\"hosp_\"+e for e in interestedCols]\n",
    "    hospDf.index = pd.to_datetime(hospDf.index) \n",
    "    df = df.join(hospDf)\n",
    "    \n",
    "    # attach daily death cases\n",
    "    deathDf = pd.read_csv(\"data/FOPH/data/COVID19Death_geoRegion.csv\")\n",
    "    deathDf = deathDf.loc[deathDf[\"geoRegion\"]==cantonId]\n",
    "    deathDf = deathDf.set_index('datum')\n",
    "    interestedCols = ['entries','inz_entries','inzsumTotal']\n",
    "    deathDf = deathDf[interestedCols]\n",
    "    deathDf.columns = [\"death_\"+e for e in interestedCols]\n",
    "    deathDf.index = pd.to_datetime(deathDf.index)\n",
    "    df = df.join(deathDf)\n",
    "    \n",
    "    \n",
    "    # attach daily test\n",
    "    testDf = pd.read_csv(\"data/FOPH/data/COVID19Test_geoRegion_all.csv\")\n",
    "    testDf = testDf.loc[testDf[\"geoRegion\"]==cantonId]\n",
    "    testDf = testDf.set_index('datum')\n",
    "    pop = testDf['pop'][0]\n",
    "    interestedCols = ['entries','inz_entries','inzsumTotal']\n",
    "    testDf = testDf[interestedCols]\n",
    "    testDf.columns = [\"test_\"+e for e in interestedCols]\n",
    "    testDf.index = pd.to_datetime(testDf.index)\n",
    "    df = df.join(testDf)\n",
    "    # compute rest of test entries\n",
    "    totalTestsInSwitzerland = pd.DataFrame(index=pd.date_range(start=datetime.datetime(2020, 2, 15), end=datetime.datetime(2020, 5, 22)))\n",
    "    temp = pd.read_csv(\"static_data/historicTests.csv\")\n",
    "    temp = temp.set_index('Datum')   \n",
    "    temp.index = pd.to_datetime(temp.index)\n",
    "    totalTestsInSwitzerland = totalTestsInSwitzerland.join(temp)  \n",
    "    totalTestsInSwitzerland = totalTestsInSwitzerland[['Tests pro Tag']]\n",
    "    totalTestsInSwitzerland.fillna(method='bfill', inplace=True)\n",
    "    testsByCanton = pd.read_csv(\"data/FOPH/data/COVID19Test_geoRegion_all.csv\")\n",
    "    testsByCanton = testsByCanton.set_index('datum')\n",
    "    testsByCanton.index = pd.to_datetime(testsByCanton.index)\n",
    "    sumSwitzerland = testsByCanton.loc[testsByCanton[\"geoRegion\"]=='CH'][['entries']]\n",
    "    sumCanton = testsByCanton.loc[testsByCanton[\"geoRegion\"]==cantonId][['entries']]\n",
    "    cantonalTestFraction = sumCanton['2020-05-23':'2020-06-05'].sum(axis=0).values[0]/sumSwitzerland['2020-05-23':'2020-06-05'].sum(axis=0).values[0]\n",
    "    #multiply this with cantonal test quotient\n",
    "    computedMissingEntries = totalTestsInSwitzerland['2020-02-15':'2020-05-22']*cantonalTestFraction \n",
    "    computedMissingEntries.rename(columns = {\"Tests pro Tag\":'test_entries'}, inplace = True)\n",
    "    df[['test_entries']] = df[['test_entries']].fillna(computedMissingEntries[['test_entries']])\n",
    "    # compute rest of test incidence\n",
    "    missingTestIncidents = 100000*(df[['test_entries']]/pop)\n",
    "    missingTestIncidents.rename(columns = {\"test_entries\":'test_inz_entries'}, inplace = True)\n",
    "    df[['test_inz_entries']] = df[['test_inz_entries']].fillna(missingTestIncidents)   \n",
    "            \n",
    "    \n",
    "    # attach daily R-values\n",
    "    rvalueDf = pd.read_csv(\"data/FOPH/data/COVID19Re_geoRegion.csv\")\n",
    "    rvalueDf = rvalueDf.loc[rvalueDf[\"geoRegion\"]==cantonId]\n",
    "    rvalueDf = rvalueDf.set_index('date')\n",
    "    interestedCols = ['median_R_mean','median_R_highHPD','median_R_lowHPD']\n",
    "    rvalueDf = rvalueDf[interestedCols]\n",
    "    rvalueDf.index = pd.to_datetime(rvalueDf.index)\n",
    "    df = df.join(rvalueDf)\n",
    "    \n",
    "    # attach google mobility data\n",
    "    mobDf = pd.read_csv(\"data/GoogleMobility/2020_CH_Region_Mobility_Report.csv\")\n",
    "    mobDf = mobDf.loc[mobDf[\"sub_region_1\"]==googleMobDict[cantonId]]\n",
    "    mobDf = mobDf.set_index('date')\n",
    "    interestedCols = ['retail_and_recreation_percent_change_from_baseline',\n",
    "                  'grocery_and_pharmacy_percent_change_from_baseline',\n",
    "                  'parks_percent_change_from_baseline',\n",
    "                  'transit_stations_percent_change_from_baseline',\n",
    "                  'workplaces_percent_change_from_baseline',\n",
    "                  'residential_percent_change_from_baseline'\n",
    "                 ]\n",
    "    mobDf = mobDf[interestedCols]\n",
    "    mobDf.index = pd.to_datetime(mobDf.index)\n",
    "    df = df.join(mobDf)\n",
    "    \n",
    "    # attach KOF strigency index\n",
    "    kofDf = pd.read_csv(\"data/KOF/KOFStrigencyIndex.csv\")\n",
    "    kofDf = kofDf.set_index('date')\n",
    "    kofDf = kofDf[[\"ch.kof.stringency.\"+cantonId.lower()+\".stringency_plus\"]]\n",
    "    kofDf.rename(columns = {\"ch.kof.stringency.\"+cantonId.lower()+\".stringency_plus\":'kofStrigency'}, inplace = True)\n",
    "    kofDf.index = pd.to_datetime(kofDf.index)\n",
    "    df = df.join(kofDf)\n",
    "    \n",
    "    # attach all measures\n",
    "    measuresDf = pd.read_csv(\"data/measures/\"+cantonId+\".csv\")\n",
    "    measuresDf = measuresDf.set_index('Time')\n",
    "    measuresDf.index = pd.to_datetime(measuresDf.index)\n",
    "    df = df.join(measuresDf)\n",
    "    \n",
    "    \n",
    "    # attach holidays & vacations \n",
    "    holy = pd.read_csv(\"data/HolidayVacation/HolidayVacation.csv\").set_index(\"date\")[[cantonId]]\n",
    "    holy.rename(columns = {cantonId:'isHolyday'}, inplace = True)\n",
    "    holy.index = pd.to_datetime(holy.index)\n",
    "    df = df.join(holy)\n",
    "   \n",
    "    \n",
    "    # attach intervista mobility data\n",
    "    averageAndMedian = pd.read_csv(\"data/IntervistaMobility/Mittelwerte_und_Median_pro_Tag.csv\", encoding=\"mac_roman\")\n",
    "    averageAndMedian = averageAndMedian.loc[(averageAndMedian[\"Beschreibung\"] == \"Distanz\") & (averageAndMedian[\"Typ\"] == \"Median\")]\n",
    "    averageAndMedian = averageAndMedian.set_index(\"Datum\")\n",
    "    averageAndMedian = averageAndMedian[['Alter_15-29', 'Alter_30-64', 'Alter_65-79','M‚Ä∞nnlich', 'Weiblich', \n",
    "                                         'St‚Ä∞dtisch', 'L‚Ä∞ndlich', 'Erwerbst‚Ä∞tig','In_Ausbildung', 'Nicht_Erwerbst‚Ä∞tig',\n",
    "                                         'Auto_Ja','Auto_Nein', 'HaushaltsgrÀÜsse_1_Person', 'HaushaltsgrÀÜsse_2_Personen',\n",
    "                                         'HaushaltsgrÀÜsse_3+_Personen', 'Kinder_Ja', 'Kinder_Nein', 'D-CH','F-CH', 'I-CH']]\n",
    "    averageAndMedian.index = pd.to_datetime(averageAndMedian.index)\n",
    "    #averageAndMedian[interestedCols].rolling(window=7).mean().plot(kind='line', y=interestedCols, figsize=(20,10))\n",
    "    D_CH = ['AG','AI','AR', 'BE','BL', 'BS','LU','GR','NW', 'OW', 'SG', 'SH', 'SO', 'SZ', 'TG','GL','UR','ZG','ZH']\n",
    "    F_CH = ['FR', 'GE', 'JU', 'VD', 'VS', 'NE']\n",
    "    #'TI'\n",
    "    if cantonId in D_CH:\n",
    "        df = df.join(averageAndMedian[['D-CH']])\n",
    "        df.rename(columns = {'D-CH':'intervistaMob'}, inplace = True) \n",
    "    elif cantonId in F_CH:\n",
    "        df = df.join(averageAndMedian[['F-CH']])\n",
    "        df.rename(columns = {'F-CH':'intervistaMob'}, inplace = True) \n",
    "    else:\n",
    "        # cantonId = TI\n",
    "        df = df.join(averageAndMedian[['I-CH']])\n",
    "        df.rename(columns = {'I-CH':'intervistaMob'}, inplace = True)\n",
    "        \n",
    "   \n",
    "    # attach neighbor incidents (WEEKLY)\n",
    "    neigbors = {\n",
    "      'AG': ['BL','SO','BE','LU','ZH','Baden-Wurttemberg','ZG'],\n",
    "      'AI': ['AR','SG'],\n",
    "      'AR': ['AI','SG'],\n",
    "      'BE': ['AG','SO','JU','NE','FR','VD','VS','UR','NW','OW','LU'], \n",
    "      'BL': ['AG','BS','SO','Baden-Wurttemberg','Grand Est'], \n",
    "      'BS': ['BL','Baden-Wurttemberg','Grand Est'], \n",
    "      'FR': ['BE','VD','NE'], \n",
    "      'GE': ['VD','Auvergne Rhone Alpes'], \n",
    "      'GL': ['SG','SZ','UR','GR'], \n",
    "      'GR': ['SG','GL','UR','TI','Vorarlberg','Lombardia','Liechtenstein'], \n",
    "      'JU': ['BL','SO','BE','NE','Grand Est','Bourgogne Franche Comte'], \n",
    "      'LU': ['AG','BE','NW', 'OW','ZG','SZ'], \n",
    "      'NE': ['JU','BE','VD','FR','Bourgogne Franche Comte'], \n",
    "      'NW': ['OW','BE','LU','SZ','UR'], \n",
    "      'OW': ['NW','LU','BE','UR'], \n",
    "      'SG': ['AI','AR','TG','ZH','SZ','GL','GR','Vorarlberg','Liechtenstein'], \n",
    "      'SH': ['TG','ZH','Baden-Wurttemberg'], \n",
    "      'SO': ['BE','JU','BL','AG','Grand Est'],\n",
    "      'SZ': ['ZG','ZH','SG','GL','LU','NW','UR'], \n",
    "      'TG': ['SH','ZH','SG','Baden-Wurttemberg'], \n",
    "      'TI': ['UR','GR','Piemonte','Lombardia'], \n",
    "      'UR': ['TI','VS','GR','BE','NW','OW','SZ','GL'], \n",
    "      'VD': ['NE','GE','FR','VS','BE','Auvergne Rhone Alpes','Bourgogne Franche Comte'], \n",
    "      'VS': ['VD','BE','UR','Piemonte','Auvergne Rhone Alpes'], \n",
    "      'ZG': ['ZH','AG','LU','SZ'],\n",
    "      'ZH': ['SH','AG','ZG','SZ','SG','TG','Baden-Wurttemberg']\n",
    "    }\n",
    "    \n",
    "\n",
    "    foph = pd.read_csv(\"data/FOPH/data/COVID19Cases_geoRegion.csv\", parse_dates=True)\n",
    "    # we have to convert each date string to datetime to merge the dataframes later\n",
    "    foph[['datum']] = foph[['datum']].applymap(lambda x: datetime.datetime.strptime(x,'%Y-%m-%d').date())\n",
    "    foph = foph.set_index('datum')\n",
    "    foph['rate_14_day_per_100k'] = (100000*foph['sum14d']) / foph['pop']\n",
    "    foph = foph[['geoRegion','rate_14_day_per_100k']]\n",
    "    \n",
    "    temp = pd.DataFrame({'time':listOfMondays})\n",
    "    temp = temp.set_index('time')\n",
    "    ecdc = pd.read_csv(\"data/ECDC/ECDCsubnationalcaseweekly.csv\")\n",
    "    ecdc[['year_week']] = ecdc[['year_week']].applymap(lambda x: mondaysByWeekNr[x[0:4]+x[6:8]]) #datetime.datetime.strptime(x.replace('-','')+' MON', '%YW%U %a').date())\n",
    "    #display(type(ecdc['year_week'][0])) # year_week is is datetime.date\n",
    "    ecdc = ecdc.set_index(\"year_week\")\n",
    "    ecdc = ecdc[['region_name','rate_14_day_per_100k']]\n",
    "    temp.index = pd.to_datetime(temp.index)\n",
    "    foph.index = pd.to_datetime(foph.index)\n",
    "    ecdc.index = pd.to_datetime(ecdc.index)\n",
    "    for n in neigbors[cantonId]:\n",
    "        if len(n) != 2:\n",
    "            temp = temp.join(ecdc.loc[ecdc['region_name']==n][['rate_14_day_per_100k']])\n",
    "            temp.rename(columns = {'rate_14_day_per_100k':('incidence_'+n)}, inplace = True)\n",
    "        else:\n",
    "            temp = temp.join(foph.loc[foph['geoRegion']==n][['rate_14_day_per_100k']])\n",
    "            temp.rename(columns = {'rate_14_day_per_100k':('incidence_'+n)}, inplace = True)\n",
    "    # also add the incidence of the actual canton\n",
    "    #temp = temp.join(foph.loc[foph['geoRegion']==cantonId][['rate_14_day_per_100k']])\n",
    "    #temp.rename(columns = {'rate_14_day_per_100k':('incidence_'+cantonId)}, inplace = True)\n",
    "    temp['meanNeighborIncidence'] = temp.mean(axis=1)\n",
    "    temp['maxNeighborIncidence'] = temp.max(axis=1)\n",
    "    df = df.join(temp[['meanNeighborIncidence','maxNeighborIncidence']])\n",
    "    \n",
    "    \n",
    "    # compute statistic weather for missing values\n",
    "    statWeathDf = pd.read_csv(\"static_data/statistical_historicweather/statistical_\"+cantonId+\".csv\")\n",
    "    statWeathDf['date'] = statWeathDf.apply(lambda row: datetime.datetime(2020,int(row[\"month\"]),int(row[\"day\"])), axis=1)\n",
    "    statWeathDf = statWeathDf.set_index('date')\n",
    "    statWeathDf = statWeathDf[['temp.average_min','temp.average_max','clouds.mean','precipitation.mean']]\n",
    "    statWeathDf.columns = ['temp_min', 'temp_max', 'clouds', 'precipitation']\n",
    "    weather = statWeathDf\n",
    "    \n",
    "    # compute historic weather from stored data\n",
    "    storedWeathDf = pd.read_csv(\"static_data/historicweather_from_19_03_2020_to_17_03_2021/\"+cantonId+\".csv\")\n",
    "    storedWeathDf = storedWeathDf.set_index('dt')\n",
    "    storedWeathDf = storedWeathDf[['main.temp_min','main.temp_max','clouds.all', 'rain.1h','snow.1h']]\n",
    "    storedWeathDf = storedWeathDf.fillna(0)\n",
    "    storedWeathDf['precipitation'] = storedWeathDf[['rain.1h','snow.1h']].sum(axis=1)\n",
    "    storedWeathDf = storedWeathDf[['main.temp_min','main.temp_max','clouds.all','precipitation']] \n",
    "\n",
    "    startDate = datetime.datetime(2020, 3, 19)\n",
    "    endDate = datetime.datetime(2021, 2, 3)\n",
    "    temp = pd.DataFrame(index=pd.date_range(start=startDate, end=endDate), columns=storedWeathDf.columns)\n",
    "    for day in pd.date_range(start=startDate, end=endDate):   \n",
    "        oneDay = storedWeathDf.filter(like=day.strftime('%Y-%m-%d'), axis=0)\n",
    "        temp.loc[day] = {'main.temp_min': oneDay['main.temp_min'].min(), \n",
    "                         'main.temp_max': oneDay['main.temp_max'].max(), \n",
    "                         'clouds.all': oneDay['clouds.all'].mean(),\n",
    "                         'precipitation': oneDay['precipitation'].sum()}\n",
    "    storedWeathDf = temp\n",
    "    storedWeathDf.columns = ['temp_min', 'temp_max', 'clouds', 'precipitation']\n",
    "    weather = weather.append(storedWeathDf)\n",
    "\n",
    "    # compute historic weather from recently loaded weather update\n",
    "    updateWeathDf = pd.read_csv(\"data/historicweatherupdate/\"+cantonId+\".csv\")\n",
    "    updateWeathDf = updateWeathDf.set_index('dt')\n",
    "    updateWeathDf = updateWeathDf[['main.temp_min','main.temp_max','clouds.all', 'rain.1h','snow.1h']]\n",
    "    updateWeathDf = updateWeathDf.fillna(0)\n",
    "    updateWeathDf['precipitation'] = updateWeathDf[['rain.1h','snow.1h']].sum(axis=1)\n",
    "    updateWeathDf = updateWeathDf[['main.temp_min','main.temp_max','clouds.all','precipitation']] \n",
    "\n",
    "    endDate = datetime.datetime.strptime(updateWeathDf.index[-1], '%Y-%m-%d %H:%M:%S')\n",
    "    endDate = endDate.replace(hour=0, minute=0)\n",
    "    startDate = datetime.datetime(2021, 2, 4)\n",
    "    temp = pd.DataFrame(index=pd.date_range(start=startDate, end=endDate), columns=updateWeathDf.columns)\n",
    "    for day in pd.date_range(start=startDate, end=endDate):   \n",
    "        oneDay = updateWeathDf.filter(like=day.strftime('%Y-%m-%d'), axis=0)\n",
    "        temp.loc[day] = {'main.temp_min': oneDay['main.temp_min'].min(), \n",
    "                         'main.temp_max': oneDay['main.temp_max'].max(), \n",
    "                         'clouds.all': oneDay['clouds.all'].mean(),\n",
    "                         'precipitation': oneDay['precipitation'].sum()}\n",
    "    updateWeathDf = temp\n",
    "    updateWeathDf.columns = ['temp_min', 'temp_max', 'clouds', 'precipitation']\n",
    "    weather = weather.append(updateWeathDf)  \n",
    "    df = df.join(weather)\n",
    "    \n",
    "    \n",
    "    df.index.names = [\"date\"]\n",
    "    if not os.path.exists('data/merged'):\n",
    "        os.makedirs('data/merged')\n",
    "    df.to_csv('data/merged/'+cantonId+'.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
