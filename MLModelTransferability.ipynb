{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entire-canyon",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time\n",
    "import logging\n",
    "from joblib import dump, load\n",
    "from datetime import timedelta\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.multioutput import RegressorChain\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import xgboost as xgb\n",
    "import lightgbm as lightgbm\n",
    "\n",
    "# settings:\n",
    "numberOfInputWeeks = 3 # must be equal to the number of input weeks set in data preperator\n",
    "numberOfOutputWeeks = 4 # must be equal to the number of output week set in data preperator\n",
    "\n",
    "\n",
    "\n",
    "# data preperation\n",
    "cantonKeys = ['AG','AI','AR', 'BE', 'BL', 'BS', 'FR', 'GE', 'GL', 'GR', 'JU', 'LU', 'NE', 'NW', 'OW', 'SG', 'SH', 'SO', 'SZ', 'TG', 'TI', 'UR', 'VD', 'VS', 'ZG','ZH']\n",
    "data = pd.read_csv(\"completedata.csv\")\n",
    "\n",
    "\n",
    "outputCategories = ['case_inz_entries_7dayAverage',\n",
    "                  'hosp_inz_entries_7dayAverage',\n",
    "                  'death_inz_entries_7dayAverage',\n",
    "                  'testPositvity_7dayAverage',\n",
    "                  'transit_stations_percent_change_from_baseline_7dayAverage',\n",
    "                  'workplaces_percent_change_from_baseline_7dayAverage'\n",
    "                   ]\n",
    "\n",
    "\n",
    "\n",
    "# by default all the data is training data\n",
    "data[['category']] = 'train'\n",
    "\n",
    "# the prediction interval is from 29.5.21 to 25.6.21 which contains 3 decreasing and 1 plateau (valley) weeks (forth wave)\n",
    "# the corressponding input is marked accordingly\n",
    "data.loc[(data['lastInputDay'] >= '2021-05-01') & (data['lastInputDay'] <= '2021-05-28'),['category']] = 'test 1'\n",
    "\n",
    "# the prediction interval is from 12.10.20 to 26.10.20 which contains 2 increasing weeks (second wave)\n",
    "data.loc[(data['lastInputDay'] >= '2020-09-28') & (data['lastInputDay'] <= '2020-10-11'),['category']] = 'test 2'\n",
    "\n",
    "# the prediction interval is from 2.12.20 to 16.12.21 which contains 1 increasing and 1 plateau (mountain) week (third wave)\n",
    "data.loc[(data['lastInputDay'] >= '2020-11-18') & (data['lastInputDay'] <= '2020-12-01'),['category']] = 'test 2'\n",
    "\n",
    "\n",
    "#data[['lastInputDay']].sort_values(by='lastInputDay')\n",
    "completeNumber = data.shape[0]\n",
    "trainNumber = data[data['category']=='train'].shape[0]\n",
    "test1Number = data[data['category']=='test 1'].shape[0]\n",
    "test2Number = data[data['category']=='test 2'].shape[0]\n",
    "\n",
    "print(\"Complete: \"+ str(completeNumber) )\n",
    "print(\"Training set: \"+ str(trainNumber)+ \"(\"+str(trainNumber/completeNumber)+\")\")\n",
    "print(\"Test set 1 (decreasing): \"+ str(test1Number)+ \"(\"+str(test1Number/completeNumber)+\")\")\n",
    "print(\"Test set 2 (increasing): \"+ str(test2Number)+ \"(\"+str(test2Number/completeNumber)+\")\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "numberOfOutputs = len(outputCategories)\n",
    "split = numberOfOutputs * numberOfOutputWeeks + 2\n",
    "\n",
    "\n",
    "\n",
    "inputShape = data.iloc[:,0:-split].drop(['lastInputDay'], axis=1).shape[1:]\n",
    "\n",
    "    \n",
    "def generic1(learningrate, hiddenLayers, dropout, l1, l2, isMultiOutput):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Dense(hiddenLayers[0], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2), input_shape=inputShape))\n",
    "    model.add(keras.layers.Dropout(rate=dropout))\n",
    "    model.add(keras.layers.Dense(numberOfOutputWeeks) if isMultiOutput else keras.layers.Dense(1))\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.Adam(learning_rate=learningrate))\n",
    "    return model\n",
    "    \n",
    "def generic2(learningrate, hiddenLayers, dropout, l1, l2, isMultiOutput):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Dense(hiddenLayers[0], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2), input_shape=inputShape))\n",
    "    model.add(keras.layers.Dropout(rate=dropout))\n",
    "    model.add(keras.layers.Dense(hiddenLayers[1], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2)))\n",
    "    model.add(keras.layers.Dropout(rate=dropout))\n",
    "    model.add(keras.layers.Dense(numberOfOutputWeeks) if isMultiOutput else keras.layers.Dense(1))\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.Adam(learning_rate=learningrate))\n",
    "    return model\n",
    "\n",
    "def generic3(learningrate, hiddenLayers, dropout, l1, l2, isMultiOutput):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Dense(hiddenLayers[0], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2), input_shape=inputShape))\n",
    "    model.add(keras.layers.Dropout(rate=dropout))\n",
    "    model.add(keras.layers.Dense(hiddenLayers[1], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2)))\n",
    "    model.add(keras.layers.Dropout(rate=dropout))\n",
    "    model.add(keras.layers.Dense(hiddenLayers[2], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2)))\n",
    "    model.add(keras.layers.Dropout(rate=dropout))\n",
    "    model.add(keras.layers.Dense(numberOfOutputWeeks) if isMultiOutput else keras.layers.Dense(1))\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.Adam(learning_rate=learningrate))\n",
    "    return model\n",
    "\n",
    "def generic4(learningrate, hiddenLayers, dropout, l1, l2, isMultiOutput):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Dense(hiddenLayers[0], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2), input_shape=inputShape))\n",
    "    model.add(keras.layers.Dropout(rate=dropout))\n",
    "    model.add(keras.layers.Dense(hiddenLayers[1], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2)))\n",
    "    model.add(keras.layers.Dropout(rate=dropout))\n",
    "    model.add(keras.layers.Dense(hiddenLayers[2], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2)))\n",
    "    model.add(keras.layers.Dropout(rate=dropout))\n",
    "    model.add(keras.layers.Dense(hiddenLayers[3], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2)))\n",
    "    model.add(keras.layers.Dropout(rate=dropout))\n",
    "    model.add(keras.layers.Dense(numberOfOutputWeeks) if isMultiOutput else keras.layers.Dense(1))\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.Adam(learning_rate=learningrate))\n",
    "    return model\n",
    "\n",
    "def generic5(learningrate, hiddenLayers, dropout, l1, l2, isMultiOutput):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Dense(hiddenLayers[0], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2), input_shape=inputShape))\n",
    "    model.add(keras.layers.Dropout(rate=dropout))\n",
    "    model.add(keras.layers.Dense(hiddenLayers[1], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2)))\n",
    "    model.add(keras.layers.Dropout(rate=dropout))\n",
    "    model.add(keras.layers.Dense(hiddenLayers[2], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2)))\n",
    "    model.add(keras.layers.Dropout(rate=dropout))\n",
    "    model.add(keras.layers.Dense(hiddenLayers[3], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2)))\n",
    "    model.add(keras.layers.Dropout(rate=dropout))\n",
    "    model.add(keras.layers.Dense(hiddenLayers[4], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2)))\n",
    "    model.add(keras.layers.Dropout(rate=dropout))\n",
    "    model.add(keras.layers.Dense(numberOfOutputWeeks) if isMultiOutput else keras.layers.Dense(1))\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.Adam(learning_rate=learningrate))\n",
    "    return model\n",
    "\n",
    "def generic6(learningrate, hiddenLayers, dropout, l1, l2, isMultiOutput):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Dense(hiddenLayers[0], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2), input_shape=inputShape))\n",
    "    model.add(keras.layers.Dropout(rate=dropout))\n",
    "    model.add(keras.layers.Dense(hiddenLayers[1], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2)))\n",
    "    model.add(keras.layers.Dropout(rate=dropout))\n",
    "    model.add(keras.layers.Dense(hiddenLayers[2], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2)))\n",
    "    model.add(keras.layers.Dropout(rate=dropout))\n",
    "    model.add(keras.layers.Dense(hiddenLayers[3], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2)))\n",
    "    model.add(keras.layers.Dropout(rate=dropout))\n",
    "    model.add(keras.layers.Dense(hiddenLayers[4], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2)))\n",
    "    model.add(keras.layers.Dropout(rate=dropout))\n",
    "    model.add(keras.layers.Dense(hiddenLayers[5], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2)))\n",
    "    model.add(keras.layers.Dropout(rate=dropout))\n",
    "    model.add(keras.layers.Dense(numberOfOutputWeeks) if isMultiOutput else keras.layers.Dense(1))\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.Adam(learning_rate=learningrate))\n",
    "    return model\n",
    "\n",
    "def generic7(learningrate, hiddenLayers, dropout, l1, l2, isMultiOutput):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Dense(hiddenLayers[0], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2), input_shape=inputShape))\n",
    "    model.add(keras.layers.Dropout(rate=dropout))\n",
    "    model.add(keras.layers.Dense(hiddenLayers[1], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2)))\n",
    "    model.add(keras.layers.Dropout(rate=dropout))\n",
    "    model.add(keras.layers.Dense(hiddenLayers[2], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2)))\n",
    "    model.add(keras.layers.Dropout(rate=dropout))\n",
    "    model.add(keras.layers.Dense(hiddenLayers[3], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2)))\n",
    "    model.add(keras.layers.Dropout(rate=dropout))\n",
    "    model.add(keras.layers.Dense(hiddenLayers[4], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2)))\n",
    "    model.add(keras.layers.Dropout(rate=dropout))\n",
    "    model.add(keras.layers.Dense(hiddenLayers[5], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2)))\n",
    "    model.add(keras.layers.Dropout(rate=dropout))\n",
    "    model.add(keras.layers.Dense(hiddenLayers[6], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2)))\n",
    "    model.add(keras.layers.Dropout(rate=dropout))\n",
    "    model.add(keras.layers.Dense(numberOfOutputWeeks) if isMultiOutput else keras.layers.Dense(1))\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.Adam(learning_rate=learningrate))\n",
    "    return model\n",
    "\n",
    "def generic8(learningrate, hiddenLayers, dropout, l1, l2, isMultiOutput):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Dense(hiddenLayers[0], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2), input_shape=inputShape))\n",
    "    model.add(keras.layers.Dropout(rate=dropout))\n",
    "    model.add(keras.layers.Dense(hiddenLayers[1], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2)))\n",
    "    model.add(keras.layers.Dropout(rate=dropout))\n",
    "    model.add(keras.layers.Dense(hiddenLayers[2], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2)))\n",
    "    model.add(keras.layers.Dropout(rate=dropout))\n",
    "    model.add(keras.layers.Dense(hiddenLayers[3], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2)))\n",
    "    model.add(keras.layers.Dropout(rate=dropout))\n",
    "    model.add(keras.layers.Dense(hiddenLayers[4], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2)))\n",
    "    model.add(keras.layers.Dropout(rate=dropout))\n",
    "    model.add(keras.layers.Dense(hiddenLayers[5], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2)))\n",
    "    model.add(keras.layers.Dropout(rate=dropout))\n",
    "    model.add(keras.layers.Dense(hiddenLayers[6], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2)))\n",
    "    model.add(keras.layers.Dropout(rate=dropout))\n",
    "    model.add(keras.layers.Dense(hiddenLayers[7], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2)))\n",
    "    model.add(keras.layers.Dropout(rate=dropout))\n",
    "    model.add(keras.layers.Dense(numberOfOutputWeeks) if isMultiOutput else keras.layers.Dense(1))\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.Adam(learning_rate=learningrate))\n",
    "    return model\n",
    "    \n",
    "\n",
    "    \n",
    "def genericKerasModel(learningrate, hiddenLayers, dropout, l1regularization, l2regularization, isMultioutput):\n",
    "    if len(hiddenLayers) == 1:\n",
    "        return generic1(learningrate, hiddenLayers, dropout, l1regularization, l2regularization, isMultioutput)\n",
    "    elif len(hiddenLayers) == 2:\n",
    "        return generic2(learningrate, hiddenLayers, dropout, l1regularization, l2regularization, isMultioutput)\n",
    "    elif len(hiddenLayers) == 3:\n",
    "        return generic3(learningrate, hiddenLayers, dropout, l1regularization, l2regularization, isMultioutput)\n",
    "    elif len(hiddenLayers) == 4:\n",
    "        return generic4(learningrate, hiddenLayers, dropout, l1regularization, l2regularization, isMultioutput)\n",
    "    elif len(hiddenLayers) == 5:\n",
    "        return generic5(learningrate, hiddenLayers, dropout, l1regularization, l2regularization, isMultioutput)\n",
    "    elif len(hiddenLayers) == 6:\n",
    "        return generic6(learningrate, hiddenLayers, dropout, l1regularization, l2regularization, isMultioutput)\n",
    "    elif len(hiddenLayers) == 7:\n",
    "        return generic7(learningrate, hiddenLayers, dropout, l1regularization, l2regularization, isMultioutput)\n",
    "    else:\n",
    "        return generic8(learningrate, hiddenLayers, dropout, l1regularization, l2regularization, isMultioutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abstract-colony",
   "metadata": {},
   "outputs": [],
   "source": [
    "numberOfIterations = 10\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"cantonNumber\", help=\"computes result for specific canton\",type=int)\n",
    "args = parser.parse_args()\n",
    "cantonNumber = args.cantonNumber\n",
    "'''\n",
    "cantonNumber = 0\n",
    "'''\n",
    "# we make additional benchmark computations when we compute the results for canton 0\n",
    "\n",
    "\n",
    "\n",
    "estimators = {\n",
    "    'case_inz_entries_7dayAverage': {\n",
    "        \"modelClass\": \"Keras\",\n",
    "        \"isMultiWeek\": True,\n",
    "        \"hiddenLayers\": [30, 20],\n",
    "        \"numberOfhiddenLayers\": 2,\n",
    "        \"isTwoWay\": False,\n",
    "        \"l1reg\": 0.1,\n",
    "        \"alpha\": 0.1,\n",
    "        \"dropoutValue\": 0.5,\n",
    "        \"learningRate\": 0.001,\n",
    "        \"epochs\":68\n",
    "    },\n",
    "    'hosp_inz_entries_7dayAverage': {\n",
    "        \"modelClass\": \"Keras\",\n",
    "        \"isMultiWeek\": True,\n",
    "        \"hiddenLayers\": [350, 175],\n",
    "        \"numberOfhiddenLayers\": 2,\n",
    "        \"isTwoWay\": False,\n",
    "        \"l1reg\": 0.001,\n",
    "        \"alpha\": 0,\n",
    "        \"dropoutValue\": 0.3,\n",
    "        \"learningRate\": 0.001,\n",
    "        \"epochs\":5\n",
    "    },\n",
    "    'death_inz_entries_7dayAverage': {\n",
    "        \"modelClass\": \"Keras\",\n",
    "        \"isMultiWeek\": True,\n",
    "        \"hiddenLayers\": [30],\n",
    "        \"numberOfhiddenLayers\": 1,\n",
    "        \"isTwoWay\": False,\n",
    "        \"l1reg\": 0.001,\n",
    "        \"alpha\": 0.01,\n",
    "        \"dropoutValue\": 0.3,\n",
    "        \"learningRate\": 0.001,\n",
    "        \"epochs\":14\n",
    "    },\n",
    "    'testPositvity_7dayAverage': {\n",
    "        \"modelClass\": \"Keras\",\n",
    "        \"isMultiWeek\": True,\n",
    "        \"hiddenLayers\": [175],\n",
    "        \"numberOfhiddenLayers\": 1,\n",
    "        \"isTwoWay\": False,\n",
    "        \"l1reg\": 0,\n",
    "        \"alpha\": 0.0001,\n",
    "        \"dropoutValue\": 0.3,\n",
    "        \"learningRate\": 0.0001,\n",
    "        \"epochs\":43\n",
    "    },\n",
    "    'transit_stations_percent_change_from_baseline_7dayAverage': {\n",
    "        \"modelClass\": \"Keras\",\n",
    "        \"isMultiWeek\": True,\n",
    "        \"hiddenLayers\": [175, 87, 40],\n",
    "        \"numberOfhiddenLayers\": 3,\n",
    "        \"isTwoWay\": False,\n",
    "        \"l1reg\": 0,\n",
    "        \"alpha\": 0.0001,\n",
    "        \"dropoutValue\": 0.2,\n",
    "        \"learningRate\": 0.0001,\n",
    "        \"epochs\":17\n",
    "    },\n",
    "    'workplaces_percent_change_from_baseline_7dayAverage': {\n",
    "        \"modelClass\": \"Keras\",\n",
    "        \"isMultiWeek\": True,\n",
    "        \"hiddenLayers\": [1000],\n",
    "        \"numberOfhiddenLayers\": 1,\n",
    "        \"isTwoWay\": False,\n",
    "        \"l1reg\": 0.01,\n",
    "        \"alpha\": 0.0001,\n",
    "        \"dropoutValue\": 0,\n",
    "        \"learningRate\": 0.001,\n",
    "        \"task\":'workplaces_percent_change_from_baseline_7dayAverage',\n",
    "        \"epochs\":23\n",
    "    }\n",
    "}  \n",
    "\n",
    "\n",
    "if not os.path.exists('transferability/'):\n",
    "    os.makedirs('transferability')\n",
    "\n",
    "    \n",
    "# first we compute a benchmark    \n",
    "# Experiment 1 (seen case / base case)\n",
    "if cantonNumber == 0:\n",
    "    resultsDf = pd.DataFrame()\n",
    "    for task in outputCategories:\n",
    "        # train on all cantons including i & validate on i (we train the model just once and validate all \n",
    "        # 26 canton in one job)     \n",
    "\n",
    "        # get the selected model for the specific task\n",
    "        estimator = genericKerasModel(estimators[task][\"learningRate\"], \n",
    "                          estimators[task][\"hiddenLayers\"], \n",
    "                          estimators[task][\"dropoutValue\"], \n",
    "                          estimators[task][\"l1reg\"], \n",
    "                          estimators[task][\"alpha\"], \n",
    "                          estimators[task][\"isMultiWeek\"])\n",
    "        \n",
    "        pip = Pipeline([('minmax_scaler', MinMaxScaler())])\n",
    "        train_features_all = data[(data['category']=='train')].iloc[:,0:-split].drop(['lastInputDay'], axis=1)\n",
    "        train_labels_all = data[data['category']=='train'].iloc[:,-split:-2]\n",
    "\n",
    "        for iteration in range(0,numberOfIterations):\n",
    "            # train the model with the training data\n",
    "            history = estimator.fit(pip.fit_transform(train_features_all.values), \n",
    "                                  train_labels_all[[\"output_\"+task+\"_\"+str(outputWeekNumber) for outputWeekNumber in range(0,numberOfOutputWeeks)]].values, \n",
    "                                  batch_size=32, \n",
    "                                  epochs=estimators[task][\"epochs\"], \n",
    "                                  verbose=0)\n",
    "\n",
    "\n",
    "            # compute and safe results for every canton\n",
    "            for cantonId in cantonKeys:\n",
    "                # get test features from the specific canton\n",
    "                test1_features_cId = data[(data['category']=='test 1') & (data['cantonId']==cantonId)].iloc[:,0:-split].drop(['lastInputDay'], axis=1)\n",
    "                test2_features_cId = data[(data['category']=='test 2') & (data['cantonId']==cantonId)].iloc[:,0:-split].drop(['lastInputDay'], axis=1)\n",
    "\n",
    "                # get test labels from the specific canton\n",
    "                test1_labels_specific = data[(data['category']=='test 1') & (data['cantonId']==cantonId)].iloc[:,-split:-2]\n",
    "                test2_labels_specific = data[(data['category']=='test 2') & (data['cantonId']==cantonId)].iloc[:,-split:-2]\n",
    "\n",
    "                # compute model predictions\n",
    "                predictions1 = pd.DataFrame(estimator.predict(pip.transform(test1_features_cId.values)), \n",
    "                                            index=test1_labels_specific.index, \n",
    "                                            columns=[\"pred_week_\"+task+\"_\"+str(outputWeekNumber) for outputWeekNumber in range(0,numberOfOutputWeeks)]) \n",
    "\n",
    "                predictions2 = pd.DataFrame(estimator.predict(pip.transform(test2_features_cId.values)), \n",
    "                                            index=test2_labels_specific.index, \n",
    "                                            columns=[\"pred_week_\"+task+\"_\"+str(outputWeekNumber) for outputWeekNumber in range(0,numberOfOutputWeeks)]) \n",
    "\n",
    "                # compute error\n",
    "                # compute and safe results for every week\n",
    "                for outputWeekNumber in range(0,numberOfOutputWeeks):\n",
    "                    # compute root mean squared error for validation sets\n",
    "                    rmse1 = np.sqrt(mean_squared_error(predictions1[\"pred_week_\"+task+\"_\"+str(outputWeekNumber)], test1_labels_specific[\"output_\"+task+\"_\"+str(outputWeekNumber)]))\n",
    "                    rmse2 = np.sqrt(mean_squared_error(predictions2[\"pred_week_\"+task+\"_\"+str(outputWeekNumber)], test2_labels_specific[\"output_\"+task+\"_\"+str(outputWeekNumber)]))\n",
    "                    # safe the results and all model parameters\n",
    "                    res = {'task':[task],\n",
    "                         'week':[outputWeekNumber], \n",
    "                         'model rmse 1':[rmse1], \n",
    "                         'model rmse 2':[rmse2],\n",
    "                         'trainedOn': 'all',\n",
    "                         'errorForCanton': cantonId,\n",
    "                         'iteration': iteration\n",
    "                        }\n",
    "                    resultsDf = resultsDf.append(pd.DataFrame(data=res), ignore_index = True)\n",
    "                \n",
    "    if not os.path.exists('transferability/allCantonalTraining'):\n",
    "                os.makedirs('transferability/allCantonalTraining')            \n",
    "    resultsDf.to_csv('transferability/allCantonalTraining/all.csv', header=True, index=False)\n",
    "    \n",
    "    \n",
    "    \n",
    "# Experiment 2 (unseen case / how are our predictions for cantons we did not train on?)\n",
    "resultsDf = pd.DataFrame()\n",
    "for task in outputCategories:\n",
    "    # train on all other cantons but i & validate on i\n",
    "    estimator = genericKerasModel(estimators[task][\"learningRate\"], \n",
    "                          estimators[task][\"hiddenLayers\"], \n",
    "                          estimators[task][\"dropoutValue\"], \n",
    "                          estimators[task][\"l1reg\"], \n",
    "                          estimators[task][\"alpha\"], \n",
    "                          estimators[task][\"isMultiWeek\"])\n",
    "    \n",
    "    pip = Pipeline([('minmax_scaler', MinMaxScaler())])\n",
    "    train_features_without = data[(data['category']=='train') & (data['cantonId']!=cantonKeys[cantonNumber])].iloc[:,0:-split].drop(['lastInputDay'], axis=1)\n",
    "    train_labels_without = data[(data['category']=='train') & (data['cantonId']!=cantonKeys[cantonNumber])].iloc[:,-split:-2]\n",
    "    \n",
    "    \n",
    "    # get test features from the specific canton\n",
    "    test1_features_cId = data[(data['category']=='test 1') & (data['cantonId']==cantonKeys[cantonNumber])].iloc[:,0:-split].drop(['lastInputDay'], axis=1)\n",
    "    test2_features_cId = data[(data['category']=='test 2') & (data['cantonId']==cantonKeys[cantonNumber])].iloc[:,0:-split].drop(['lastInputDay'], axis=1)\n",
    "\n",
    "    # get test labels from the specific canton\n",
    "    test1_labels_specific = data[(data['category']=='test 1') & (data['cantonId']==cantonKeys[cantonNumber])].iloc[:,-split:-2]\n",
    "    test2_labels_specific = data[(data['category']=='test 2') & (data['cantonId']==cantonKeys[cantonNumber])].iloc[:,-split:-2]\n",
    "    \n",
    "    for iteration in range(0,numberOfIterations):\n",
    "        history = estimator.fit(pip.fit_transform(train_features_without.values), \n",
    "                                  train_labels_without[[\"output_\"+task+\"_\"+str(outputWeekNumber) for outputWeekNumber in range(0,numberOfOutputWeeks)]].values, \n",
    "                                  batch_size=32, \n",
    "                                  epochs=estimators[task][\"epochs\"], \n",
    "                                  verbose=0)\n",
    "\n",
    "        # compute model predictions\n",
    "        predictions1 = pd.DataFrame(estimator.predict(pip.transform(test1_features_cId.values)), \n",
    "                                    index=test1_labels_specific.index, \n",
    "                                    columns=[\"pred_week_\"+task+\"_\"+str(outputWeekNumber) for outputWeekNumber in range(0,numberOfOutputWeeks)]) \n",
    "\n",
    "        predictions2 = pd.DataFrame(estimator.predict(pip.transform(test2_features_cId.values)), \n",
    "                                    index=test2_labels_specific.index, \n",
    "                                    columns=[\"pred_week_\"+task+\"_\"+str(outputWeekNumber) for outputWeekNumber in range(0,numberOfOutputWeeks)]) \n",
    "\n",
    "        # compute error\n",
    "        # compute and safe results for every week\n",
    "\n",
    "        for outputWeekNumber in range(0,numberOfOutputWeeks):\n",
    "            # compute root mean squared error for validation sets\n",
    "            rmse1 = np.sqrt(mean_squared_error(predictions1[\"pred_week_\"+task+\"_\"+str(outputWeekNumber)], test1_labels_specific[\"output_\"+task+\"_\"+str(outputWeekNumber)]))\n",
    "            rmse2 = np.sqrt(mean_squared_error(predictions2[\"pred_week_\"+task+\"_\"+str(outputWeekNumber)], test2_labels_specific[\"output_\"+task+\"_\"+str(outputWeekNumber)]))\n",
    "            # safe the results and all model parameters\n",
    "            res = {'task':[task],\n",
    "                 'week':[outputWeekNumber], \n",
    "                 'model rmse 1':[rmse1], \n",
    "                 'model rmse 2':[rmse2],\n",
    "                 'trainedOn': 'allWithoutCantonI',\n",
    "                 'errorForCanton': cantonKeys[cantonNumber],\n",
    "                 'iteration': iteration\n",
    "                }\n",
    "            resultsDf = resultsDf.append(pd.DataFrame(data=res), ignore_index = True)\n",
    "if not os.path.exists('transferability/allWithoutCantonI'):\n",
    "            os.makedirs('transferability/allWithoutCantonI')            \n",
    "resultsDf.to_csv('transferability/allWithoutCantonI/'+cantonKeys[cantonNumber]+'.csv', header=True, index=False)\n",
    "     \n",
    "    \n",
    "    \n",
    "    \n",
    "# Experiment 3 (advantage from data of other regions / does data from other cantons help?)\n",
    "resultsDf = pd.DataFrame()\n",
    "for task in outputCategories:\n",
    "    # train on i\n",
    "    # validate on i\n",
    "    estimator = genericKerasModel(estimators[task][\"learningRate\"], \n",
    "                          estimators[task][\"hiddenLayers\"], \n",
    "                          estimators[task][\"dropoutValue\"], \n",
    "                          estimators[task][\"l1reg\"], \n",
    "                          estimators[task][\"alpha\"], \n",
    "                          estimators[task][\"isMultiWeek\"])\n",
    "    pip = Pipeline([('minmax_scaler', MinMaxScaler())])\n",
    "    train_features_only = data[(data['category']=='train') & (data['cantonId']==cantonKeys[cantonNumber])].iloc[:,0:-split].drop(['lastInputDay'], axis=1)\n",
    "    train_labels_only = data[(data['category']=='train') & (data['cantonId']==cantonKeys[cantonNumber])].iloc[:,-split:-2]\n",
    "    \n",
    "    # get test features from the specific canton\n",
    "    test1_features_cId = data[(data['category']=='test 1') & (data['cantonId']==cantonKeys[cantonNumber])].iloc[:,0:-split].drop(['lastInputDay'], axis=1)\n",
    "    test2_features_cId = data[(data['category']=='test 2') & (data['cantonId']==cantonKeys[cantonNumber])].iloc[:,0:-split].drop(['lastInputDay'], axis=1)\n",
    "\n",
    "    # get test labels from the specific canton\n",
    "    test1_labels_specific = data[(data['category']=='test 1') & (data['cantonId']==cantonKeys[cantonNumber])].iloc[:,-split:-2]\n",
    "    test2_labels_specific = data[(data['category']=='test 2') & (data['cantonId']==cantonKeys[cantonNumber])].iloc[:,-split:-2]\n",
    "    \n",
    "    for iteration in range(0,numberOfIterations):\n",
    "        history = estimator.fit(pip.fit_transform(train_features_only.values), \n",
    "                                  train_labels_only[[\"output_\"+task+\"_\"+str(outputWeekNumber) for outputWeekNumber in range(0,numberOfOutputWeeks)]].values, \n",
    "                                  batch_size=32, \n",
    "                                  epochs=estimators[task][\"epochs\"], \n",
    "                                  verbose=0)        \n",
    "\n",
    "        # compute model predictions\n",
    "        predictions1 = pd.DataFrame(estimator.predict(pip.transform(test1_features_cId.values)), \n",
    "                                    index=test1_labels_specific.index, \n",
    "                                    columns=[\"pred_week_\"+task+\"_\"+str(outputWeekNumber) for outputWeekNumber in range(0,numberOfOutputWeeks)]) \n",
    "\n",
    "        predictions2 = pd.DataFrame(estimator.predict(pip.transform(test2_features_cId.values)), \n",
    "                                    index=test2_labels_specific.index, \n",
    "                                    columns=[\"pred_week_\"+task+\"_\"+str(outputWeekNumber) for outputWeekNumber in range(0,numberOfOutputWeeks)]) \n",
    "\n",
    "        # compute error\n",
    "        # compute and safe results for every week\n",
    "\n",
    "        for outputWeekNumber in range(0,numberOfOutputWeeks):\n",
    "            # compute root mean squared error for validation sets\n",
    "            rmse1 = np.sqrt(mean_squared_error(predictions1[\"pred_week_\"+task+\"_\"+str(outputWeekNumber)], test1_labels_specific[\"output_\"+task+\"_\"+str(outputWeekNumber)]))\n",
    "            rmse2 = np.sqrt(mean_squared_error(predictions2[\"pred_week_\"+task+\"_\"+str(outputWeekNumber)], test2_labels_specific[\"output_\"+task+\"_\"+str(outputWeekNumber)]))\n",
    "            # safe the results and all model parameters\n",
    "            res = {'task':[task],\n",
    "                 'week':[outputWeekNumber], \n",
    "                 'model rmse 1':[rmse1], \n",
    "                 'model rmse 2':[rmse2],\n",
    "                 'trainedOn': 'onlyCantonI',\n",
    "                 'errorForCanton': cantonKeys[cantonNumber],\n",
    "                 'iteration': iteration\n",
    "                }\n",
    "            resultsDf = resultsDf.append(pd.DataFrame(data=res), ignore_index = True)\n",
    "        \n",
    "if not os.path.exists('transferability/onlyCantonI'):\n",
    "            os.makedirs('transferability/onlyCantonI')            \n",
    "resultsDf.to_csv('transferability/onlyCantonI/'+cantonKeys[cantonNumber]+'.csv', header=True, index=False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joint-excellence",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
