{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "prime-laundry",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time\n",
    "import logging\n",
    "from joblib import dump, load\n",
    "from datetime import timedelta\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.multioutput import RegressorChain\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import xgboost as xgb\n",
    "import lightgbm as lightgbm\n",
    "\n",
    "pd.options.display.max_rows = 10\n",
    "\n",
    "\n",
    "logging.basicConfig(filename='output.log', level=logging.INFO, filemode='w')\n",
    "\n",
    "completeTimer = time.time()\n",
    "typeTimer = time.time()\n",
    "estimatorTimer = time.time()\n",
    "\n",
    "\n",
    "# environment\n",
    "isSuperComputer = True\n",
    "\n",
    "cantonKeys = ['AG','AI','AR', 'BE', 'BL', 'BS', 'FR', 'GE', 'GL', 'GR', 'JU', 'LU', 'NE', 'NW', 'OW', 'SG', 'SH', 'SO', 'SZ', 'TG', 'TI', 'UR', 'VD', 'VS', 'ZG','ZH']\n",
    "\n",
    "data = pd.read_csv(\"completedata.csv\")\n",
    "\n",
    "# last check if there are some NaN values in the data which we use\n",
    "for col in data.columns:\n",
    "    if data[col].isna().sum() != 0:\n",
    "        print(col+\" (#NaN/#NotNaN): (\" + str(data[col].isna().sum())+\"/\"+str(data[col].notna().sum())+\")\")\n",
    "\n",
    "\n",
    "numberOfInputWeeks = 3\n",
    "\n",
    "#display([col for col in data.columns])\n",
    "\n",
    "\n",
    "outputCategories = [\n",
    "                  'case_inz_entries_7dayAverage',\n",
    "                  'hosp_inz_entries_7dayAverage',\n",
    "                  'death_inz_entries_7dayAverage',\n",
    "                  'testPositvity',\n",
    "                  'testPositvity_7dayAverageBoth',    \n",
    "                  'transit_stations_percent_change_from_baseline_7dayAverage',\n",
    "                  'workplaces_percent_change_from_baseline_7dayAverage',    \n",
    "                   ]\n",
    "numberOfOutputs = len(outputCategories)\n",
    "numberOfPreComputedOutputWeeks = 4\n",
    "\n",
    "split = numberOfOutputs * numberOfPreComputedOutputWeeks + 2\n",
    "\n",
    "train_features = data[data['category']=='train'].iloc[:,0:-split].drop(['lastInputDay'], axis=1)\n",
    "train_labels = data[data['category']=='train'].iloc[:,-split:-2]\n",
    "validation1_features = data[data['category']=='validation 1'].iloc[:,0:-split].drop(['lastInputDay'], axis=1)\n",
    "validation1_labels = data[data['category']=='validation 1'].iloc[:,-split:-2]\n",
    "validation2_features = data[data['category']=='validation 2'].iloc[:,0:-split].drop(['lastInputDay'], axis=1)\n",
    "validation2_labels = data[data['category']=='validation 2'].iloc[:,-split:-2]\n",
    "\n",
    "'''\n",
    "cId = 'AG'\n",
    "train_features = data[(data['category']=='train') & (data['cantonId']==cId)].iloc[:,0:-split].drop(['lastInputDay'], axis=1)\n",
    "train_labels = data[(data['category']=='train') & (data['cantonId']==cId)].iloc[:,-split:-2]\n",
    "validation1_features = data[(data['category']=='validation 1') & (data['cantonId']==cId)].iloc[:,0:-split].drop(['lastInputDay'], axis=1)\n",
    "validation1_labels = data[(data['category']=='validation 1') & (data['cantonId']==cId)].iloc[:,-split:-2]\n",
    "validation2_features = data[(data['category']=='validation 2') & (data['cantonId']==cId)].iloc[:,0:-split].drop(['lastInputDay'], axis=1)\n",
    "validation2_labels = data[(data['category']=='validation 2') & (data['cantonId']==cId)].iloc[:,-split:-2]\n",
    "'''\n",
    "\n",
    "#pip = Pipeline([('std_scaler', StandardScaler())])\n",
    "pip = Pipeline([('minmax_scaler', MinMaxScaler())])\n",
    "\n",
    "X_train = pip.fit_transform(train_features[train_features.columns].values)\n",
    "X_valid1 = pip.transform(validation1_features[train_features.columns].values)\n",
    "X_valid2 = pip.transform(validation2_features[train_features.columns].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "stunning-letters",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# numberOfOutputWeeks = 1  week number which will be computed, possibilities 1,2,3 or 4\n",
    "modelId = 0\n",
    "results = pd.DataFrame(columns=['model',\n",
    "                                'model type',\n",
    "                                'target',\n",
    "                                'week',\n",
    "                                'model rsme 1', \n",
    "                                'model rsme 2'])\n",
    "results.to_csv(\"results.csv\", index=False)\n",
    "for numberOfOutputWeeks in [1,2,3,4]:\n",
    "    logging.info(\"============== fitting week \"+str(numberOfOutputWeeks)+\" ============== \")\n",
    "    \n",
    "    # (str(type(estimator)) == \"<class 'tensorflow.python.keras.engine.functional.Functional'>\")\n",
    "\n",
    "\n",
    "\n",
    "    # --------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    typeTimer = time.time()\n",
    "    logging.info(\"============== fitting normal sklearn estimators ============== \")\n",
    "\n",
    "    # fitting normal sklearn estimators\n",
    "    \n",
    "    \n",
    "\n",
    "    estimators = [\n",
    "        linear_model.Ridge(alpha=1),\n",
    "        linear_model.Lasso(alpha=1),\n",
    "    ]\n",
    "\n",
    "\n",
    "    if isSuperComputer:\n",
    "        # all all estimators which we want to search through\n",
    "        estimators = []\n",
    "        \n",
    "        kernels = ['linear','poly','polynomial','rbf','laplacian','sigmoid','cosine']\n",
    "        for alpha in [0.01,0.1,10]:\n",
    "            for k in kernels:\n",
    "                estimators.append(KernelRidge(kernel=k, alpha=alpha))\n",
    "\n",
    "        # add linear models\n",
    "        estimators.append(linear_model.Ridge(alpha=100))\n",
    "        estimators.append(linear_model.Ridge(alpha=1000))\n",
    "        \n",
    "        '''\n",
    "        estimators.append(linear_model.Ridge(alpha=0))\n",
    "        estimators.append(linear_model.Lasso(alpha=0)) \n",
    "        \n",
    "        for alpha in [0.00001, 0.0001,0.001,0.01,0.1,1,10]:\n",
    "            estimators.append(linear_model.Ridge(alpha=alpha))\n",
    "            estimators.append(linear_model.Lasso(alpha=alpha)) \n",
    "            for lr in ['invscaling']: #,'adaptive','optimal'\n",
    "                for p in ['l1','l2']: #,'elasticnet'\n",
    "                    estimators.append(linear_model.SGDRegressor(learning_rate = lr,penalty=p, alpha=alpha,max_iter=10000, tol=0.00001, n_iter_no_change=10))\n",
    "\n",
    "        # add SVRs\n",
    "        for c in [0.001,0.01,0.1,1,10, 100]: \n",
    "            estimators.append(svm.SVR(C=c)) \n",
    "\n",
    "\n",
    "        # add MLPs\n",
    "        for alpha in [0.00001, 0.0001,0.001,0.01,0.1,1,10]: \n",
    "            for layer in [\n",
    "                (20),\n",
    "                (100),\n",
    "                (20,20),\n",
    "                (100,10),\n",
    "                (100,50,10),\n",
    "                (100,100,10),\n",
    "            ]:\n",
    "                estimators.append(MLPRegressor(hidden_layer_sizes=layer, alpha=alpha, max_iter=1000))\n",
    "\n",
    "\n",
    "        # add LGBMs & XGBoost\n",
    "        for regalpha in [0.01, 0.1,0,1,10]:\n",
    "            for reglambda in [0.01, 0.1,0,1,10]: \n",
    "                estimators.append(lightgbm.LGBMRegressor(n_estimators=1000, reg_alpha=regalpha, reg_lambda=reglambda, n_jobs=-1))\n",
    "                estimators.append(xgb.XGBRegressor(n_estimators=1000, reg_alpha=regalpha, reg_lambda=reglambda, n_jobs=-1))  \n",
    "\n",
    "        # add random forrest  \n",
    "        for nrOfEstimators in [100,200,500]: #1000\n",
    "            for maxdepth in [None, 2, 4]:\n",
    "                for mss in [2,3,4]:\n",
    "                    for msl in [1,2,3]:\n",
    "                        estimators.append(RandomForestRegressor(n_estimators=nrOfEstimators, \n",
    "                                                                max_depth=maxdepth, \n",
    "                                                                min_samples_split=mss, \n",
    "                                                                min_samples_leaf=msl,\n",
    "                                                                n_jobs=-1))\n",
    "        '''\n",
    "    logging.info(\"number of estimators to fit: \" + str(len(estimators)*len(outputCategories))+\" \")\n",
    "\n",
    "\n",
    "    while len(estimators) > 0:\n",
    "        estimator = estimators.pop(0)\n",
    "        estimatorTimer = time.time()\n",
    "        logging.info(str(estimator)+\"\")\n",
    "        for category in outputCategories:\n",
    "            logging.info(\"-- \" + category+\"\")\n",
    "\n",
    "            trainOutput = train_labels[[\"output_\"+category+\"_\"+str(numberOfOutputWeeks-1)]] \n",
    "            reg = estimator.fit(X_train, trainOutput.values)           \n",
    "            # save the fitted regressor\n",
    "            #dump(reg, 'models/'+str(modelId)+'.joblib') \n",
    "            #reg = load('models/'+str(modelId)+'.joblib') \n",
    "            modelId = modelId + 1\n",
    "\n",
    "            predictions_valid1 = pd.DataFrame(reg.predict(X_valid1), index=validation1_labels.index, columns=[\"pred_week_\" + str(numberOfOutputWeeks-1)+\"_\"+category]) \n",
    "            predictions_valid2 = pd.DataFrame(reg.predict(X_valid2), index=validation2_labels.index, columns=[\"pred_week_\" + str(numberOfOutputWeeks-1)+\"_\"+category])\n",
    "\n",
    "            valid1Output = validation1_labels[[\"output_\"+category+\"_\"+str(numberOfOutputWeeks-1)]]\n",
    "            valid2Output = validation2_labels[[\"output_\"+category+\"_\"+str(numberOfOutputWeeks-1)]]\n",
    "\n",
    "            rmse1 = np.sqrt(mean_squared_error(predictions_valid1[\"pred_week_\" + str(numberOfOutputWeeks-1)+\"_\"+category], valid1Output.values))\n",
    "            rmse2 = np.sqrt(mean_squared_error(predictions_valid2[\"pred_week_\" + str(numberOfOutputWeeks-1)+\"_\"+category], valid2Output.values))\n",
    "\n",
    "            d = {'model':[reg],'model type': [\"sklearn_single_output\"],'target':[category],'week':[numberOfOutputWeeks-1], 'model rsme 1':[rmse1], 'model rsme 2':[rmse2]}\n",
    "            df = pd.DataFrame(data=d)\n",
    "            df.to_csv(\"results.csv\",mode='a', header=False, index=False)\n",
    "\n",
    "        logging.info(str(estimator)+\" (%s) \" % (time.time() - estimatorTimer))\n",
    "\n",
    "    logging.info(\"============== finished fitting normal sklearn estimators ============== (%s) \" % (time.time() - typeTimer))           \n",
    "\n",
    "    # ------------------------------------------------------------------------------------------------                       \n",
    "\n",
    "    typeTimer = time.time()\n",
    "    logging.info(\"============== fitting single output keras estimators ============== \")\n",
    "    start = time.time()\n",
    "    # fitting single output keras estimators\n",
    "\n",
    "    numberOfOutputsForMultiOutput = len(outputCategories)\n",
    "    epochs = 1000\n",
    "    batch_size = 32 #16\n",
    "\n",
    "    def getNormalKerasModel(name, learningrate, nrOfNeurons, isMultiOutput):\n",
    "        input_ = keras.layers.Input(shape=X_train.shape[1:])\n",
    "        hidden1 = keras.layers.Dense(nrOfNeurons, activation=\"relu\")(input_)\n",
    "        output = keras.layers.Dense(numberOfOutputsForMultiOutput)(hidden1) if isMultiOutput else keras.layers.Dense(1)(hidden1)\n",
    "        model = keras.Model(name=name, inputs=[input_],outputs=[output])\n",
    "        model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.Adam(learning_rate=learningrate))\n",
    "        return model\n",
    "\n",
    "\n",
    "    def getDropoutKerasModel(name, learningrate, nrOfNeurons, isMultiOutput):\n",
    "        input_ = keras.layers.Input(shape=X_train.shape[1:])\n",
    "        hidden1 = keras.layers.Dense(nrOfNeurons, activation=\"relu\")(input_)\n",
    "        dropout1 = keras.layers.Dropout(rate=0.5)(hidden1)\n",
    "        output = keras.layers.Dense(numberOfOutputsForMultiOutput)(dropout1) if isMultiOutput else keras.layers.Dense(1)(dropout1)\n",
    "        model = keras.Model(name=name, inputs=[input_],outputs=[output])\n",
    "        model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.Adam(learning_rate=learningrate))\n",
    "        return model\n",
    "\n",
    "    def getRegularizedKerasModel(name, learningrate, nrOfNeurons, alpha, isMultiOutput):\n",
    "        input_ = keras.layers.Input(shape=X_train.shape[1:])\n",
    "        hidden1 = keras.layers.Dense(nrOfNeurons, activation=\"relu\", kernel_regularizer=keras.regularizers.l2(alpha))(input_)\n",
    "        output = keras.layers.Dense(numberOfOutputsForMultiOutput)(hidden1) if isMultiOutput else keras.layers.Dense(1)(hidden1)\n",
    "        model = keras.Model(name=name, inputs=[input_],outputs=[output])\n",
    "        model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.Adam(learning_rate=learningrate))\n",
    "        return model\n",
    "\n",
    "    def getTwoWayKerasModel(name, learningrate, nrOfNeurons, isMultiOutput):\n",
    "        input_ = keras.layers.Input(shape=X_train.shape[1:])\n",
    "        hidden1 = keras.layers.Dense(nrOfNeurons, activation=\"relu\")(input_)\n",
    "        concat = keras.layers.Concatenate()([input_,hidden1])\n",
    "        output = keras.layers.Dense(numberOfOutputsForMultiOutput)(concat) if isMultiOutput else keras.layers.Dense(1)(concat)\n",
    "        model = keras.Model(name=name, inputs=[input_],outputs=[output])\n",
    "        model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.Adam(learning_rate=learningrate))\n",
    "        return model\n",
    "\n",
    "    def getComplexKerasModel(name, learningrate, nrOfNeurons1, nrOfNeurons2, isMultiOutput):\n",
    "        input_ = keras.layers.Input(shape=X_train.shape[1:])\n",
    "        hidden1 = keras.layers.Dense(nrOfNeurons1, activation=\"relu\")(input_)\n",
    "        hidden2 = keras.layers.Dense(nrOfNeurons2, activation=\"relu\")(hidden1)\n",
    "        output = keras.layers.Dense(numberOfOutputsForMultiOutput)(hidden1) if isMultiOutput else keras.layers.Dense(1)(hidden2)\n",
    "        model = keras.Model(name=name, inputs=[input_],outputs=[output])\n",
    "        model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.Adam(learning_rate=learningrate))\n",
    "        return model\n",
    "\n",
    "    def getTwoWayComplexDropoutKerasModel(name, learningrate, nrOfNeurons1, nrOfNeurons2, nrOfNeurons3, nrOfNeurons4, nrOfNeurons5, isMultiOutput):\n",
    "        input_ = keras.layers.Input(shape=X_train.shape[1:])\n",
    "        hidden1 = keras.layers.Dense(nrOfNeurons1, activation=\"relu\")(input_)\n",
    "        dropout1 = keras.layers.Dropout(rate=0.5)(hidden1)\n",
    "        hidden2 = keras.layers.Dense(nrOfNeurons2, activation=\"relu\")(hidden1)\n",
    "        dropout1 = keras.layers.Dropout(rate=0.5)(hidden2)\n",
    "        hidden3 = keras.layers.Dense(nrOfNeurons3, activation=\"relu\")(hidden2)\n",
    "        dropout1 = keras.layers.Dropout(rate=0.5)(hidden3)\n",
    "        hidden4 = keras.layers.Dense(nrOfNeurons4, activation=\"relu\")(hidden3)\n",
    "        dropout1 = keras.layers.Dropout(rate=0.5)(hidden4)\n",
    "        hidden5 = keras.layers.Dense(nrOfNeurons5, activation=\"relu\")(hidden4)\n",
    "        dropout1 = keras.layers.Dropout(rate=0.5)(hidden5)\n",
    "        concat = keras.layers.Concatenate()([input_,dropout1])\n",
    "        output = keras.layers.Dense(numberOfOutputsForMultiOutput)(concat) if isMultiOutput else keras.layers.Dense(1)(concat)\n",
    "        model = keras.Model(name=name, inputs=[input_],outputs=[output])\n",
    "        model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.Adam(learning_rate=learningrate))\n",
    "        return model\n",
    "    \n",
    "    def getDeepDropoutKerasModel(name, learningrate, nrOfNeurons, isMultiOutput):\n",
    "        input_ = keras.layers.Input(shape=X_train.shape[1:])\n",
    "        dropout0 = keras.layers.Dropout(rate=0.3)(input_)\n",
    "        hidden1 = keras.layers.Dense(nrOfNeurons, activation=\"relu\")(dropout0)\n",
    "        dropout1 = keras.layers.Dropout(rate=0.3)(hidden1)\n",
    "        hidden2 = keras.layers.Dense(nrOfNeurons, activation=\"relu\")(dropout1)\n",
    "        dropout2 = keras.layers.Dropout(rate=0.3)(hidden2)\n",
    "        hidden3 = keras.layers.Dense(nrOfNeurons, activation=\"relu\")(dropout2)\n",
    "        dropout3 = keras.layers.Dropout(rate=0.3)(hidden3)\n",
    "        hidden4 = keras.layers.Dense(nrOfNeurons, activation=\"relu\")(dropout3)\n",
    "        dropout4 = keras.layers.Dropout(rate=0.3)(hidden4)\n",
    "        hidden5 = keras.layers.Dense(nrOfNeurons, activation=\"relu\")(dropout4)\n",
    "        dropout5 = keras.layers.Dropout(rate=0.3)(hidden5)\n",
    "        hidden6 = keras.layers.Dense(nrOfNeurons, activation=\"relu\")(dropout5)\n",
    "        dropout6 = keras.layers.Dropout(rate=0.3)(hidden6)\n",
    "        hidden7 = keras.layers.Dense(nrOfNeurons, activation=\"relu\")(dropout6)\n",
    "        dropout7 = keras.layers.Dropout(rate=0.3)(hidden7)\n",
    "        hidden8 = keras.layers.Dense(nrOfNeurons, activation=\"relu\")(dropout7)\n",
    "        dropout8 = keras.layers.Dropout(rate=0.3)(hidden8)\n",
    "        output = keras.layers.Dense(numberOfOutputsForMultiOutput)(dropout8) if isMultiOutput else keras.layers.Dense(1)(dropout8)\n",
    "        model = keras.Model(name=name, inputs=[input_],outputs=[output])\n",
    "        model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.Adam(learning_rate=learningrate))\n",
    "        return model\n",
    "\n",
    "\n",
    "    estimators = []\n",
    "\n",
    "    estimators.append(getNormalKerasModel(\"simple\", 0.0001, 10, False))\n",
    "\n",
    "    if isSuperComputer:\n",
    "        # all all estimators which we want to search through\n",
    "        estimators = []\n",
    "        \n",
    "        '''\n",
    "        for learningrate in [0.001,0.0001,0.00001]:\n",
    "            estimators.append(getNormalKerasModel(\"normal_learningrate_\"+str(learningrate)+\"_batchsize_\"+str(batch_size), learningrate, 100, False))\n",
    "            estimators.append(getDropoutKerasModel(\"dropout_learningrate_\"+str(learningrate)+\"_batchsize_\"+str(batch_size), learningrate, 100, False))\n",
    "            estimators.append(getTwoWayKerasModel(\"twoway_learningrate_\"+str(learningrate)+\"_batchsize_\"+str(batch_size), learningrate, 100, False))\n",
    "            estimators.append(getComplexKerasModel(\"complex_learningrate_\"+str(learningrate)+\"_batchsize_\"+str(batch_size), learningrate, 100, 10, False))\n",
    "            estimators.append(getTwoWayComplexDropoutKerasModel(\"twowaycomplexdropout_learningrate_\"+str(learningrate)+\"_batchsize_\"+str(batch_size), learningrate, 200, 150, 100, 50, 10, False))\n",
    "\n",
    "            for alpha in [0.0001, 0.001, 0.01, 0.1]: \n",
    "                estimators.append(getRegularizedKerasModel(\"regularized_learningrate_\"+str(learningrate)+\"_alpha_\"+str(alpha)+\"_batchsize_\"+str(batch_size), learningrate, 100, alpha, False))     \n",
    "        '''\n",
    "        for learningrate in [0.001,0.0001,0.00001]:\n",
    "            estimators.append(getDeepDropoutKerasModel(\"DeepDropout\"+str(learningrate)+\"_batchsize_\"+str(batch_size)+\"_neuronr_\"+str(100), learningrate, 100, False))\n",
    "            estimators.append(getNormalKerasModel(\"normal_learningrate_\"+str(learningrate)+\"_batchsize_\"+str(batch_size)+\"_neuronr_\"+str(200), learningrate, 200, False))\n",
    "            estimators.append(getDropoutKerasModel(\"dropout_learningrate_\"+str(learningrate)+\"_batchsize_\"+str(batch_size)+\"_neuronr_\"+str(200), learningrate, 200, False))\n",
    "            estimators.append(getTwoWayKerasModel(\"twoway_learningrate_\"+str(learningrate)+\"_batchsize_\"+str(batch_size)+\"_neuronr_\"+str(200), learningrate, 200, False))\n",
    "            estimators.append(getComplexKerasModel(\"complex_learningrate_\"+str(learningrate)+\"_batchsize_\"+str(batch_size)+\"_neuronr_\"+str(200)+\"_\"+str(20), learningrate, 200, 20, False))\n",
    "            estimators.append(getTwoWayComplexDropoutKerasModel(\"twowaycomplexdropout_learningrate_\"+str(learningrate)+\"_batchsize_\"+str(batch_size)+\"_neuronr_\"+str(300)+\"_\"+str(150)+\"_\"+str(100)+\"_\"+str(50)+\"_\"+str(20), learningrate, 300, 150, 100, 50, 20, False))\n",
    "\n",
    "            for alpha in [0.0001, 0.001, 0.01, 0.1]: \n",
    "                estimators.append(getRegularizedKerasModel(\"regularized_learningrate_\"+str(learningrate)+\"_alpha_\"+str(alpha)+\"_batchsize_\"+str(batch_size)+\"_neuronr_\"+str(200), learningrate, 200, alpha, False)) \n",
    "\n",
    "\n",
    "    logging.info(\"number of estimators to fit: \" + str(len(estimators)*len(outputCategories)))\n",
    "\n",
    "    while len(estimators) > 0:\n",
    "        estimator = estimators.pop(0)\n",
    "        estimatorTimer = time.time()\n",
    "\n",
    "        logging.info(str(estimator.name))\n",
    "        for category in outputCategories:\n",
    "\n",
    "            logging.info(\"-- \" + category)\n",
    "\n",
    "            y_train = train_labels[[\"output_\"+category+\"_\"+str(numberOfOutputWeeks-1)]].values \n",
    "            y_valid1 = validation1_labels[[\"output_\"+category+\"_\"+str(numberOfOutputWeeks-1)]].values \n",
    "            estimator.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=0, validation_data=(X_valid1,y_valid1), callbacks=[keras.callbacks.EarlyStopping(patience=10)])\n",
    "            reg = estimator\n",
    "\n",
    "            # save the fitted regressor\n",
    "            #reg.save('models/'+str(modelId))\n",
    "            #reg = keras.models.load_model('models/'+str(modelId))\n",
    "            modelId = modelId + 1\n",
    "\n",
    "            predictions_valid1 = pd.DataFrame(reg.predict(X_valid1), index=validation1_labels.index, columns=[\"pred_week_\" + str(numberOfOutputWeeks-1)+\"_\"+category]) \n",
    "            predictions_valid2 = pd.DataFrame(reg.predict(X_valid2), index=validation2_labels.index, columns=[\"pred_week_\" + str(numberOfOutputWeeks-1)+\"_\"+category])\n",
    "\n",
    "            valid1Output = validation1_labels[[\"output_\"+category+\"_\"+str(numberOfOutputWeeks-1)]]\n",
    "            valid2Output = validation2_labels[[\"output_\"+category+\"_\"+str(numberOfOutputWeeks-1)]]\n",
    "\n",
    "\n",
    "            rmse1 = np.sqrt(mean_squared_error(predictions_valid1[\"pred_week_\" + str(numberOfOutputWeeks-1)+\"_\"+category], valid1Output.values))\n",
    "            rmse2 = np.sqrt(mean_squared_error(predictions_valid2[\"pred_week_\" + str(numberOfOutputWeeks-1)+\"_\"+category], valid2Output.values))\n",
    "\n",
    "            d = {'model':[reg.name],'model type': [\"keras_single_output\"],'target':[category],'week':[numberOfOutputWeeks-1], 'model rsme 1':[rmse1], 'model rsme 2':[rmse2]}\n",
    "            df = pd.DataFrame(data=d)\n",
    "            df.to_csv(\"results.csv\",mode='a', header=False, index=False)\n",
    "\n",
    "        logging.info(str(estimator.name)+\" (%s)\" % (time.time() - estimatorTimer))\n",
    "\n",
    "    logging.info(\"============== finished fitting single output keras estimators ============== (%s) \" % (time.time() - typeTimer))   \n",
    "\n",
    "    # ----------------------------------------------------------------------------------------------------\n",
    "\n",
    "    typeTimer = time.time()\n",
    "    logging.info(\"============== fitting multi output keras estimators ============== \")\n",
    "\n",
    "    estimators = []\n",
    "\n",
    "    estimators.append(getNormalKerasModel(\"simple\", 0.0001, 10, True))\n",
    "\n",
    "    if isSuperComputer:\n",
    "        # all all estimators which we want to search through\n",
    "        estimators = []\n",
    "        '''\n",
    "        for learningrate in [0.001,0.0001,0.00001]:\n",
    "            estimators.append(getNormalKerasModel(\"normal_learningrate_\"+str(learningrate), learningrate, 100, True))\n",
    "            estimators.append(getDropoutKerasModel(\"dropout_learningrate_\"+str(learningrate), learningrate, 100, True))\n",
    "            estimators.append(getTwoWayKerasModel(\"twoway_learningrate_\"+str(learningrate), learningrate, 100, True))\n",
    "            estimators.append(getComplexKerasModel(\"complex_learningrate_\"+str(learningrate), learningrate, 100, 10, True))\n",
    "            estimators.append(getTwoWayComplexDropoutKerasModel(\"twowaycomplexdropout_learningrate_\"+str(learningrate)+\"_batchsize_\"+str(batch_size), learningrate, 200, 150, 100, 50, 10, True))\n",
    "\n",
    "            for alpha in [0.01, 0.001, 0.0001]: #0.1\n",
    "                estimators.append(getRegularizedKerasModel(\"regularized_learningrate_\"+str(learningrate)+\"_alpha_\"+str(alpha)+\"_batchsize_\"+str(batch_size), learningrate, 100, alpha, True))     \n",
    "\n",
    "        '''\n",
    "        for learningrate in [0.001,0.0001,0.00001]:\n",
    "                estimators.append(getDeepDropoutKerasModel(\"DeepDropout\"+str(learningrate)+\"_batchsize_\"+str(batch_size)+\"_neuronr_\"+str(100), learningrate, 100, True))\n",
    "                estimators.append(getNormalKerasModel(\"normal_learningrate_\"+str(learningrate)+\"_batchsize_\"+str(batch_size)+\"_neuronr_\"+str(200), learningrate, 200, True))\n",
    "                estimators.append(getDropoutKerasModel(\"dropout_learningrate_\"+str(learningrate)+\"_batchsize_\"+str(batch_size)+\"_neuronr_\"+str(200), learningrate, 200, True))\n",
    "                estimators.append(getTwoWayKerasModel(\"twoway_learningrate_\"+str(learningrate)+\"_batchsize_\"+str(batch_size)+\"_neuronr_\"+str(200), learningrate, 200, True))\n",
    "                estimators.append(getComplexKerasModel(\"complex_learningrate_\"+str(learningrate)+\"_batchsize_\"+str(batch_size)+\"_neuronr_\"+str(200)+\"_\"+str(20), learningrate, 200, 20, True))\n",
    "                estimators.append(getTwoWayComplexDropoutKerasModel(\"twowaycomplexdropout_learningrate_\"+str(learningrate)+\"_batchsize_\"+str(batch_size)+\"_neuronr_\"+str(300)+\"_\"+str(150)+\"_\"+str(100)+\"_\"+str(50)+\"_\"+str(20), learningrate, 300, 150, 100, 50, 20, True))\n",
    "\n",
    "                for alpha in [0.0001, 0.001, 0.01, 0.1]: \n",
    "                    estimators.append(getRegularizedKerasModel(\"regularized_learningrate_\"+str(learningrate)+\"_alpha_\"+str(alpha)+\"_batchsize_\"+str(batch_size)+\"_neuronr_\"+str(200), learningrate, 200, alpha, True))             \n",
    "\n",
    "    \n",
    "    logging.info(\"number of estimators to fit: \" + str(len(estimators)))\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    while len(estimators) > 0:\n",
    "        estimator = estimators.pop(0)\n",
    "        estimatorTimer = time.time()\n",
    "        logging.info(str(estimator.name))\n",
    "        logging.info(\"-- all categories\")\n",
    "\n",
    "\n",
    "        y_train = train_labels[[\"output_\"+category+\"_\"+str(numberOfOutputWeeks-1) for category in outputCategories]].values\n",
    "        y_valid1 = validation1_labels[[\"output_\"+category+\"_\"+str(numberOfOutputWeeks-1) for category in outputCategories]].values \n",
    "        estimator.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=0, validation_data=(X_valid1,y_valid1), callbacks=[keras.callbacks.EarlyStopping(patience=10)])\n",
    "        reg = estimator\n",
    "\n",
    "        # save the fitted regressor\n",
    "        #reg.save('models/'+str(modelId))\n",
    "        #reg = keras.models.load_model('models/'+str(modelId))\n",
    "        modelId = modelId + 1\n",
    "\n",
    "        predictions_valid1 = pd.DataFrame(reg.predict(X_valid1), index=validation1_labels.index, columns=[\"pred_week_\" + str(numberOfOutputWeeks-1)+\"_\"+category for category in outputCategories]) \n",
    "        predictions_valid2 = pd.DataFrame(reg.predict(X_valid2), index=validation2_labels.index, columns=[\"pred_week_\" + str(numberOfOutputWeeks-1)+\"_\"+category for category in outputCategories])\n",
    "\n",
    "        valid1Output = validation1_labels[[\"output_\"+category+\"_\"+str(numberOfOutputWeeks-1) for category in outputCategories]]\n",
    "        valid2Output = validation2_labels[[\"output_\"+category+\"_\"+str(numberOfOutputWeeks-1) for category in outputCategories]]\n",
    "\n",
    "        for category in outputCategories:\n",
    "            rmse1 = np.sqrt(mean_squared_error(predictions_valid1[\"pred_week_\" + str(numberOfOutputWeeks-1)+\"_\"+category], valid1Output[\"output_\"+category+\"_\"+str(numberOfOutputWeeks-1)]))\n",
    "            rmse2 = np.sqrt(mean_squared_error(predictions_valid2[\"pred_week_\" + str(numberOfOutputWeeks-1)+\"_\"+category], valid2Output[\"output_\"+category+\"_\"+str(numberOfOutputWeeks-1)]))\n",
    "\n",
    "            d = {'model':[reg.name],'model type': [\"keras_multi_output\"],'target':[category],'week':[numberOfOutputWeeks-1], 'model rsme 1':[rmse1], 'model rsme 2':[rmse2]}\n",
    "            df = pd.DataFrame(data=d)\n",
    "            df.to_csv(\"results.csv\",mode='a', header=False, index=False)\n",
    "\n",
    "        logging.info(str(estimator.name)+\" (%s)\" % (time.time() - estimatorTimer))\n",
    "\n",
    "    logging.info(\"============== finished fitting multi output keras estimators ============== (%s) \" % (time.time() - typeTimer))\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------------------\n",
    "    typeTimer = time.time()\n",
    "\n",
    "    logging.info(\"============== fitting multioutput sklearn estimators ============== \")\n",
    "    # fitting multioutput sklearn estimators\n",
    "    estimators = [\n",
    "        linear_model.MultiTaskLasso(),\n",
    "    ]\n",
    "    if isSuperComputer:\n",
    "        # all all estimators which we want to search through\n",
    "\n",
    "        estimators = []\n",
    "        \n",
    "        estimators.append(linear_model.Ridge(alpha=100))\n",
    "        estimators.append(linear_model.Ridge(alpha=1000))\n",
    "\n",
    "        '''\n",
    "        kernels = ['linear','poly','polynomial','rbf','laplacian','sigmoid','cosine']\n",
    "        for alpha in [0.01,0.1,10]:\n",
    "            for k in kernels:\n",
    "                estimators.append(KernelRidge(kernel=k, alpha=alpha))\n",
    "\n",
    "        for alpha in [0.00001, 0.0001,0.001,0.01,0.1,1,10]: \n",
    "            estimators.append(linear_model.MultiTaskLasso(alpha=alpha))\n",
    "        \n",
    "        for alpha in [0.00001, 0.0001,0.001,0.01,0.1,0,1,10]: \n",
    "            estimators.append(linear_model.Lasso(alpha=alpha))\n",
    "            estimators.append(linear_model.Ridge(alpha=alpha))\n",
    "\n",
    "        for alpha in [0.00001, 0.0001,0.001,0.01,0.1,1,10]:\n",
    "            for layer in [\n",
    "                (20),\n",
    "                (100),\n",
    "                (20,20),\n",
    "                (100,10),\n",
    "                (100,50,10),\n",
    "                (100,100,10),\n",
    "            ]:\n",
    "                estimators.append(MLPRegressor(hidden_layer_sizes=layer, alpha=alpha, max_iter=1000))\n",
    "\n",
    "        for nrOfEstimators in [100,200,500]: #1000\n",
    "            for maxdepth in [None, 2, 4]:\n",
    "                for mss in [2,3,4]:\n",
    "                    for msl in [1,2,3]:\n",
    "                        estimators.append(RandomForestRegressor(n_estimators=nrOfEstimators, \n",
    "                                                                max_depth=maxdepth, \n",
    "                                                                min_samples_split=mss, \n",
    "                                                                min_samples_leaf=msl,\n",
    "                                                                n_jobs=-1))\n",
    "\n",
    "        '''\n",
    "    logging.info(\"number of estimators to fit: \" + str(len(estimators))+\" \")\n",
    "\n",
    "    while len(estimators) > 0:\n",
    "        estimator = estimators.pop(0)\n",
    "        estimatorTimer = time.time()\n",
    "        logging.info(str(estimator))\n",
    "        logging.info(\"-- all categories\")\n",
    "\n",
    "\n",
    "        trainOutput = train_labels[[\"output_\"+category+\"_\"+str(numberOfOutputWeeks-1) for category in outputCategories]]\n",
    "        reg = estimator.fit(X_train, trainOutput.values)\n",
    "        # save the fitted regressor\n",
    "        #dump(reg, 'models/'+str(modelId)+'.joblib') \n",
    "        #reg = load('models/'+str(modelId)+'.joblib') \n",
    "        modelId = modelId + 1\n",
    "\n",
    "        predictions_valid1 = pd.DataFrame(reg.predict(X_valid1), index=validation1_labels.index, columns=[\"pred_week_\" + str(numberOfOutputWeeks-1)+\"_\"+category for category in outputCategories]) \n",
    "        predictions_valid2 = pd.DataFrame(reg.predict(X_valid2), index=validation2_labels.index, columns=[\"pred_week_\" + str(numberOfOutputWeeks-1)+\"_\"+category for category in outputCategories])\n",
    "\n",
    "        valid1Output = validation1_labels[[\"output_\"+category+\"_\"+str(numberOfOutputWeeks-1) for category in outputCategories]]\n",
    "        valid2Output = validation2_labels[[\"output_\"+category+\"_\"+str(numberOfOutputWeeks-1) for category in outputCategories]]\n",
    "\n",
    "        for category in outputCategories:\n",
    "            rmse1 = np.sqrt(mean_squared_error(predictions_valid1[\"pred_week_\" + str(numberOfOutputWeeks-1)+\"_\"+category], valid1Output[\"output_\"+category+\"_\"+str(numberOfOutputWeeks-1)]))\n",
    "            rmse2 = np.sqrt(mean_squared_error(predictions_valid2[\"pred_week_\" + str(numberOfOutputWeeks-1)+\"_\"+category], valid2Output[\"output_\"+category+\"_\"+str(numberOfOutputWeeks-1)]))\n",
    "\n",
    "            d = {'model':[reg],'model type': [\"sklearn_multi_output\"],'target':[category],'week':[numberOfOutputWeeks-1], 'model rsme 1':[rmse1], 'model rsme 2':[rmse2]}\n",
    "            df = pd.DataFrame(data=d)\n",
    "            df.to_csv(\"results.csv\",mode='a', header=False, index=False)\n",
    "\n",
    "        logging.info(str(estimator)+\" (%s) \" % (time.time() - estimatorTimer))\n",
    "\n",
    "    logging.info(\"============== finished fitting multioutput sklearn estimators ============== (%s) \" % (time.time() - typeTimer))\n",
    "\n",
    "    # -----------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "    logging.info(\"============== finished ============== %s \" % (time.time() - completeTimer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dirty-concert",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "original-bruce",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpha-affairs",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "through-tuesday",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concrete-leone",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "based-competition",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smoking-software",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diverse-nigeria",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improving-execution",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concerned-differential",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interesting-forum",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic-viking",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improving-pattern",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
