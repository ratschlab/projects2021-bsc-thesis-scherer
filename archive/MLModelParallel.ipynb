{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "familiar-repair",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time\n",
    "import logging\n",
    "from joblib import dump, load\n",
    "from datetime import timedelta\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.multioutput import RegressorChain\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import xgboost as xgb\n",
    "import lightgbm as lightgbm\n",
    "\n",
    "# settings:\n",
    "numberOfInputWeeks = 3 # must be equal to the number of input weeks set in data preperator\n",
    "numberOfOutputWeeks = 4 # must be equal to the number of output week set in data preperator\n",
    "\n",
    "\n",
    "\n",
    "# data preperation\n",
    "cantonKeys = ['AG','AI','AR', 'BE', 'BL', 'BS', 'FR', 'GE', 'GL', 'GR', 'JU', 'LU', 'NE', 'NW', 'OW', 'SG', 'SH', 'SO', 'SZ', 'TG', 'TI', 'UR', 'VD', 'VS', 'ZG','ZH']\n",
    "data = pd.read_csv(\"completedata.csv\")\n",
    "\n",
    "\n",
    "outputCategories = ['case_inz_entries_7dayAverage',\n",
    "                  'hosp_inz_entries_7dayAverage',\n",
    "                  'death_inz_entries_7dayAverage',\n",
    "                  'testPositvity',\n",
    "                  'testPositvity_7dayAverageBoth',\n",
    "                  'testPositvity_7dayAverageSeperate',\n",
    "                  'testPositvity_7dayAverageDivision',\n",
    "                  'test_entries_7dayAverage',\n",
    "                  'transit_stations_percent_change_from_baseline_7dayAverage',\n",
    "                  'workplaces_percent_change_from_baseline_7dayAverage'\n",
    "                   ]\n",
    "numberOfOutputs = len(outputCategories)\n",
    "\n",
    "split = numberOfOutputs * numberOfOutputWeeks + 2\n",
    "train_features = data[data['category']=='train'].iloc[:,0:-split].drop(['lastInputDay'], axis=1)\n",
    "train_labels = data[data['category']=='train'].iloc[:,-split:-2]\n",
    "validation1_features = data[data['category']=='validation 1'].iloc[:,0:-split].drop(['lastInputDay'], axis=1)\n",
    "validation1_labels = data[data['category']=='validation 1'].iloc[:,-split:-2]\n",
    "validation2_features = data[data['category']=='validation 2'].iloc[:,0:-split].drop(['lastInputDay'], axis=1)\n",
    "validation2_labels = data[data['category']=='validation 2'].iloc[:,-split:-2]\n",
    "validation1And2_labels = data[(data['category']=='validation 1') | (data['category']=='validation 2')].iloc[:,-split:-2]\n",
    "validation1And2_features = data[(data['category']=='validation 1') | (data['category']=='validation 2')].iloc[:,0:-split].drop(['lastInputDay'], axis=1)\n",
    "\n",
    "pip = Pipeline([('minmax_scaler', MinMaxScaler())])\n",
    "X_train = pip.fit_transform(train_features[train_features.columns].values)\n",
    "X_valid1 = pip.transform(validation1_features[train_features.columns].values)\n",
    "X_valid2 = pip.transform(validation2_features[train_features.columns].values)\n",
    "X_valid1And2 = pip.transform(validation1And2_features[train_features.columns].values)\n",
    "\n",
    "\n",
    "def generic1(learningrate, hiddenLayers, dropout, l1, l2, isTwoWay, isMultiOutput):\n",
    "    input_ = keras.layers.Input(shape=X_train.shape[1:])\n",
    "    dropout0 = keras.layers.Dropout(rate=dropout)(input_)\n",
    "    \n",
    "    hidden0 = keras.layers.Dense(hiddenLayers[0], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2))(dropout0)\n",
    "    dropout1 = keras.layers.Dropout(rate=dropout)(hidden0)\n",
    "    \n",
    "    lastHidden = dropout1\n",
    "    concat = keras.layers.Concatenate()([input_,lastHidden]) if isTwoWay else lastHidden\n",
    "    output = keras.layers.Dense(numberOfOutputWeeks)(concat) if isMultiOutput else keras.layers.Dense(1)(concat)\n",
    "    model = keras.Model(inputs=[input_],outputs=[output])\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.Adam(learning_rate=learningrate))\n",
    "    return model\n",
    "\n",
    "def generic2(learningrate, hiddenLayers, dropout, l1, l2, isTwoWay, isMultiOutput):\n",
    "    input_ = keras.layers.Input(shape=X_train.shape[1:])\n",
    "    dropout0 = keras.layers.Dropout(rate=dropout)(input_)\n",
    "    \n",
    "    hidden0 = keras.layers.Dense(hiddenLayers[0], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2))(dropout0)\n",
    "    dropout1 = keras.layers.Dropout(rate=dropout)(hidden0)\n",
    "    hidden1 = keras.layers.Dense(hiddenLayers[1], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2))(dropout1)\n",
    "    dropout2 = keras.layers.Dropout(rate=dropout)(hidden1)\n",
    "    \n",
    "    lastHidden = dropout2\n",
    "    concat = keras.layers.Concatenate()([input_,lastHidden]) if isTwoWay else lastHidden\n",
    "    output = keras.layers.Dense(numberOfOutputWeeks)(concat) if isMultiOutput else keras.layers.Dense(1)(concat)\n",
    "    model = keras.Model(inputs=[input_],outputs=[output])\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.Adam(learning_rate=learningrate))\n",
    "    return model\n",
    "\n",
    "def generic3(learningrate, hiddenLayers, dropout, l1, l2, isTwoWay, isMultiOutput):\n",
    "    input_ = keras.layers.Input(shape=X_train.shape[1:])\n",
    "    dropout0 = keras.layers.Dropout(rate=dropout)(input_)\n",
    "    \n",
    "    hidden0 = keras.layers.Dense(hiddenLayers[0], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2))(dropout0)\n",
    "    dropout1 = keras.layers.Dropout(rate=dropout)(hidden0)\n",
    "    hidden1 = keras.layers.Dense(hiddenLayers[1], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2))(dropout1)\n",
    "    dropout2 = keras.layers.Dropout(rate=dropout)(hidden1)\n",
    "    hidden2 = keras.layers.Dense(hiddenLayers[2], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2))(dropout2)\n",
    "    dropout3 = keras.layers.Dropout(rate=dropout)(hidden2)\n",
    "    \n",
    "    lastHidden = dropout3\n",
    "    concat = keras.layers.Concatenate()([input_,lastHidden]) if isTwoWay else lastHidden\n",
    "    output = keras.layers.Dense(numberOfOutputWeeks)(concat) if isMultiOutput else keras.layers.Dense(1)(concat)\n",
    "    model = keras.Model(inputs=[input_],outputs=[output])\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.Adam(learning_rate=learningrate))\n",
    "    return model\n",
    "\n",
    "def generic4(learningrate, hiddenLayers, dropout, l1, l2, isTwoWay, isMultiOutput):\n",
    "    input_ = keras.layers.Input(shape=X_train.shape[1:])\n",
    "    dropout0 = keras.layers.Dropout(rate=dropout)(input_)\n",
    "    \n",
    "    hidden0 = keras.layers.Dense(hiddenLayers[0], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2))(dropout0)\n",
    "    dropout1 = keras.layers.Dropout(rate=dropout)(hidden0)\n",
    "    hidden1 = keras.layers.Dense(hiddenLayers[1], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2))(dropout1)\n",
    "    dropout2 = keras.layers.Dropout(rate=dropout)(hidden1)\n",
    "    hidden2 = keras.layers.Dense(hiddenLayers[2], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2))(dropout2)\n",
    "    dropout3 = keras.layers.Dropout(rate=dropout)(hidden2)\n",
    "    hidden3 = keras.layers.Dense(hiddenLayers[3], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2))(dropout3)\n",
    "    dropout4 = keras.layers.Dropout(rate=dropout)(hidden3)\n",
    "    \n",
    "    lastHidden = dropout4\n",
    "    concat = keras.layers.Concatenate()([input_,lastHidden]) if isTwoWay else lastHidden\n",
    "    output = keras.layers.Dense(numberOfOutputWeeks)(concat) if isMultiOutput else keras.layers.Dense(1)(concat)\n",
    "    model = keras.Model(inputs=[input_],outputs=[output])\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.Adam(learning_rate=learningrate))\n",
    "    return model\n",
    "\n",
    "def generic5(learningrate, hiddenLayers, dropout, l1, l2, isTwoWay, isMultiOutput):\n",
    "    input_ = keras.layers.Input(shape=X_train.shape[1:])\n",
    "    dropout0 = keras.layers.Dropout(rate=dropout)(input_)\n",
    "    \n",
    "    hidden0 = keras.layers.Dense(hiddenLayers[0], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2))(dropout0)\n",
    "    dropout1 = keras.layers.Dropout(rate=dropout)(hidden0)\n",
    "    hidden1 = keras.layers.Dense(hiddenLayers[1], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2))(dropout1)\n",
    "    dropout2 = keras.layers.Dropout(rate=dropout)(hidden1)\n",
    "    hidden2 = keras.layers.Dense(hiddenLayers[2], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2))(dropout2)\n",
    "    dropout3 = keras.layers.Dropout(rate=dropout)(hidden2)\n",
    "    hidden3 = keras.layers.Dense(hiddenLayers[3], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2))(dropout3)\n",
    "    dropout4 = keras.layers.Dropout(rate=dropout)(hidden3)\n",
    "    hidden4 = keras.layers.Dense(hiddenLayers[4], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2))(dropout4)\n",
    "    dropout5 = keras.layers.Dropout(rate=dropout)(hidden4)\n",
    "    \n",
    "    lastHidden = dropout5\n",
    "    concat = keras.layers.Concatenate()([input_,lastHidden]) if isTwoWay else lastHidden\n",
    "    output = keras.layers.Dense(numberOfOutputWeeks)(concat) if isMultiOutput else keras.layers.Dense(1)(concat)\n",
    "    model = keras.Model(inputs=[input_],outputs=[output])\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.Adam(learning_rate=learningrate))\n",
    "    return model\n",
    "\n",
    "def generic6(learningrate, hiddenLayers, dropout, l1, l2, isTwoWay, isMultiOutput):\n",
    "    input_ = keras.layers.Input(shape=X_train.shape[1:])\n",
    "    dropout0 = keras.layers.Dropout(rate=dropout)(input_)\n",
    "    \n",
    "    hidden0 = keras.layers.Dense(hiddenLayers[0], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2))(dropout0)\n",
    "    dropout1 = keras.layers.Dropout(rate=dropout)(hidden0)\n",
    "    hidden1 = keras.layers.Dense(hiddenLayers[1], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2))(dropout1)\n",
    "    dropout2 = keras.layers.Dropout(rate=dropout)(hidden1)\n",
    "    hidden2 = keras.layers.Dense(hiddenLayers[2], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2))(dropout2)\n",
    "    dropout3 = keras.layers.Dropout(rate=dropout)(hidden2)\n",
    "    hidden3 = keras.layers.Dense(hiddenLayers[3], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2))(dropout3)\n",
    "    dropout4 = keras.layers.Dropout(rate=dropout)(hidden3)\n",
    "    hidden4 = keras.layers.Dense(hiddenLayers[4], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2))(dropout4)\n",
    "    dropout5 = keras.layers.Dropout(rate=dropout)(hidden4)\n",
    "    hidden5 = keras.layers.Dense(hiddenLayers[5], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2))(dropout5)\n",
    "    dropout6 = keras.layers.Dropout(rate=dropout)(hidden5)\n",
    "    \n",
    "    lastHidden = dropout6\n",
    "    concat = keras.layers.Concatenate()([input_,lastHidden]) if isTwoWay else lastHidden\n",
    "    output = keras.layers.Dense(numberOfOutputWeeks)(concat) if isMultiOutput else keras.layers.Dense(1)(concat)\n",
    "    model = keras.Model(inputs=[input_],outputs=[output])\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.Adam(learning_rate=learningrate))\n",
    "    return model\n",
    "\n",
    "def generic7(learningrate, hiddenLayers, dropout, l1, l2, isTwoWay, isMultiOutput):\n",
    "    input_ = keras.layers.Input(shape=X_train.shape[1:])\n",
    "    dropout0 = keras.layers.Dropout(rate=dropout)(input_)\n",
    "    \n",
    "    hidden0 = keras.layers.Dense(hiddenLayers[0], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2))(dropout0)\n",
    "    dropout1 = keras.layers.Dropout(rate=dropout)(hidden0)\n",
    "    hidden1 = keras.layers.Dense(hiddenLayers[1], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2))(dropout1)\n",
    "    dropout2 = keras.layers.Dropout(rate=dropout)(hidden1)\n",
    "    hidden2 = keras.layers.Dense(hiddenLayers[2], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2))(dropout2)\n",
    "    dropout3 = keras.layers.Dropout(rate=dropout)(hidden2)\n",
    "    hidden3 = keras.layers.Dense(hiddenLayers[3], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2))(dropout3)\n",
    "    dropout4 = keras.layers.Dropout(rate=dropout)(hidden3)\n",
    "    hidden4 = keras.layers.Dense(hiddenLayers[4], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2))(dropout4)\n",
    "    dropout5 = keras.layers.Dropout(rate=dropout)(hidden4)\n",
    "    hidden5 = keras.layers.Dense(hiddenLayers[5], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2))(dropout5)\n",
    "    dropout6 = keras.layers.Dropout(rate=dropout)(hidden5)\n",
    "    hidden6 = keras.layers.Dense(hiddenLayers[6], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2))(dropout6)\n",
    "    dropout7 = keras.layers.Dropout(rate=dropout)(hidden6)\n",
    "    \n",
    "    lastHidden = dropout7\n",
    "    concat = keras.layers.Concatenate()([input_,lastHidden]) if isTwoWay else lastHidden\n",
    "    output = keras.layers.Dense(numberOfOutputWeeks)(concat) if isMultiOutput else keras.layers.Dense(1)(concat)\n",
    "    model = keras.Model(inputs=[input_],outputs=[output])\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.Adam(learning_rate=learningrate))\n",
    "    return model\n",
    "\n",
    "def generic8(learningrate, hiddenLayers, dropout, l1, l2, isTwoWay, isMultiOutput):\n",
    "    input_ = keras.layers.Input(shape=X_train.shape[1:])\n",
    "    dropout0 = keras.layers.Dropout(rate=dropout)(input_)\n",
    "    \n",
    "    hidden0 = keras.layers.Dense(hiddenLayers[0], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2))(dropout0)\n",
    "    dropout1 = keras.layers.Dropout(rate=dropout)(hidden0)\n",
    "    hidden1 = keras.layers.Dense(hiddenLayers[1], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2))(dropout1)\n",
    "    dropout2 = keras.layers.Dropout(rate=dropout)(hidden1)\n",
    "    hidden2 = keras.layers.Dense(hiddenLayers[2], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2))(dropout2)\n",
    "    dropout3 = keras.layers.Dropout(rate=dropout)(hidden2)\n",
    "    hidden3 = keras.layers.Dense(hiddenLayers[3], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2))(dropout3)\n",
    "    dropout4 = keras.layers.Dropout(rate=dropout)(hidden3)\n",
    "    hidden4 = keras.layers.Dense(hiddenLayers[4], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2))(dropout4)\n",
    "    dropout5 = keras.layers.Dropout(rate=dropout)(hidden4)\n",
    "    hidden5 = keras.layers.Dense(hiddenLayers[5], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2))(dropout5)\n",
    "    dropout6 = keras.layers.Dropout(rate=dropout)(hidden5)\n",
    "    hidden6 = keras.layers.Dense(hiddenLayers[6], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2))(dropout6)\n",
    "    dropout7 = keras.layers.Dropout(rate=dropout)(hidden6)\n",
    "    hidden7 = keras.layers.Dense(hiddenLayers[7], activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2))(dropout7)\n",
    "    dropout8 = keras.layers.Dropout(rate=dropout)(hidden7)\n",
    "    \n",
    "    lastHidden = dropout8\n",
    "    concat = keras.layers.Concatenate()([input_,lastHidden]) if isTwoWay else lastHidden\n",
    "    output = keras.layers.Dense(numberOfOutputWeeks)(concat) if isMultiOutput else keras.layers.Dense(1)(concat)\n",
    "    model = keras.Model(inputs=[input_],outputs=[output])\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.Adam(learning_rate=learningrate))\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def genericKerasModel(learningrate, hiddenLayers, dropout, l1regularization, l2regularization, isTwoWay, isMultioutput):\n",
    "    if len(hiddenLayers) == 1:\n",
    "        return generic1(learningrate, hiddenLayers, dropout, l1regularization, l2regularization, isTwoWay, isMultioutput)\n",
    "    elif len(hiddenLayers) == 2:\n",
    "        return generic2(learningrate, hiddenLayers, dropout, l1regularization, l2regularization, isTwoWay, isMultioutput)\n",
    "    elif len(hiddenLayers) == 3:\n",
    "        return generic3(learningrate, hiddenLayers, dropout, l1regularization, l2regularization, isTwoWay, isMultioutput)\n",
    "    elif len(hiddenLayers) == 4:\n",
    "        return generic4(learningrate, hiddenLayers, dropout, l1regularization, l2regularization, isTwoWay, isMultioutput)\n",
    "    elif len(hiddenLayers) == 5:\n",
    "        return generic5(learningrate, hiddenLayers, dropout, l1regularization, l2regularization, isTwoWay, isMultioutput)\n",
    "    elif len(hiddenLayers) == 6:\n",
    "        return generic6(learningrate, hiddenLayers, dropout, l1regularization, l2regularization, isTwoWay, isMultioutput)\n",
    "    elif len(hiddenLayers) == 7:\n",
    "        return generic7(learningrate, hiddenLayers, dropout, l1regularization, l2regularization, isTwoWay, isMultioutput)\n",
    "    else:\n",
    "        return generic8(learningrate, hiddenLayers, dropout, l1regularization, l2regularization, isTwoWay, isMultioutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "serial-choice",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge: 18\n",
      "Lasso: 18\n",
      "KernelRidge: 126\n",
      "MultiTaskLasso: 9\n",
      "LGBM: 75\n",
      "XGB: 75\n",
      "SVR: 6\n",
      "SGD: 54\n",
      "RandomForrest: 216\n",
      "Keras: 19968\n",
      "Total: 20565\n"
     ]
    }
   ],
   "source": [
    "estimators = []\n",
    "numberOfEstimators = 0\n",
    "results = pd.DataFrame()\n",
    "\n",
    "\n",
    "\n",
    "for alpha in [0.00001, 0.0001,0.001,0.01,0.1,0,1,10, 100]:\n",
    "    for isMultiWeek in [True,False]:\n",
    "         estimators.append(\n",
    "                {\n",
    "                  \"modelClass\": \"Ridge\",\n",
    "                  \"isMultiWeek\": isMultiWeek,\n",
    "                  \"alpha\": alpha\n",
    "                })          \n",
    "print(\"Ridge: \"+str(len(estimators)-numberOfEstimators))\n",
    "numberOfEstimators = len(estimators)\n",
    "                    \n",
    "for alpha in [0.00001, 0.0001,0.001,0.01,0.1,0,1,10, 100]:\n",
    "    for isMultiWeek in [True,False]:\n",
    "         estimators.append(\n",
    "                {\n",
    "                  \"modelClass\": \"Lasso\",\n",
    "                  \"isMultiWeek\": isMultiWeek,\n",
    "                  \"alpha\": alpha\n",
    "                })\n",
    "print(\"Lasso: \"+str(len(estimators)-numberOfEstimators))\n",
    "numberOfEstimators = len(estimators)             \n",
    "             \n",
    "for alpha in [0.00001, 0.0001,0.001,0.01,0.1,0,1,10, 100]: \n",
    "    for isMultiWeek in [True,False]:\n",
    "        for kernel in ['linear','poly','polynomial','rbf','laplacian','sigmoid','cosine']:\n",
    "            estimators.append(\n",
    "            {\n",
    "              \"modelClass\": \"KernelRidge\",\n",
    "              \"isMultiWeek\": isMultiWeek,\n",
    "              \"alpha\": alpha,\n",
    "              \"kernel\": kernel\n",
    "            })\n",
    "print(\"KernelRidge: \"+str(len(estimators)-numberOfEstimators))\n",
    "numberOfEstimators = len(estimators)\n",
    "                    \n",
    "for alpha in [0.00001, 0.0001,0.001,0.01,0.1,0,1,10, 100]:\n",
    "    estimators.append(\n",
    "            {\n",
    "              \"modelClass\": \"MultiTaskLasso\",\n",
    "              \"isMultiWeek\": True,\n",
    "              \"alpha\": alpha\n",
    "            })\n",
    "print(\"MultiTaskLasso: \"+str(len(estimators)-numberOfEstimators))\n",
    "numberOfEstimators = len(estimators)\n",
    "                    \n",
    "      \n",
    "\n",
    "for alpha in [0.01,0.1,0,1,10]:\n",
    "    for lamb in [0.01,0.1,0,1,10]:\n",
    "            for n_estimators in [100,500,1000]:\n",
    "                estimators.append(\n",
    "                        {\n",
    "                            \"modelClass\": \"LGBM\",\n",
    "                            \"isMultiWeek\": False,\n",
    "                            \"n_estimators\": n_estimators,\n",
    "                            \"alpha\": alpha, \n",
    "                            \"lambda\": lamb\n",
    "                        }\n",
    "                    )\n",
    "print(\"LGBM: \"+str(len(estimators)-numberOfEstimators))\n",
    "numberOfEstimators = len(estimators)\n",
    "                    \n",
    "for alpha in [0.01,0.1,0,1,10]:\n",
    "    for lamb in [0.01,0.1,0,1,10]:\n",
    "            for n_estimators in [100,500,1000]:\n",
    "                estimators.append(\n",
    "                        {\n",
    "                            \"modelClass\": \"XGB\",\n",
    "                            \"isMultiWeek\": False,\n",
    "                            \"n_estimators\": n_estimators,\n",
    "                            \"alpha\": alpha,\n",
    "                            \"lambda\": lamb\n",
    "                        }\n",
    "                    )\n",
    "print(\"XGB: \"+str(len(estimators)-numberOfEstimators))\n",
    "numberOfEstimators = len(estimators)\n",
    "                    \n",
    "            \n",
    "for C in [0.001,0.01,0.1,1,10,100]:\n",
    "    estimators.append(\n",
    "                        {\n",
    "                            \"modelClass\": \"SVR\",\n",
    "                            \"isMultiWeek\": False,\n",
    "                            \"C\": C\n",
    "                        }\n",
    "                    )\n",
    "print(\"SVR: \"+str(len(estimators)-numberOfEstimators))\n",
    "numberOfEstimators = len(estimators)\n",
    "\n",
    "for alpha in [0.001, 0.01,0.1, 1, 10, 100]:\n",
    "    for learning_rate in ['invscaling','adaptive','optimal']:\n",
    "        for penalty in ['l1','l2','elasticnet']:\n",
    "            estimators.append(\n",
    "                {\n",
    "                    \"modelClass\": \"SGD\",\n",
    "                    \"isMultiWeek\": False ,\n",
    "                    \"alpha\": alpha,\n",
    "                    \"learning_rate\": learning_rate,\n",
    "                    \"penalty\": penalty  \n",
    "                }\n",
    "            )\n",
    "print(\"SGD: \"+str(len(estimators)-numberOfEstimators))\n",
    "numberOfEstimators = len(estimators)\n",
    "                    \n",
    "for max_features in [\"auto\",10,100,200]:\n",
    "    for min_samples_split in [2,3,4]:\n",
    "        for min_samples_leaf in [1,2,3]:\n",
    "            for n_estimators in [100,500,1000]:\n",
    "                for isMultiWeek in [True,False]:\n",
    "                    estimators.append(\n",
    "                        {\n",
    "                            \"modelClass\": \"RandomForrest \",\n",
    "                            \"isMultiWeek\": isMultiWeek,\n",
    "                            \"n_estimators\": n_estimators,\n",
    "                            \"max_features\": max_features,\n",
    "                            \"min_samples_split\": min_samples_split,\n",
    "                            \"min_samples_leaf\": min_samples_leaf\n",
    "                        }\n",
    "                    )\n",
    "print(\"RandomForrest: \"+str(len(estimators)-numberOfEstimators))\n",
    "numberOfEstimators = len(estimators)\n",
    "\n",
    "# adding keras model construction parameters\n",
    "for learningRate in [0.001,0.0001]: \n",
    "    for dropoutValue in [0,0.3,0.5]: \n",
    "        for l1reg in [0.0001, 0.001, 0.01,0.1, 1, 0, 10, 100]:\n",
    "            for alpha in [0.0001, 0.001, 0.01,0.1, 1, 0, 10, 100]: #\n",
    "                for isTwoWay in [True, False]:\n",
    "                    for hiddenLayers in [[50],\n",
    "                                    [100],\n",
    "                                    [150],\n",
    "                                    [200],\n",
    "                                    [300],\n",
    "                                    [200,100],\n",
    "                                    [100, 50],\n",
    "                                    [200, 100, 50],\n",
    "                                    [200, 150, 100, 50],\n",
    "                                    [200, 150, 100, 75, 50],\n",
    "                                    [200, 150, 100, 90, 80, 50],\n",
    "                                    [200, 150, 100, 90, 80, 70, 50],\n",
    "                                    [400, 200, 100, 90, 80, 70, 60, 50]\n",
    "                                   ]:\n",
    "                        for isMultiWeek in [True,False]:\n",
    "                            estimators.append(\n",
    "                                {\n",
    "                                  \"modelClass\": \"Keras\",\n",
    "                                  \"isMultiWeek\": isMultiWeek,\n",
    "                                  \"hiddenLayers\": hiddenLayers,\n",
    "                                  \"numberOfhiddenLayers\": len(hiddenLayers),\n",
    "                                  \"isTwoWay\": isTwoWay,\n",
    "                                  \"l1reg\": l1reg,\n",
    "                                  \"alpha\": alpha,\n",
    "                                  \"dropoutValue\": dropoutValue,\n",
    "                                  \"learningRate\": learningRate\n",
    "                                }\n",
    "                            )\n",
    "print(\"Keras: \"+str(len(estimators)-numberOfEstimators))\n",
    "numberOfEstimators = len(estimators)\n",
    "                    \n",
    "print(\"Total: \"+str(len(estimators)))\n",
    "             \n",
    "\n",
    "'''\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"estimatorId\", help=\"computes estimator with given id\",type=int)\n",
    "args = parser.parse_args()\n",
    "estimatorId = args.estimatorId\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Test\n",
    "estimators.append({\"modelClass\": \"Ridge\",\"alpha\": 10,\"isMultiWeek\": True})\n",
    "estimators.append({\"modelClass\": \"Ridge\",\"alpha\": 10,\"isMultiWeek\": False})\n",
    "estimators.append({\"modelClass\": \"Lasso\",\"alpha\": 10,\"isMultiWeek\": True})\n",
    "estimators.append({\"modelClass\": \"Lasso\",\"alpha\": 10,\"isMultiWeek\": False})\n",
    "estimators.append({\n",
    "                      \"modelClass\": \"Keras\",\n",
    "                      \"isMultiWeek\": True,\n",
    "                      \"hiddenLayers\": [10,10],\n",
    "                      \"numberOfhiddenLayers\": 2,\n",
    "                      \"isTwoWay\": True,\n",
    "                      \"l1reg\": 0.001,\n",
    "                      \"alpha\": 0.01,\n",
    "                      \"dropoutValue\": 0.3,\n",
    "                      \"learningRate\": 0.0001\n",
    "                  }\n",
    "                )\n",
    "estimators.append({\n",
    "                      \"modelClass\": \"Keras\",\n",
    "                      \"isMultiWeek\": False,\n",
    "                      \"hiddenLayers\": [10],\n",
    "                      \"numberOfhiddenLayers\": 2,\n",
    "                      \"isTwoWay\": True,\n",
    "                      \"l1reg\": 0.001,\n",
    "                      \"alpha\": 0.01,\n",
    "                      \"dropoutValue\": 0.3,\n",
    "                      \"learningRate\": 0.0001\n",
    "                  }\n",
    "                )\n",
    "estimators.append(\n",
    "                    {\n",
    "                        \"modelClass\": \"SGD\",\n",
    "                        \"alpha\": 10,\n",
    "                        \"learning_rate\": 'invscaling',\n",
    "                        \"penalty\": 'l2',\n",
    "                        \"isMultiWeek\": False    \n",
    "                    }\n",
    "                )\n",
    "estimators.append(\n",
    "            {\n",
    "              \"modelClass\": \"MultiTaskLasso\",\n",
    "              \"alpha\": 10,\n",
    "              \"isMultiWeek\": True\n",
    "            })\n",
    "estimators.append(\n",
    "                    {\n",
    "                        \"modelClass\": \"SVR\",\n",
    "                        \"C\": 10,\n",
    "                        \"isMultiWeek\": False\n",
    "                    }\n",
    "                )\n",
    "\n",
    "estimators.append(\n",
    "                    {\n",
    "                        \"modelClass\": \"RandomForrest\",\n",
    "                        \"n_estimators\": 10,\n",
    "                        \"max_features\": None,\n",
    "                        \"min_samples_split\": 2,\n",
    "                        \"min_samples_leaf\": 1,\n",
    "                        \"isMultiWeek\": True\n",
    "                    }\n",
    "                 )\n",
    "\n",
    "estimators.append(\n",
    "                    {\n",
    "                        \"modelClass\": \"RandomForrest\",\n",
    "                        \"n_estimators\": 10,\n",
    "                        \"max_features\": 100,\n",
    "                        \"min_samples_split\": 2,\n",
    "                        \"min_samples_leaf\": 1,\n",
    "                        \"isMultiWeek\": False\n",
    "                    }\n",
    "                 )\n",
    "\n",
    "estimators.append(\n",
    "                    {\n",
    "                        \"modelClass\": \"XGB\",\n",
    "                        \"n_estimators\": 10,\n",
    "                        \"alpha\": 0.001,\n",
    "                        \"lambda\": 0.01,\n",
    "                        \"isMultiWeek\": False\n",
    "                    }\n",
    "                 )\n",
    "estimators.append(\n",
    "                    {\n",
    "                        \"modelClass\": \"LGBM\",\n",
    "                        \"n_estimators\": 10,\n",
    "                        \"alpha\": 0.001,\n",
    "                        \"lambda\": 0.01,\n",
    "                        \"isMultiWeek\": False\n",
    "                    }\n",
    "                )\n",
    "\n",
    "estimators.append(\n",
    "            {\n",
    "              \"modelClass\": \"KernelRidge\",\n",
    "              \"alpha\": 10,\n",
    "              \"kernel\": \"linear\",\n",
    "              \"isMultiWeek\": True\n",
    "            })\n",
    "\n",
    "estimators.append(\n",
    "            {\n",
    "              \"modelClass\": \"KernelRidge\",\n",
    "              \"alpha\": 10,\n",
    "              \"kernel\": \"linear\",\n",
    "              \"isMultiWeek\": False\n",
    "            })\n",
    "\n",
    "'''\n",
    "estimatorId = 0\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# constructs and saves the results of a fitted estimator    \n",
    "def constructResults(estimator, task, weekNumber, numberOfRanEpochs):\n",
    "    if weekNumber == -1: # we make predictions for all weeks\n",
    "        # predictions for all weeks\n",
    "        predictions1 = pd.DataFrame(estimator.predict(X_valid1), index=validation1_labels.index, columns=[\"pred_week_\"+task+\"_\"+str(outputWeekNumber) for outputWeekNumber in range(0,numberOfOutputWeeks)]) \n",
    "        predictions2 = pd.DataFrame(estimator.predict(X_valid2), index=validation2_labels.index, columns=[\"pred_week_\"+task+\"_\"+str(outputWeekNumber) for outputWeekNumber in range(0,numberOfOutputWeeks)])\n",
    "        \n",
    "        # validation for all weeks\n",
    "        y_valid1 = validation1_labels[[\"output_\"+task+\"_\"+str(outputWeekNumber) for outputWeekNumber in range(0,numberOfOutputWeeks)]]\n",
    "        y_valid2 = validation2_labels[[\"output_\"+task+\"_\"+str(outputWeekNumber) for outputWeekNumber in range(0,numberOfOutputWeeks)]]\n",
    "        \n",
    "        resultsDf = pd.DataFrame()\n",
    "        # compute and safe results for every week\n",
    "        for outputWeekNumber in range(0,numberOfOutputWeeks):\n",
    "            # compute root mean squared error for validation sets\n",
    "            rmse1 = np.sqrt(mean_squared_error(predictions1[\"pred_week_\"+task+\"_\"+str(outputWeekNumber)], y_valid1[\"output_\"+task+\"_\"+str(outputWeekNumber)]))\n",
    "            rmse2 = np.sqrt(mean_squared_error(predictions2[\"pred_week_\"+task+\"_\"+str(outputWeekNumber)], y_valid2[\"output_\"+task+\"_\"+str(outputWeekNumber)]))\n",
    "            # safe the results and all model parameters\n",
    "            res = {}\n",
    "            if estimators[estimatorId][\"modelClass\"] == \"Keras\":\n",
    "                res = {'modelClass':[estimators[estimatorId][\"modelClass\"]],\n",
    "                     'task':[task],\n",
    "                     'week':[outputWeekNumber], \n",
    "                     'model rmse 1':[rmse1], \n",
    "                     'model rmse 2':[rmse2],\n",
    "                     'isMultiWeek':[estimators[estimatorId][\"isMultiWeek\"]],\n",
    "                     \"hiddelLayers\":[estimators[estimatorId][\"hiddenLayers\"]],\n",
    "                     \"numberOfhiddenLayers\":[len(estimators[estimatorId][\"hiddenLayers\"])],\n",
    "                     \"isTwoWay\": [estimators[estimatorId][\"isTwoWay\"]],\n",
    "                     \"l1reg\": [estimators[estimatorId][\"l1reg\"]],\n",
    "                     \"alpha\": [estimators[estimatorId][\"alpha\"]],\n",
    "                     \"dropoutValue\":[estimators[estimatorId][\"dropoutValue\"]],\n",
    "                     \"learningRate\":[estimators[estimatorId][\"learningRate\"]],\n",
    "                     \"numberOfRanEpochs\": [numberOfRanEpochs]\n",
    "                    }\n",
    "                \n",
    "            elif estimators[estimatorId][\"modelClass\"] == \"RandomForrest\":\n",
    "                res = {'modelClass':[estimators[estimatorId][\"modelClass\"]],\n",
    "                     'task':[task],\n",
    "                     'week':[outputWeekNumber], \n",
    "                     'model rmse 1':[rmse1], \n",
    "                     'model rmse 2':[rmse2],\n",
    "                     'isMultiWeek':[estimators[estimatorId][\"isMultiWeek\"]],\n",
    "                     \"n_estimators\": [estimators[estimatorId][\"n_estimators\"]],\n",
    "                     \"max_features\": [estimators[estimatorId][\"max_features\"]],\n",
    "                     \"min_samples_split\": [estimators[estimatorId][\"min_samples_split\"]],\n",
    "                     \"min_samples_leaf\": [estimators[estimatorId][\"min_samples_leaf\"]]\n",
    "                    }               \n",
    "            elif estimators[estimatorId][\"modelClass\"] == \"KernelRidge\":\n",
    "                res = {'modelClass':[estimators[estimatorId][\"modelClass\"]],\n",
    "                     'task':[task],\n",
    "                     'week':[outputWeekNumber], \n",
    "                     'model rmse 1':[rmse1], \n",
    "                     'model rmse 2':[rmse2],\n",
    "                     'isMultiWeek':[estimators[estimatorId][\"isMultiWeek\"]],\n",
    "                     \"alpha\": [estimators[estimatorId][\"alpha\"]],\n",
    "                     \"kernel\": [estimators[estimatorId][\"kernel\"]],\n",
    "                    }\n",
    "            elif estimators[estimatorId][\"modelClass\"] == \"Ridge\":\n",
    "                res = {'modelClass':[estimators[estimatorId][\"modelClass\"]],\n",
    "                     'task':[task],\n",
    "                     'week':[outputWeekNumber], \n",
    "                     'model rmse 1':[rmse1], \n",
    "                     'model rmse 2':[rmse2],\n",
    "                     'isMultiWeek':[estimators[estimatorId][\"isMultiWeek\"]],\n",
    "                     \"alpha\": [estimators[estimatorId][\"alpha\"]]\n",
    "                    }\n",
    "            elif estimators[estimatorId][\"modelClass\"] == \"Lasso\":\n",
    "                res = {'modelClass':[estimators[estimatorId][\"modelClass\"]],\n",
    "                     'task':[task],\n",
    "                     'week':[outputWeekNumber], \n",
    "                     'model rmse 1':[rmse1], \n",
    "                     'model rmse 2':[rmse2],\n",
    "                     'isMultiWeek':[estimators[estimatorId][\"isMultiWeek\"]],\n",
    "                     \"alpha\": [estimators[estimatorId][\"alpha\"]]\n",
    "                    }\n",
    "            elif estimators[estimatorId][\"modelClass\"] == \"MultiTaskLasso\":\n",
    "                res = {'modelClass':[estimators[estimatorId][\"modelClass\"]],\n",
    "                     'task':[task],\n",
    "                     'week':[outputWeekNumber], \n",
    "                     'model rmse 1':[rmse1], \n",
    "                     'model rmse 2':[rmse2],\n",
    "                     'isMultiWeek':[estimators[estimatorId][\"isMultiWeek\"]],\n",
    "                     \"alpha\": [estimators[estimatorId][\"alpha\"]]\n",
    "                    }\n",
    "            else:\n",
    "                raise ValueError('Tried to save results for an unsupported estimator')\n",
    "            resultsDf = resultsDf.append(pd.DataFrame(data=res), ignore_index = True)\n",
    "        return resultsDf\n",
    "    else: # we make predictions only for one week\n",
    "        # predictions for one week\n",
    "        predictions1 = pd.DataFrame(estimator.predict(X_valid1), index=validation1_labels.index, columns=[\"pred_week_\"+task+\"_\"+str(weekNumber)]) \n",
    "        predictions2 = pd.DataFrame(estimator.predict(X_valid2), index=validation2_labels.index, columns=[\"pred_week_\"+task+\"_\"+str(weekNumber)])\n",
    "        \n",
    "        # validation for one week\n",
    "        y_valid1 = validation1_labels[[\"output_\"+task+\"_\"+str(weekNumber)]]\n",
    "        y_valid2 = validation2_labels[[\"output_\"+task+\"_\"+str(weekNumber)]]\n",
    "        \n",
    "        rmse1 = np.sqrt(mean_squared_error(predictions1[\"pred_week_\"+task+\"_\"+str(weekNumber)], y_valid1[\"output_\"+task+\"_\"+str(weekNumber)]))\n",
    "        rmse2 = np.sqrt(mean_squared_error(predictions2[\"pred_week_\"+task+\"_\"+str(weekNumber)], y_valid2[\"output_\"+task+\"_\"+str(weekNumber)]))\n",
    "        # safe the results and all model parameters\n",
    "        res = {}\n",
    "        if estimators[estimatorId][\"modelClass\"] == \"Keras\":\n",
    "            res = {'modelClass':[estimators[estimatorId][\"modelClass\"]],\n",
    "                 'task':[task],\n",
    "                 'week':[weekNumber], \n",
    "                 'model rmse 1':[rmse1], \n",
    "                 'model rmse 2':[rmse2],\n",
    "                 'isMultiWeek':[estimators[estimatorId][\"isMultiWeek\"]],\n",
    "                 \"hiddelLayers\":[estimators[estimatorId][\"hiddenLayers\"]],\n",
    "                 \"numberOfhiddenLayers\":[len(estimators[estimatorId][\"hiddenLayers\"])],\n",
    "                 \"isTwoWay\": [estimators[estimatorId][\"isTwoWay\"]],\n",
    "                 \"l1reg\": [estimators[estimatorId][\"l1reg\"]],\n",
    "                 \"alpha\": [estimators[estimatorId][\"alpha\"]],\n",
    "                 \"dropoutValue\":[estimators[estimatorId][\"dropoutValue\"]],\n",
    "                 \"learningRate\":[estimators[estimatorId][\"learningRate\"]],\n",
    "                 \"numberOfRanEpochs\": [numberOfRanEpochs]\n",
    "                }\n",
    "\n",
    "        elif estimators[estimatorId][\"modelClass\"] == \"RandomForrest\":\n",
    "            res = {'modelClass':[estimators[estimatorId][\"modelClass\"]],\n",
    "                 'task':[task],\n",
    "                 'week':[weekNumber], \n",
    "                 'model rmse 1':[rmse1], \n",
    "                 'model rmse 2':[rmse2],\n",
    "                 'isMultiWeek':[estimators[estimatorId][\"isMultiWeek\"]],\n",
    "                 \"n_estimators\": [estimators[estimatorId][\"n_estimators\"]],\n",
    "                 \"max_features\": [estimators[estimatorId][\"max_features\"]],\n",
    "                 \"min_samples_split\": [estimators[estimatorId][\"min_samples_split\"]],\n",
    "                 \"min_samples_leaf\": [estimators[estimatorId][\"min_samples_leaf\"]]\n",
    "                }\n",
    "        elif estimators[estimatorId][\"modelClass\"] == \"XGB\":\n",
    "            res = {'modelClass':[estimators[estimatorId][\"modelClass\"]],\n",
    "                 'task':[task],\n",
    "                 'week':[weekNumber], \n",
    "                 'model rmse 1':[rmse1], \n",
    "                 'model rmse 2':[rmse2],\n",
    "                 'isMultiWeek':[estimators[estimatorId][\"isMultiWeek\"]],\n",
    "                 \"n_estimators\": [estimators[estimatorId][\"n_estimators\"]],\n",
    "                 \"alpha\": [estimators[estimatorId][\"alpha\"]],\n",
    "                 \"lambda\": [estimators[estimatorId][\"lambda\"]]\n",
    "                } \n",
    "        elif estimators[estimatorId][\"modelClass\"] == \"LGBM\":\n",
    "            res = {'modelClass':[estimators[estimatorId][\"modelClass\"]],\n",
    "                 'task':[task],\n",
    "                 'week':[weekNumber], \n",
    "                 'model rmse 1':[rmse1], \n",
    "                 'model rmse 2':[rmse2],\n",
    "                 'isMultiWeek':[estimators[estimatorId][\"isMultiWeek\"]],\n",
    "                 \"n_estimators\": [estimators[estimatorId][\"n_estimators\"]],\n",
    "                 \"alpha\": [estimators[estimatorId][\"alpha\"]],\n",
    "                 \"lambda\": [estimators[estimatorId][\"lambda\"]]\n",
    "                } \n",
    "        elif estimators[estimatorId][\"modelClass\"] == \"KernelRidge\":\n",
    "            res = {'modelClass':[estimators[estimatorId][\"modelClass\"]],\n",
    "                 'task':[task],\n",
    "                 'week':[weekNumber], \n",
    "                 'model rmse 1':[rmse1], \n",
    "                 'model rmse 2':[rmse2],\n",
    "                 'isMultiWeek':[estimators[estimatorId][\"isMultiWeek\"]],\n",
    "                 \"alpha\": [estimators[estimatorId][\"alpha\"]],\n",
    "                 \"kernel\": [estimators[estimatorId][\"kernel\"]],\n",
    "                }\n",
    "        elif estimators[estimatorId][\"modelClass\"] == \"Ridge\":\n",
    "            res = {'modelClass':[estimators[estimatorId][\"modelClass\"]],\n",
    "                 'task':[task],\n",
    "                 'week':[weekNumber], \n",
    "                 'model rmse 1':[rmse1], \n",
    "                 'model rmse 2':[rmse2],\n",
    "                 'isMultiWeek':[estimators[estimatorId][\"isMultiWeek\"]],\n",
    "                 \"alpha\": [estimators[estimatorId][\"alpha\"]]\n",
    "                }\n",
    "        elif estimators[estimatorId][\"modelClass\"] == \"Lasso\":\n",
    "            res = {'modelClass':[estimators[estimatorId][\"modelClass\"]],\n",
    "                 'task':[task],\n",
    "                 'week':[weekNumber], \n",
    "                 'model rmse 1':[rmse1], \n",
    "                 'model rmse 2':[rmse2],\n",
    "                 'isMultiWeek':[estimators[estimatorId][\"isMultiWeek\"]],\n",
    "                 \"alpha\": [estimators[estimatorId][\"alpha\"]]\n",
    "                }\n",
    "        elif estimators[estimatorId][\"modelClass\"] == \"SGD\":\n",
    "            res = {'modelClass':[estimators[estimatorId][\"modelClass\"]],\n",
    "                 'task':[task],\n",
    "                 'week':[weekNumber], \n",
    "                 'model rmse 1':[rmse1], \n",
    "                 'model rmse 2':[rmse2],\n",
    "                 'isMultiWeek':[estimators[estimatorId][\"isMultiWeek\"]],\n",
    "                 \"alpha\": [estimators[estimatorId][\"alpha\"]],\n",
    "                 \"learning_rate\": [estimators[estimatorId][\"learning_rate\"]],\n",
    "                 \"penalty\": [estimators[estimatorId][\"penalty\"]]\n",
    "                }\n",
    "        elif estimators[estimatorId][\"modelClass\"] == \"SVR\":\n",
    "            res = {'modelClass':[estimators[estimatorId][\"modelClass\"]],\n",
    "                 'task':[task],\n",
    "                 'week':[weekNumber], \n",
    "                 'model rmse 1':[rmse1], \n",
    "                 'model rmse 2':[rmse2],\n",
    "                 'isMultiWeek':[estimators[estimatorId][\"isMultiWeek\"]],\n",
    "                 \"C\": [estimators[estimatorId][\"C\"]]\n",
    "                }\n",
    "        else:\n",
    "            raise ValueError('Tried to save results for an unsupported estimator')\n",
    "        return pd.DataFrame(data=res)\n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "# training\n",
    "\n",
    "\n",
    "\n",
    "if estimators[estimatorId][\"isMultiWeek\"]:\n",
    "    # we just train one model per task\n",
    "    for task in outputCategories:\n",
    "        # get train labels for all weeks\n",
    "        y_train = train_labels[[\"output_\"+task+\"_\"+str(outputWeekNumber) for outputWeekNumber in range(0,numberOfOutputWeeks)]].values\n",
    "        if estimators[estimatorId][\"modelClass\"] == \"Keras\":\n",
    "            # get validation labels for all weeks (used for early stopping)\n",
    "            y_valid1And2 = validation1And2_labels[[\"output_\"+task+\"_\"+str(outputWeekNumber) for outputWeekNumber in range(0,numberOfOutputWeeks)]].values \n",
    "            # construct Keras model\n",
    "            estimator = genericKerasModel(\n",
    "                estimators[estimatorId][\"learningRate\"],\n",
    "                estimators[estimatorId][\"hiddenLayers\"],\n",
    "                estimators[estimatorId][\"dropoutValue\"],\n",
    "                estimators[estimatorId][\"l1reg\"],\n",
    "                estimators[estimatorId][\"alpha\"],\n",
    "                estimators[estimatorId][\"isTwoWay\"],\n",
    "                estimators[estimatorId][\"isMultiWeek\"]\n",
    "            )\n",
    "            # fit Keras model\n",
    "            history = estimator.fit(X_train, \n",
    "              y_train, \n",
    "              batch_size=32, \n",
    "              epochs=1000, \n",
    "              verbose=0, \n",
    "              validation_data=(X_valid1And2,y_valid1And2), \n",
    "              callbacks=[keras.callbacks.EarlyStopping(patience=10)])\n",
    "            numberOfRanEpochs = len(history.history['loss'])\n",
    "            results = results.append(constructResults(estimator, task, -1, numberOfRanEpochs), ignore_index = True)\n",
    "            \n",
    "        elif estimators[estimatorId][\"modelClass\"] == \"RandomForrest\":\n",
    "            estimator = RandomForestRegressor(n_estimators=estimators[estimatorId][\"n_estimators\"], \n",
    "                                              max_features=estimators[estimatorId][\"max_features\"],\n",
    "                                              min_samples_split=estimators[estimatorId][\"min_samples_split\"],\n",
    "                                              min_samples_leaf=estimators[estimatorId][\"min_samples_leaf\"],\n",
    "                                              n_jobs=-1\n",
    "                                             )\n",
    "            estimator.fit(X_train,y_train)\n",
    "            results = results.append(constructResults(estimator, task, -1, -1), ignore_index = True)\n",
    "        elif estimators[estimatorId][\"modelClass\"] == \"KernelRidge\":\n",
    "            estimator = KernelRidge(alpha=estimators[estimatorId][\"alpha\"], \n",
    "                                    kernel=estimators[estimatorId][\"kernel\"],\n",
    "                                    )\n",
    "            estimator.fit(X_train,y_train)\n",
    "            results = results.append(constructResults(estimator, task, -1, -1), ignore_index = True)\n",
    "        elif estimators[estimatorId][\"modelClass\"] == \"Ridge\":\n",
    "            estimator = linear_model.Ridge(alpha=estimators[estimatorId][\"alpha\"])\n",
    "            estimator.fit(X_train,y_train)\n",
    "            results = results.append(constructResults(estimator, task, -1, -1), ignore_index = True)\n",
    "        elif estimators[estimatorId][\"modelClass\"] == \"Lasso\":\n",
    "            estimator = linear_model.Lasso(alpha=estimators[estimatorId][\"alpha\"])\n",
    "            estimator.fit(X_train,y_train)\n",
    "            results = results.append(constructResults(estimator, task, -1, -1), ignore_index = True)\n",
    "        elif estimators[estimatorId][\"modelClass\"] == \"SGD\":\n",
    "            estimator = linear_model.SGDRegressor(learning_rate = estimators[estimatorId][\"learning_rate\"],\n",
    "                                                  penalty=estimators[estimatorId][\"penalty\"],\n",
    "                                                  alpha=estimators[estimatorId][\"alpha\"],\n",
    "                                                  max_iter=10000, \n",
    "                                                  tol=0.00001, \n",
    "                                                  n_iter_no_change=10)\n",
    "            estimator.fit(X_train,y_train)\n",
    "            results = results.append(constructResults(estimator, task, -1, -1), ignore_index = True)\n",
    "        elif estimators[estimatorId][\"modelClass\"] == \"MultiTaskLasso\":\n",
    "            estimator = linear_model.MultiTaskLasso(alpha = estimators[estimatorId][\"alpha\"])\n",
    "            estimator.fit(X_train,y_train)\n",
    "            results = results.append(constructResults(estimator, task, -1, -1), ignore_index = True)\n",
    "        else:\n",
    "            raise ValueError('Tried to fit an unsupported estimator')            \n",
    "else:\n",
    "    # we have to train one model per output week and per task\n",
    "    for outputWeekNumber in range(0,numberOfOutputWeeks):\n",
    "        for task in outputCategories:\n",
    "            # get train labels for one week\n",
    "            y_train = train_labels[[\"output_\"+task+\"_\"+str(outputWeekNumber)]].values\n",
    "            if estimators[estimatorId][\"modelClass\"] == \"Keras\":\n",
    "                # get validation labels for one week (used for early stopping)\n",
    "                y_valid = validation1And2_labels[[\"output_\"+task+\"_\"+str(outputWeekNumber)]].values \n",
    "                # construct Keras model\n",
    "                estimator = genericKerasModel(\n",
    "                    estimators[estimatorId][\"learningRate\"],\n",
    "                    estimators[estimatorId][\"hiddenLayers\"],\n",
    "                    estimators[estimatorId][\"dropoutValue\"],\n",
    "                    estimators[estimatorId][\"l1reg\"],\n",
    "                    estimators[estimatorId][\"alpha\"],\n",
    "                    estimators[estimatorId][\"isTwoWay\"],\n",
    "                    estimators[estimatorId][\"isMultiWeek\"]\n",
    "                )\n",
    "                # fit Keras model\n",
    "                history = estimator.fit(X_train, \n",
    "                  y_train, \n",
    "                  batch_size=32, \n",
    "                  epochs=1000, \n",
    "                  verbose=0, \n",
    "                  validation_data=(X_valid1And2,y_valid), \n",
    "                  callbacks=[keras.callbacks.EarlyStopping(patience=10)])\n",
    "                numberOfRanEpochs = len(history.history['loss'])\n",
    "                results = results.append(constructResults(estimator, task, outputWeekNumber, numberOfRanEpochs), ignore_index = True)\n",
    "            elif estimators[estimatorId][\"modelClass\"] == \"RandomForrest\":\n",
    "                estimator = RandomForestRegressor(n_estimators=estimators[estimatorId][\"n_estimators\"], \n",
    "                                              max_features=estimators[estimatorId][\"max_features\"],\n",
    "                                              min_samples_split=estimators[estimatorId][\"min_samples_split\"],\n",
    "                                              min_samples_leaf=estimators[estimatorId][\"min_samples_leaf\"],\n",
    "                                              n_jobs=-1\n",
    "                                             )\n",
    "                estimator.fit(X_train,y_train.ravel())\n",
    "                results = results.append(constructResults(estimator, task, outputWeekNumber, -1), ignore_index = True)\n",
    "            elif estimators[estimatorId][\"modelClass\"] == \"KernelRidge\":\n",
    "                estimator = KernelRidge(alpha=estimators[estimatorId][\"alpha\"], \n",
    "                                    kernel=estimators[estimatorId][\"kernel\"],\n",
    "                                    )\n",
    "                estimator.fit(X_train,y_train)\n",
    "                results = results.append(constructResults(estimator, task, outputWeekNumber, -1), ignore_index = True)\n",
    "            elif estimators[estimatorId][\"modelClass\"] == \"Ridge\":\n",
    "                estimator = linear_model.Ridge(alpha=estimators[estimatorId][\"alpha\"])\n",
    "                estimator.fit(X_train,y_train)\n",
    "                results = results.append(constructResults(estimator, task, outputWeekNumber, -1), ignore_index = True)\n",
    "            elif estimators[estimatorId][\"modelClass\"] == \"Lasso\":\n",
    "                estimator = linear_model.Lasso(alpha=estimators[estimatorId][\"alpha\"])\n",
    "                estimator.fit(X_train,y_train)\n",
    "                results = results.append(constructResults(estimator, task, outputWeekNumber, -1), ignore_index = True)\n",
    "            elif estimators[estimatorId][\"modelClass\"] == \"LGBM\":\n",
    "                estimator = lightgbm.LGBMRegressor(n_estimators=estimators[estimatorId][\"n_estimators\"], \n",
    "                                                   reg_alpha=estimators[estimatorId][\"alpha\"], \n",
    "                                                   reg_lambda=estimators[estimatorId][\"lambda\"], \n",
    "                                                   n_jobs=-1)\n",
    "                estimator.fit(X_train,y_train.ravel())\n",
    "                results = results.append(constructResults(estimator, task, outputWeekNumber, -1), ignore_index = True)\n",
    "            elif estimators[estimatorId][\"modelClass\"] == \"XGB\":\n",
    "                estimator = xgb.XGBRegressor(n_estimators=estimators[estimatorId][\"n_estimators\"], \n",
    "                                             reg_alpha=estimators[estimatorId][\"alpha\"], \n",
    "                                             reg_lambda=estimators[estimatorId][\"lambda\"], \n",
    "                                             n_jobs=-1)\n",
    "                estimator.fit(X_train,y_train.ravel())\n",
    "                results = results.append(constructResults(estimator, task, outputWeekNumber, -1), ignore_index = True)\n",
    "            elif estimators[estimatorId][\"modelClass\"] == \"SGD\":\n",
    "                estimator = linear_model.SGDRegressor(learning_rate = estimators[estimatorId][\"learning_rate\"],\n",
    "                                                  penalty=estimators[estimatorId][\"penalty\"],\n",
    "                                                  alpha=estimators[estimatorId][\"alpha\"],\n",
    "                                                  max_iter=10000, \n",
    "                                                  tol=0.00001, \n",
    "                                                  n_iter_no_change=10)\n",
    "                estimator.fit(X_train,y_train.ravel())\n",
    "                results = results.append(constructResults(estimator, task, outputWeekNumber, -1), ignore_index = True)\n",
    "            elif estimators[estimatorId][\"modelClass\"] == \"SVR\":\n",
    "                estimator = svm.SVR(C=estimators[estimatorId][\"C\"])\n",
    "                estimator.fit(X_train,y_train.ravel())\n",
    "                results = results.append(constructResults(estimator, task, outputWeekNumber, -1), ignore_index = True)\n",
    "            else:\n",
    "                raise ValueError('Tried to fit an unsupported estimator')\n",
    "\n",
    "def generateModelId(dictionary):\n",
    "    modelId = \"\"\n",
    "    for key in dictionary.keys():\n",
    "        modelId = modelId + key +\"=\"+ str(dictionary[key]) +\"/\"\n",
    "    modelId = modelId[0:-1]\n",
    "    return modelId\n",
    "\n",
    "# add a modelId\n",
    "results[\"modelId\"] = generateModelId(estimators[estimatorId])\n",
    "                \n",
    "if not os.path.exists('results/'):\n",
    "    os.makedirs('results')\n",
    "results.to_csv(\"results/\"+str(estimatorId)+\".csv\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "executed-purpose",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"251: {'modelClass': 'XGB', 'isMultiWeek': False, 'n_estimators': 1000, 'alpha': 0.01, 'lambda': 0.1}\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"257: {'modelClass': 'XGB', 'isMultiWeek': False, 'n_estimators': 1000, 'alpha': 0.01, 'lambda': 1}\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"260: {'modelClass': 'XGB', 'isMultiWeek': False, 'n_estimators': 1000, 'alpha': 0.01, 'lambda': 10}\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"263: {'modelClass': 'XGB', 'isMultiWeek': False, 'n_estimators': 1000, 'alpha': 0.1, 'lambda': 0.01}\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"278: {'modelClass': 'XGB', 'isMultiWeek': False, 'n_estimators': 1000, 'alpha': 0, 'lambda': 0.01}\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"384: {'modelClass': 'RandomForrest', 'isMultiWeek': False, 'n_estimators': 500, 'max_features': 'auto', 'min_samples_split': 2, 'min_samples_leaf': 1}\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"385: {'modelClass': 'RandomForrest', 'isMultiWeek': True, 'n_estimators': 1000, 'max_features': 'auto', 'min_samples_split': 2, 'min_samples_leaf': 1}\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"386: {'modelClass': 'RandomForrest', 'isMultiWeek': False, 'n_estimators': 1000, 'max_features': 'auto', 'min_samples_split': 2, 'min_samples_leaf': 1}\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"390: {'modelClass': 'RandomForrest', 'isMultiWeek': False, 'n_estimators': 500, 'max_features': 'auto', 'min_samples_split': 2, 'min_samples_leaf': 2}\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"391: {'modelClass': 'RandomForrest', 'isMultiWeek': True, 'n_estimators': 1000, 'max_features': 'auto', 'min_samples_split': 2, 'min_samples_leaf': 2}\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"392: {'modelClass': 'RandomForrest', 'isMultiWeek': False, 'n_estimators': 1000, 'max_features': 'auto', 'min_samples_split': 2, 'min_samples_leaf': 2}\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"396: {'modelClass': 'RandomForrest', 'isMultiWeek': False, 'n_estimators': 500, 'max_features': 'auto', 'min_samples_split': 2, 'min_samples_leaf': 3}\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"397: {'modelClass': 'RandomForrest', 'isMultiWeek': True, 'n_estimators': 1000, 'max_features': 'auto', 'min_samples_split': 2, 'min_samples_leaf': 3}\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"398: {'modelClass': 'RandomForrest', 'isMultiWeek': False, 'n_estimators': 1000, 'max_features': 'auto', 'min_samples_split': 2, 'min_samples_leaf': 3}\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"402: {'modelClass': 'RandomForrest', 'isMultiWeek': False, 'n_estimators': 500, 'max_features': 'auto', 'min_samples_split': 3, 'min_samples_leaf': 1}\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"403: {'modelClass': 'RandomForrest', 'isMultiWeek': True, 'n_estimators': 1000, 'max_features': 'auto', 'min_samples_split': 3, 'min_samples_leaf': 1}\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"404: {'modelClass': 'RandomForrest', 'isMultiWeek': False, 'n_estimators': 1000, 'max_features': 'auto', 'min_samples_split': 3, 'min_samples_leaf': 1}\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"408: {'modelClass': 'RandomForrest', 'isMultiWeek': False, 'n_estimators': 500, 'max_features': 'auto', 'min_samples_split': 3, 'min_samples_leaf': 2}\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"409: {'modelClass': 'RandomForrest', 'isMultiWeek': True, 'n_estimators': 1000, 'max_features': 'auto', 'min_samples_split': 3, 'min_samples_leaf': 2}\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"410: {'modelClass': 'RandomForrest', 'isMultiWeek': False, 'n_estimators': 1000, 'max_features': 'auto', 'min_samples_split': 3, 'min_samples_leaf': 2}\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"414: {'modelClass': 'RandomForrest', 'isMultiWeek': False, 'n_estimators': 500, 'max_features': 'auto', 'min_samples_split': 3, 'min_samples_leaf': 3}\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"415: {'modelClass': 'RandomForrest', 'isMultiWeek': True, 'n_estimators': 1000, 'max_features': 'auto', 'min_samples_split': 3, 'min_samples_leaf': 3}\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"416: {'modelClass': 'RandomForrest', 'isMultiWeek': False, 'n_estimators': 1000, 'max_features': 'auto', 'min_samples_split': 3, 'min_samples_leaf': 3}\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"420: {'modelClass': 'RandomForrest', 'isMultiWeek': False, 'n_estimators': 500, 'max_features': 'auto', 'min_samples_split': 4, 'min_samples_leaf': 1}\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"421: {'modelClass': 'RandomForrest', 'isMultiWeek': True, 'n_estimators': 1000, 'max_features': 'auto', 'min_samples_split': 4, 'min_samples_leaf': 1}\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"422: {'modelClass': 'RandomForrest', 'isMultiWeek': False, 'n_estimators': 1000, 'max_features': 'auto', 'min_samples_split': 4, 'min_samples_leaf': 1}\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"425: {'modelClass': 'RandomForrest', 'isMultiWeek': True, 'n_estimators': 500, 'max_features': 'auto', 'min_samples_split': 4, 'min_samples_leaf': 2}\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"426: {'modelClass': 'RandomForrest', 'isMultiWeek': False, 'n_estimators': 500, 'max_features': 'auto', 'min_samples_split': 4, 'min_samples_leaf': 2}\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"427: {'modelClass': 'RandomForrest', 'isMultiWeek': True, 'n_estimators': 1000, 'max_features': 'auto', 'min_samples_split': 4, 'min_samples_leaf': 2}\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"428: {'modelClass': 'RandomForrest', 'isMultiWeek': False, 'n_estimators': 1000, 'max_features': 'auto', 'min_samples_split': 4, 'min_samples_leaf': 2}\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"431: {'modelClass': 'RandomForrest', 'isMultiWeek': True, 'n_estimators': 500, 'max_features': 'auto', 'min_samples_split': 4, 'min_samples_leaf': 3}\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"432: {'modelClass': 'RandomForrest', 'isMultiWeek': False, 'n_estimators': 500, 'max_features': 'auto', 'min_samples_split': 4, 'min_samples_leaf': 3}\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"433: {'modelClass': 'RandomForrest', 'isMultiWeek': True, 'n_estimators': 1000, 'max_features': 'auto', 'min_samples_split': 4, 'min_samples_leaf': 3}\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"434: {'modelClass': 'RandomForrest', 'isMultiWeek': False, 'n_estimators': 1000, 'max_features': 'auto', 'min_samples_split': 4, 'min_samples_leaf': 3}\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"518: {'modelClass': 'RandomForrest', 'isMultiWeek': False, 'n_estimators': 1000, 'max_features': 100, 'min_samples_split': 3, 'min_samples_leaf': 2}\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"524: {'modelClass': 'RandomForrest', 'isMultiWeek': False, 'n_estimators': 1000, 'max_features': 100, 'min_samples_split': 3, 'min_samples_leaf': 3}\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"536: {'modelClass': 'RandomForrest', 'isMultiWeek': False, 'n_estimators': 1000, 'max_features': 100, 'min_samples_split': 4, 'min_samples_leaf': 2}\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"542: {'modelClass': 'RandomForrest', 'isMultiWeek': False, 'n_estimators': 1000, 'max_features': 100, 'min_samples_split': 4, 'min_samples_leaf': 3}\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"548: {'modelClass': 'RandomForrest', 'isMultiWeek': False, 'n_estimators': 1000, 'max_features': 200, 'min_samples_split': 2, 'min_samples_leaf': 1}\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"554: {'modelClass': 'RandomForrest', 'isMultiWeek': False, 'n_estimators': 1000, 'max_features': 200, 'min_samples_split': 2, 'min_samples_leaf': 2}\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"560: {'modelClass': 'RandomForrest', 'isMultiWeek': False, 'n_estimators': 1000, 'max_features': 200, 'min_samples_split': 2, 'min_samples_leaf': 3}\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"566: {'modelClass': 'RandomForrest', 'isMultiWeek': False, 'n_estimators': 1000, 'max_features': 200, 'min_samples_split': 3, 'min_samples_leaf': 1}\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"572: {'modelClass': 'RandomForrest', 'isMultiWeek': False, 'n_estimators': 1000, 'max_features': 200, 'min_samples_split': 3, 'min_samples_leaf': 2}\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"578: {'modelClass': 'RandomForrest', 'isMultiWeek': False, 'n_estimators': 1000, 'max_features': 200, 'min_samples_split': 3, 'min_samples_leaf': 3}\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"584: {'modelClass': 'RandomForrest', 'isMultiWeek': False, 'n_estimators': 1000, 'max_features': 200, 'min_samples_split': 4, 'min_samples_leaf': 1}\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"590: {'modelClass': 'RandomForrest', 'isMultiWeek': False, 'n_estimators': 1000, 'max_features': 200, 'min_samples_split': 4, 'min_samples_leaf': 2}\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"596: {'modelClass': 'RandomForrest', 'isMultiWeek': False, 'n_estimators': 1000, 'max_features': 200, 'min_samples_split': 4, 'min_samples_leaf': 3}\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-143674b367ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mestimatorErrors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m125\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\": \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "file1 = open('output.log', 'r')\n",
    "Lines = file1.readlines()\n",
    " \n",
    "\n",
    "\n",
    "estimatorErrors = []\n",
    "\n",
    "for line in Lines:\n",
    "    estimatorErrors.append(int(line[32:-11]))\n",
    "\n",
    "   \n",
    "\n",
    "with open('erstimatorErrors.txt', 'w') as f:\n",
    "    for e in estimatorErrors:\n",
    "        f.write(str(e)+\"\\n\")\n",
    "display(len(estimatorErrors))\n",
    "\n",
    "\n",
    "for e in estimatorErrors[0:125]:\n",
    "    display(str(e)+\": \"+str(estimators[e]))\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "remarkable-destination",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriented-assembly",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
