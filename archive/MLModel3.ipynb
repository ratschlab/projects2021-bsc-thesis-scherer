{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "prime-laundry",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time\n",
    "import logging\n",
    "from joblib import dump, load\n",
    "from datetime import timedelta\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.multioutput import RegressorChain\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import xgboost as xgb\n",
    "import lightgbm as lightgbm\n",
    "\n",
    "pd.options.display.max_rows = 10\n",
    "\n",
    "\n",
    "logging.basicConfig(filename='output.log', level=logging.INFO, filemode='w')\n",
    "\n",
    "completeTimer = time.time()\n",
    "typeTimer = time.time()\n",
    "estimatorTimer = time.time()\n",
    "\n",
    "\n",
    "# environment\n",
    "isSuperComputer = True\n",
    "\n",
    "cantonKeys = ['AG','AI','AR', 'BE', 'BL', 'BS', 'FR', 'GE', 'GL', 'GR', 'JU', 'LU', 'NE', 'NW', 'OW', 'SG', 'SH', 'SO', 'SZ', 'TG', 'TI', 'UR', 'VD', 'VS', 'ZG','ZH']\n",
    "\n",
    "data = pd.read_csv(\"completedata.csv\")\n",
    "\n",
    "# last check if there are some NaN values in the data which we use\n",
    "for col in data.columns:\n",
    "    if data[col].isna().sum() != 0:\n",
    "        print(col+\" (#NaN/#NotNaN): (\" + str(data[col].isna().sum())+\"/\"+str(data[col].notna().sum())+\")\")\n",
    "\n",
    "\n",
    "numberOfInputWeeks = 3\n",
    "\n",
    "#display([col for col in data.columns])\n",
    "\n",
    "\n",
    "outputCategories = [\n",
    "                  'case_inz_entries_7dayAverage',\n",
    "                  'hosp_inz_entries_7dayAverage',\n",
    "                  'death_inz_entries_7dayAverage',\n",
    "                  'testPositvity',\n",
    "                  'testPositvity_7dayAverageBoth',    \n",
    "                  'transit_stations_percent_change_from_baseline_7dayAverage',\n",
    "                  'workplaces_percent_change_from_baseline_7dayAverage',    \n",
    "                   ]\n",
    "numberOfOutputs = len(outputCategories)\n",
    "numberOfPreComputedOutputWeeks = 4\n",
    "\n",
    "split = numberOfOutputs * numberOfPreComputedOutputWeeks + 2\n",
    "\n",
    "train_features = data[data['category']=='train'].iloc[:,0:-split].drop(['lastInputDay'], axis=1)\n",
    "train_labels = data[data['category']=='train'].iloc[:,-split:-2]\n",
    "validation1_features = data[data['category']=='validation 1'].iloc[:,0:-split].drop(['lastInputDay'], axis=1)\n",
    "validation1_labels = data[data['category']=='validation 1'].iloc[:,-split:-2]\n",
    "validation2_features = data[data['category']=='validation 2'].iloc[:,0:-split].drop(['lastInputDay'], axis=1)\n",
    "validation2_labels = data[data['category']=='validation 2'].iloc[:,-split:-2]\n",
    "\n",
    "'''\n",
    "cId = 'AG'\n",
    "train_features = data[(data['category']=='train') & (data['cantonId']==cId)].iloc[:,0:-split].drop(['lastInputDay'], axis=1)\n",
    "train_labels = data[(data['category']=='train') & (data['cantonId']==cId)].iloc[:,-split:-2]\n",
    "validation1_features = data[(data['category']=='validation 1') & (data['cantonId']==cId)].iloc[:,0:-split].drop(['lastInputDay'], axis=1)\n",
    "validation1_labels = data[(data['category']=='validation 1') & (data['cantonId']==cId)].iloc[:,-split:-2]\n",
    "validation2_features = data[(data['category']=='validation 2') & (data['cantonId']==cId)].iloc[:,0:-split].drop(['lastInputDay'], axis=1)\n",
    "validation2_labels = data[(data['category']=='validation 2') & (data['cantonId']==cId)].iloc[:,-split:-2]\n",
    "'''\n",
    "\n",
    "#pip = Pipeline([('std_scaler', StandardScaler())])\n",
    "pip = Pipeline([('minmax_scaler', MinMaxScaler())])\n",
    "\n",
    "X_train = pip.fit_transform(train_features[train_features.columns].values)\n",
    "X_valid1 = pip.transform(validation1_features[train_features.columns].values)\n",
    "X_valid2 = pip.transform(validation2_features[train_features.columns].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "stunning-letters",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-59cb5bf2ab0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"output_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcategory\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumberOfOutputWeeks\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnumberOfOutputWeeks\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnumberOfPreComputedOutputWeeks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0my_valid1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation1_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"output_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcategory\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumberOfOutputWeeks\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnumberOfOutputWeeks\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnumberOfPreComputedOutputWeeks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m         estimator.fit(X_train, \n\u001b[0m\u001b[1;32m    169\u001b[0m                       \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                       \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                 _r=1):\n\u001b[1;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3021\u001b[0m       (graph_function,\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3023\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1958\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1960\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1961\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc_graph_output\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func_graph_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m       \u001b[0mcustom_gradient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_handle_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_graph_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/ops/handle_data_util.py\u001b[0m in \u001b[0;36mcopy_handle_data\u001b[0;34m(source_t, target_t)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mcopy_handle_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m   \"\"\"Copies HandleData for variant and resource type tensors if available.\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "modelId = 0\n",
    "results = pd.DataFrame(columns=['model',\n",
    "                                'model type',\n",
    "                                'target',\n",
    "                                'week',\n",
    "                                'model rsme 1', \n",
    "                                'model rsme 2'])\n",
    "results.to_csv(\"results.csv\", index=False)\n",
    "\n",
    "\n",
    "# (str(type(estimator)) == \"<class 'tensorflow.python.keras.engine.functional.Functional'>\")\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------                       \n",
    "\n",
    "typeTimer = time.time()\n",
    "logging.info(\"============== fitting single output keras estimators ============== \")\n",
    "start = time.time()\n",
    "# fitting single output keras estimators\n",
    "\n",
    "numberOfOutputsForMultiOutput = numberOfPreComputedOutputWeeks\n",
    "epochs = 1000\n",
    "batch_size = 32 #16\n",
    "\n",
    "def getNormalKerasModel(name, learningrate, nrOfNeurons, isMultiOutput):\n",
    "    input_ = keras.layers.Input(shape=X_train.shape[1:])\n",
    "    hidden1 = keras.layers.Dense(nrOfNeurons, activation=\"relu\")(input_)\n",
    "    output = keras.layers.Dense(numberOfOutputsForMultiOutput)(hidden1) if isMultiOutput else keras.layers.Dense(1)(hidden1)\n",
    "    model = keras.Model(name=name, inputs=[input_],outputs=[output])\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.Adam(learning_rate=learningrate))\n",
    "    return model\n",
    "\n",
    "\n",
    "def getDropoutKerasModel(name, learningrate, nrOfNeurons, isMultiOutput):\n",
    "    input_ = keras.layers.Input(shape=X_train.shape[1:])\n",
    "    hidden1 = keras.layers.Dense(nrOfNeurons, activation=\"relu\")(input_)\n",
    "    dropout1 = keras.layers.Dropout(rate=0.5)(hidden1)\n",
    "    output = keras.layers.Dense(numberOfOutputsForMultiOutput)(dropout1) if isMultiOutput else keras.layers.Dense(1)(dropout1)\n",
    "    model = keras.Model(name=name, inputs=[input_],outputs=[output])\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.Adam(learning_rate=learningrate))\n",
    "    return model\n",
    "\n",
    "def getRegularizedKerasModel(name, learningrate, nrOfNeurons, alpha, isMultiOutput):\n",
    "    input_ = keras.layers.Input(shape=X_train.shape[1:])\n",
    "    hidden1 = keras.layers.Dense(nrOfNeurons, activation=\"relu\", kernel_regularizer=keras.regularizers.l2(alpha))(input_)\n",
    "    output = keras.layers.Dense(numberOfOutputsForMultiOutput)(hidden1) if isMultiOutput else keras.layers.Dense(1)(hidden1)\n",
    "    model = keras.Model(name=name, inputs=[input_],outputs=[output])\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.Adam(learning_rate=learningrate))\n",
    "    return model\n",
    "\n",
    "def getTwoWayKerasModel(name, learningrate, nrOfNeurons, isMultiOutput):\n",
    "    input_ = keras.layers.Input(shape=X_train.shape[1:])\n",
    "    hidden1 = keras.layers.Dense(nrOfNeurons, activation=\"relu\")(input_)\n",
    "    concat = keras.layers.Concatenate()([input_,hidden1])\n",
    "    output = keras.layers.Dense(numberOfOutputsForMultiOutput)(concat) if isMultiOutput else keras.layers.Dense(1)(concat)\n",
    "    model = keras.Model(name=name, inputs=[input_],outputs=[output])\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.Adam(learning_rate=learningrate))\n",
    "    return model\n",
    "\n",
    "def getComplexKerasModel(name, learningrate, nrOfNeurons1, nrOfNeurons2, isMultiOutput):\n",
    "    input_ = keras.layers.Input(shape=X_train.shape[1:])\n",
    "    hidden1 = keras.layers.Dense(nrOfNeurons1, activation=\"relu\")(input_)\n",
    "    hidden2 = keras.layers.Dense(nrOfNeurons2, activation=\"relu\")(hidden1)\n",
    "    output = keras.layers.Dense(numberOfOutputsForMultiOutput)(hidden1) if isMultiOutput else keras.layers.Dense(1)(hidden2)\n",
    "    model = keras.Model(name=name, inputs=[input_],outputs=[output])\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.Adam(learning_rate=learningrate))\n",
    "    return model\n",
    "\n",
    "def getTwoWayComplexDropoutKerasModel(name, learningrate, nrOfNeurons1, nrOfNeurons2, nrOfNeurons3, nrOfNeurons4, nrOfNeurons5, isMultiOutput):\n",
    "    input_ = keras.layers.Input(shape=X_train.shape[1:])\n",
    "    hidden1 = keras.layers.Dense(nrOfNeurons1, activation=\"relu\")(input_)\n",
    "    dropout1 = keras.layers.Dropout(rate=0.5)(hidden1)\n",
    "    hidden2 = keras.layers.Dense(nrOfNeurons2, activation=\"relu\")(hidden1)\n",
    "    dropout1 = keras.layers.Dropout(rate=0.5)(hidden2)\n",
    "    hidden3 = keras.layers.Dense(nrOfNeurons3, activation=\"relu\")(hidden2)\n",
    "    dropout1 = keras.layers.Dropout(rate=0.5)(hidden3)\n",
    "    hidden4 = keras.layers.Dense(nrOfNeurons4, activation=\"relu\")(hidden3)\n",
    "    dropout1 = keras.layers.Dropout(rate=0.5)(hidden4)\n",
    "    hidden5 = keras.layers.Dense(nrOfNeurons5, activation=\"relu\")(hidden4)\n",
    "    dropout1 = keras.layers.Dropout(rate=0.5)(hidden5)\n",
    "    concat = keras.layers.Concatenate()([input_,dropout1])\n",
    "    output = keras.layers.Dense(numberOfOutputsForMultiOutput)(concat) if isMultiOutput else keras.layers.Dense(1)(concat)\n",
    "    model = keras.Model(name=name, inputs=[input_],outputs=[output])\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.Adam(learning_rate=learningrate))\n",
    "    return model\n",
    "\n",
    "def getDeepDropoutKerasModel(name, learningrate, nrOfNeurons, isMultiOutput):\n",
    "    input_ = keras.layers.Input(shape=X_train.shape[1:])\n",
    "    dropout0 = keras.layers.Dropout(rate=0.3)(input_)\n",
    "    hidden1 = keras.layers.Dense(nrOfNeurons, activation=\"relu\")(dropout0)\n",
    "    dropout1 = keras.layers.Dropout(rate=0.3)(hidden1)\n",
    "    hidden2 = keras.layers.Dense(nrOfNeurons, activation=\"relu\")(dropout1)\n",
    "    dropout2 = keras.layers.Dropout(rate=0.3)(hidden2)\n",
    "    hidden3 = keras.layers.Dense(nrOfNeurons, activation=\"relu\")(dropout2)\n",
    "    dropout3 = keras.layers.Dropout(rate=0.3)(hidden3)\n",
    "    hidden4 = keras.layers.Dense(nrOfNeurons, activation=\"relu\")(dropout3)\n",
    "    dropout4 = keras.layers.Dropout(rate=0.3)(hidden4)\n",
    "    hidden5 = keras.layers.Dense(nrOfNeurons, activation=\"relu\")(dropout4)\n",
    "    dropout5 = keras.layers.Dropout(rate=0.3)(hidden5)\n",
    "    hidden6 = keras.layers.Dense(nrOfNeurons, activation=\"relu\")(dropout5)\n",
    "    dropout6 = keras.layers.Dropout(rate=0.3)(hidden6)\n",
    "    hidden7 = keras.layers.Dense(nrOfNeurons, activation=\"relu\")(dropout6)\n",
    "    dropout7 = keras.layers.Dropout(rate=0.3)(hidden7)\n",
    "    hidden8 = keras.layers.Dense(nrOfNeurons, activation=\"relu\")(dropout7)\n",
    "    dropout8 = keras.layers.Dropout(rate=0.3)(hidden8)\n",
    "    output = keras.layers.Dense(numberOfOutputsForMultiOutput)(dropout8) if isMultiOutput else keras.layers.Dense(1)(dropout8)\n",
    "    model = keras.Model(name=name, inputs=[input_],outputs=[output])\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.Adam(learning_rate=learningrate))\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "\n",
    "typeTimer = time.time()\n",
    "logging.info(\"============== fitting multi output keras estimators ============== \")\n",
    "\n",
    "estimators = []\n",
    "\n",
    "estimators.append(getNormalKerasModel(\"simple\", 0.0001, 10, True))\n",
    "\n",
    "if isSuperComputer:\n",
    "    # all all estimators which we want to search through\n",
    "    estimators = []\n",
    "    \n",
    "    for learningrate in [0.001,0.0001,0.00001]:\n",
    "        estimators.append(getNormalKerasModel(\"normal_learningrate_\"+str(learningrate), learningrate, 100, True))\n",
    "        estimators.append(getDropoutKerasModel(\"dropout_learningrate_\"+str(learningrate), learningrate, 100, True))\n",
    "        estimators.append(getTwoWayKerasModel(\"twoway_learningrate_\"+str(learningrate), learningrate, 100, True))\n",
    "        estimators.append(getComplexKerasModel(\"complex_learningrate_\"+str(learningrate), learningrate, 100, 10, True))\n",
    "        estimators.append(getTwoWayComplexDropoutKerasModel(\"twowaycomplexdropout_learningrate_\"+str(learningrate)+\"_batchsize_\"+str(batch_size), learningrate, 200, 150, 100, 50, 10, True))\n",
    "\n",
    "        for alpha in [0.1,0.01, 0.001, 0.0001]: \n",
    "            estimators.append(getRegularizedKerasModel(\"regularized_learningrate_\"+str(learningrate)+\"_alpha_\"+str(alpha)+\"_batchsize_\"+str(batch_size), learningrate, 100, alpha, True))     \n",
    "\n",
    "    \n",
    "    for learningrate in [0.001,0.0001,0.00001]:\n",
    "            estimators.append(getDeepDropoutKerasModel(\"DeepDropout\"+str(learningrate)+\"_batchsize_\"+str(batch_size)+\"_neuronr_\"+str(100), learningrate, 100, True))\n",
    "            estimators.append(getNormalKerasModel(\"normal_learningrate_\"+str(learningrate)+\"_batchsize_\"+str(batch_size)+\"_neuronr_\"+str(200), learningrate, 200, True))\n",
    "            estimators.append(getDropoutKerasModel(\"dropout_learningrate_\"+str(learningrate)+\"_batchsize_\"+str(batch_size)+\"_neuronr_\"+str(200), learningrate, 200, True))\n",
    "            estimators.append(getTwoWayKerasModel(\"twoway_learningrate_\"+str(learningrate)+\"_batchsize_\"+str(batch_size)+\"_neuronr_\"+str(200), learningrate, 200, True))\n",
    "            estimators.append(getComplexKerasModel(\"complex_learningrate_\"+str(learningrate)+\"_batchsize_\"+str(batch_size)+\"_neuronr_\"+str(200)+\"_\"+str(20), learningrate, 200, 20, True))\n",
    "            estimators.append(getTwoWayComplexDropoutKerasModel(\"twowaycomplexdropout_learningrate_\"+str(learningrate)+\"_batchsize_\"+str(batch_size)+\"_neuronr_\"+str(300)+\"_\"+str(150)+\"_\"+str(100)+\"_\"+str(50)+\"_\"+str(20), learningrate, 300, 150, 100, 50, 20, True))\n",
    "\n",
    "            for alpha in [0.0001, 0.001, 0.01, 0.1]: \n",
    "                estimators.append(getRegularizedKerasModel(\"regularized_learningrate_\"+str(learningrate)+\"_alpha_\"+str(alpha)+\"_batchsize_\"+str(batch_size)+\"_neuronr_\"+str(200), learningrate, 200, alpha, True))             \n",
    "\n",
    "\n",
    "logging.info(\"number of estimators to fit: \" + str(len(estimators)))\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "while len(estimators) > 0:\n",
    "    estimator = estimators.pop(0)\n",
    "    estimatorTimer = time.time()\n",
    "    logging.info(str(estimator.name))\n",
    "    \n",
    "    for category in outputCategories:\n",
    "        \n",
    "        logging.info(\"-- \"+category)\n",
    "        \n",
    "        y_train = train_labels[[\"output_\"+category+\"_\"+str(numberOfOutputWeeks) for numberOfOutputWeeks in range(0,numberOfPreComputedOutputWeeks)]].values\n",
    "        y_valid1 = validation1_labels[[\"output_\"+category+\"_\"+str(numberOfOutputWeeks) for numberOfOutputWeeks in range(0,numberOfPreComputedOutputWeeks)]].values \n",
    "        estimator.fit(X_train, \n",
    "                      y_train, \n",
    "                      batch_size=batch_size, \n",
    "                      epochs=epochs, \n",
    "                      verbose=0, \n",
    "                      validation_data=(X_valid1,y_valid1), \n",
    "                      callbacks=[keras.callbacks.EarlyStopping(patience=10)])\n",
    "        reg = estimator\n",
    "\n",
    "        # save the fitted regressor\n",
    "        #reg.save('models/'+str(modelId))\n",
    "        #reg = keras.models.load_model('models/'+str(modelId))\n",
    "        modelId = modelId + 1\n",
    "\n",
    "        predictions_valid1 = pd.DataFrame(reg.predict(X_valid1), index=validation1_labels.index, columns=[\"pred_week_\" + str(numberOfOutputWeeks)+\"_\"+category for numberOfOutputWeeks in range(0,numberOfPreComputedOutputWeeks)]) \n",
    "        predictions_valid2 = pd.DataFrame(reg.predict(X_valid2), index=validation2_labels.index, columns=[\"pred_week_\" + str(numberOfOutputWeeks)+\"_\"+category for numberOfOutputWeeks in range(0,numberOfPreComputedOutputWeeks)])\n",
    "\n",
    "        valid1Output = validation1_labels[[\"output_\"+category+\"_\"+str(numberOfOutputWeeks) for numberOfOutputWeeks in range(0,numberOfPreComputedOutputWeeks)]]\n",
    "        valid2Output = validation2_labels[[\"output_\"+category+\"_\"+str(numberOfOutputWeeks) for numberOfOutputWeeks in range(0,numberOfPreComputedOutputWeeks)]]\n",
    "\n",
    "        for outputWeekNumber in range(0,numberOfPreComputedOutputWeeks):\n",
    "            rmse1 = np.sqrt(mean_squared_error(predictions_valid1[\"pred_week_\" + str(outputWeekNumber)+\"_\"+category], valid1Output[\"output_\"+category+\"_\"+str(outputWeekNumber)]))\n",
    "            rmse2 = np.sqrt(mean_squared_error(predictions_valid2[\"pred_week_\" + str(outputWeekNumber)+\"_\"+category], valid2Output[\"output_\"+category+\"_\"+str(outputWeekNumber)]))\n",
    "\n",
    "            d = {'model':[reg.name],'model type': [\"keras_multi_weeks_output\"],'target':[category],'week':[outputWeekNumber], 'model rsme 1':[rmse1], 'model rsme 2':[rmse2]}\n",
    "            df = pd.DataFrame(data=d)\n",
    "            df.to_csv(\"results.csv\",mode='a', header=False, index=False)\n",
    "\n",
    "        logging.info(str(estimator.name)+\" (%s)\" % (time.time() - estimatorTimer))\n",
    "\n",
    "logging.info(\"============== finished fitting multi output keras estimators ============== (%s) \" % (time.time() - typeTimer))\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "typeTimer = time.time()\n",
    "\n",
    "logging.info(\"============== fitting multioutput sklearn estimators ============== \")\n",
    "# fitting multioutput sklearn estimators\n",
    "estimators = [\n",
    "    linear_model.MultiTaskLasso(),\n",
    "]\n",
    "if isSuperComputer:\n",
    "    # all estimators which we want to search through\n",
    "\n",
    "    estimators = []\n",
    "\n",
    "    estimators.append(linear_model.Ridge(alpha=100))\n",
    "    estimators.append(linear_model.Ridge(alpha=1000))\n",
    "\n",
    "    \n",
    "    kernels = ['linear','poly','polynomial','rbf','laplacian','sigmoid','cosine']\n",
    "    for alpha in [0.01,0.1,10]:\n",
    "        for k in kernels:\n",
    "            estimators.append(KernelRidge(kernel=k, alpha=alpha))\n",
    "\n",
    "    for alpha in [0.00001, 0.0001,0.001,0.01,0.1,1,10]: \n",
    "        estimators.append(linear_model.MultiTaskLasso(alpha=alpha))\n",
    "\n",
    "    for alpha in [0.00001, 0.0001,0.001,0.01,0.1,0,1,10]: \n",
    "        estimators.append(linear_model.Lasso(alpha=alpha))\n",
    "        estimators.append(linear_model.Ridge(alpha=alpha))\n",
    "\n",
    "    for alpha in [0.00001, 0.0001,0.001,0.01,0.1,1,10]:\n",
    "        for layer in [\n",
    "            (20),\n",
    "            (100),\n",
    "            (20,20),\n",
    "            (100,10),\n",
    "            (100,50,10),\n",
    "            (100,100,10),\n",
    "        ]:\n",
    "            estimators.append(MLPRegressor(hidden_layer_sizes=layer, alpha=alpha, max_iter=1000))\n",
    "\n",
    "    for nrOfEstimators in [100,200,500]: #1000\n",
    "        for maxdepth in [None, 2, 4]:\n",
    "            for mss in [2,3,4]:\n",
    "                for msl in [1,2,3]:\n",
    "                    estimators.append(RandomForestRegressor(n_estimators=nrOfEstimators, \n",
    "                                                            max_depth=maxdepth, \n",
    "                                                            min_samples_split=mss, \n",
    "                                                            min_samples_leaf=msl,\n",
    "                                                            n_jobs=-1))\n",
    "\n",
    "    \n",
    "logging.info(\"number of estimators to fit: \" + str(len(estimators))+\" \")\n",
    "\n",
    "while len(estimators) > 0:\n",
    "    estimator = estimators.pop(0)\n",
    "    estimatorTimer = time.time()\n",
    "    logging.info(str(estimator))\n",
    "    \n",
    "    for category in outputCategories:\n",
    "                                      \n",
    "        logging.info(\"-- \"+category)\n",
    "\n",
    "\n",
    "        trainOutput = train_labels[[\"output_\"+category+\"_\"+str(numberOfOutputWeeks) for numberOfOutputWeeks in range(0,numberOfPreComputedOutputWeeks)]]\n",
    "        reg = estimator.fit(X_train, trainOutput.values)\n",
    "        # save the fitted regressor\n",
    "        #dump(reg, 'models/'+str(modelId)+'.joblib') \n",
    "        #reg = load('models/'+str(modelId)+'.joblib') \n",
    "        modelId = modelId + 1\n",
    "\n",
    "        predictions_valid1 = pd.DataFrame(reg.predict(X_valid1), index=validation1_labels.index, columns=[\"pred_week_\" + str(numberOfOutputWeeks)+\"_\"+category for numberOfOutputWeeks in range(0,numberOfPreComputedOutputWeeks)]) \n",
    "        predictions_valid2 = pd.DataFrame(reg.predict(X_valid2), index=validation2_labels.index, columns=[\"pred_week_\" + str(numberOfOutputWeeks)+\"_\"+category for numberOfOutputWeeks in range(0,numberOfPreComputedOutputWeeks)])\n",
    "\n",
    "        valid1Output = validation1_labels[[\"output_\"+category+\"_\"+str(numberOfOutputWeeks) for numberOfOutputWeeks in range(0,numberOfPreComputedOutputWeeks)]]\n",
    "        valid2Output = validation2_labels[[\"output_\"+category+\"_\"+str(numberOfOutputWeeks) for numberOfOutputWeeks in range(0,numberOfPreComputedOutputWeeks)]]\n",
    "\n",
    "        for outputWeekNumber in range(0,numberOfPreComputedOutputWeeks):\n",
    "            rmse1 = np.sqrt(mean_squared_error(predictions_valid1[\"pred_week_\" + str(outputWeekNumber)+\"_\"+category], valid1Output[\"output_\"+category+\"_\"+str(outputWeekNumber)]))\n",
    "            rmse2 = np.sqrt(mean_squared_error(predictions_valid2[\"pred_week_\" + str(outputWeekNumber)+\"_\"+category], valid2Output[\"output_\"+category+\"_\"+str(outputWeekNumber)]))\n",
    "\n",
    "            d = {'model':[reg],'model type': [\"sklearn_multi_weeks_output\"],'target':[category],'week':[outputWeekNumber], 'model rsme 1':[rmse1], 'model rsme 2':[rmse2]}\n",
    "            df = pd.DataFrame(data=d)\n",
    "            df.to_csv(\"results.csv\",mode='a', header=False, index=False)\n",
    "\n",
    "        logging.info(str(estimator)+\" (%s) \" % (time.time() - estimatorTimer))\n",
    "\n",
    "logging.info(\"============== finished fitting multioutput sklearn estimators ============== (%s) \" % (time.time() - typeTimer))\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "logging.info(\"============== finished ============== %s \" % (time.time() - completeTimer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dirty-concert",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "original-bruce",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpha-affairs",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "through-tuesday",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concrete-leone",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "based-competition",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smoking-software",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diverse-nigeria",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improving-execution",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concerned-differential",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interesting-forum",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic-viking",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improving-pattern",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
